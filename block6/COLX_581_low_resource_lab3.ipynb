{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colx 525 Lab 3: POS projection\n",
    "\n",
    "- Yarowsky, D., & Ngai, G. (2001). **Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection Across Aligned Corpora**. In *Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics*, p.200–207. https://aclanthology.org/N01-1026/\n",
    "\n",
    "- Das, D., & Petrov, S. (2011). **Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections**. In *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies*, p.600–609. http://www.aclweb.org/anthology/P11-1061\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Assignment objectives\n",
    "\n",
    "In this assignment, you will be building a POS-tagger for a low resource language - Mbya Guaraní.\n",
    "Mbya Guaraní is an indigenous Tupian language spoken in South America.  Although speakers have likely been influenced by the colonizing languages around them, it is not related to Spanish, Portuguese, or any other European language.  It only has about 15,000 speakers (according to a 2008 census), and\n",
    "has very few digital tools - in the Universal Dependencies treebank, there are only about 1000 annotated sentences.  In this lab, we will be constructing a POS tagger for Mbya Guaraní, using a parallel Bible translation to get some silver-annotated data.  The first 8 exercises are mandatory, and you should try to complete them.  The final exercise is optional, but should be attempted for full marks.\n",
    "\n",
    "1. Datahandling:  answer questions about data handling\n",
    "    - No coding exercise\n",
    "    - \n",
    "\n",
    "1. Word-based biLSTM POS-tagger: Implement a word-based LSTM POS-tagger.\n",
    "    - No coding exercise\n",
    "\t- ACCURACY: 50.19\n",
    "    - \n",
    "\n",
    "1. Character-based model: Augment the LSTM tagger with character-based features.\n",
    "    - Coding exercise:\n",
    "    - See https://github.com/jungyeul/mds-cl-2023-24/blob/main/block5/COLX_525_morphology_lab3.ipynb (Block 5)\t\n",
    "    - `def reverse_words(tensor, lengths)`\n",
    "\t- `class CharacterModel(nn.Module)`\n",
    "\t    - including `def __init__(self)` and `def forward(self,embs,rev_embs,word_lengths)`\n",
    "\t- `def forward(self,ex)` in class `WordEncoder(nn.Module)`\n",
    "    - ACCURACY: 70.51\n",
    "    -\n",
    "\n",
    "\n",
    "\n",
    "1. Mixing Mbyá Guaraní and German Data: Supplement the taggers with some Standard German data.  \n",
    "    - Not really coding exercise ...\n",
    "    - ACCURACY only w/ German: 26.73 \n",
    "    - ACCURACY fine-tuning: 72.52\n",
    "    -\n",
    "\n",
    "\n",
    "1. Annotation Projection (preparation): Using a small parallel corpus, align Guaraní and Standard German texts.\n",
    "    - Coding exercise\n",
    "    - `def prepare_data(source_file, target_file)`\n",
    "    - `def align_source_target(source, target)`\n",
    "    - \n",
    "\n",
    "1. Annotation Projection: Project Standard German tags onto the Guaraní data, and re-train the model.\n",
    "    - Coding exercise:\n",
    "\t- `def project_tags(bitext, source_tags, target_tags, target_words, out_train, out_test)`\n",
    "\t- Training `train_gun_bible` (projected Guarani from German)\n",
    "\t- ACCURACY: 37.64\t(because of _ )\n",
    "    -\n",
    "\n",
    "1. Don't predict \"_\"!: Allow the model to ignore missing data.\n",
    "    - Not really coding exercise ...\n",
    "    - \n",
    "\n",
    "1. Using both Gold and Silver data: Re-train the model, this time fine-tuning on gold data\n",
    "    - Not really coding exercise ...\n",
    "\t- ACCURACY w/o _ : 42.53\n",
    "\t- ACCURACY fine-tuning: 72.77\n",
    "    - \n",
    "\n",
    "1. Learning less from noise (optional): Bias the learning model towards the gold data.\n",
    "    - Not really coding exercise ...\n",
    "    - change the hyperparameters including learning rate,  dropout, etc. \n",
    "    - ACCURACY goes up to 73.53 (you can do better)\n",
    "    - \n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "You will need to install the Python modules `torchtext`, `torch`, `numpy` and `nltk`. The easiest way to do this is using `anaconda` or `pip`.\n",
    "\n",
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "* Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "* Be sure to follow the general lab instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datahandling\n",
    "\n",
    "We will first read in the data from the data folder in this week's lab.  Prior\n",
    "to starting work on the lab, you will need to make a copy of the data folder (in the student repo for 581) in your own working directory.\n",
    "This data is in \"CONLLU\" format, which consists of tab-separated fields \n",
    "such as words, POS, fine-grained POS, and others. \n",
    "\n",
    "In this lab, we are interested in the word and POS: \n",
    "\n",
    "```\n",
    "1   This    this    DET     DT    Number=Sing|PronType=Dem                                2   det     _       _\n",
    "2   item    item    NOUN    NN    Number=Sing                                             6   nsubj   _       _\n",
    "3   is      be      AUX     VBZ   Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin   6   cop     _       _\n",
    "4   a       a       DET     DT    Definite=Ind|PronType=Art                               6   det     _       _\n",
    "5   small   small   ADJ     JJ    Degree=Pos                                              6   amod    _       _\n",
    "6   one     one     NOUN    NN    Number=Sing                                             0   root    _       _\n",
    "7   and     and     CCONJ   CC    _                                                       9   cc      _       _\n",
    "8   easily  easily  ADV     RB    _                                                       9   advmod  _       _\n",
    "9   missed  miss    VERB    VBN   Tense=Past|VerbForm=Part                                6   conj    _       _\n",
    "10  .       .       PUNCT   .     _                                                       6   punct   _       _\n",
    "```\n",
    "\n",
    "The format consists of tab-separated lines. Each line has 10 fields:\n",
    "\n",
    "1. ID\n",
    "2. Word form\n",
    "3. lemma/base form\n",
    "4. POS tag according to the UD annotation standard.\n",
    "5. Language Specific POS tag.\n",
    "6. Morphological features.\n",
    "7. ID of the syntactic head of the current word.\n",
    "8. The dependency relation between this word and its head.\n",
    "9. A list of depdendencies (can be empty).\n",
    "10. Misc. annotations.\n",
    "  \n",
    "The syntactic dependency trees for different sentences are separated by an empty line. See [this website](https://universaldependencies.org/format.html) for further documentation.\n",
    "\n",
    "The code for data reading is given to you. There is no need to modify it. However, make sure that you understand the output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import conllu\n",
    "import os\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "UNK=\"<unk>\"\n",
    "START=\"<start>\"\n",
    "END=\"<end>\"\n",
    "PAD=\"<pad>\"\n",
    "\n",
    "word_transform = lambda s: [word_vocab[w] for w in [\"<start>\"] + s + [\"<end>\"]]\n",
    "pos_transform = lambda s: [pos_vocab[w] for w in s]\n",
    "char_transform = lambda w: [char_vocab[c] for c in [\"<start>\"] + w + [\"<end>\"]]\n",
    "\n",
    "from itertools import islice\n",
    "from collections import namedtuple\n",
    "Example = namedtuple(\"Example\",[\"word\", \"pos\", \"char\"])\n",
    "\n",
    "class UDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def yield_tokens(data):\n",
    "    for ex in data:\n",
    "        yield([tok[\"form\"] for tok in ex])\n",
    "        \n",
    "def yield_chars(data):\n",
    "    for ex in data:\n",
    "        yield([c for tok in ex for c in tok[\"form\"]])\n",
    "        \n",
    "def yield_pos(data):\n",
    "    for ex in data:\n",
    "        yield([tok[\"upos\"] for tok in ex])\n",
    "        \n",
    "def read_ud_data(lan, vocabs = None):\n",
    "    \n",
    "    def read_data(dire, lang):\n",
    "        train_data = conllu.parse(open(os.path.join(dire, f\"{lang}_train\")).read())\n",
    "        test_data = conllu.parse(open(os.path.join(dire, f\"{lang}_test\")).read())\n",
    "        return train_data, test_data\n",
    "\n",
    "    train_data, test_data = read_data(\"data\",lan)\n",
    "    train = UDDataset(train_data)\n",
    "    test = UDDataset(test_data)\n",
    "\n",
    "    word_vocab = char_vocab = pos_vocab = None\n",
    "    \n",
    "    if vocabs:\n",
    "        word_vocab, char_vocab, pos_vocab = vocabs\n",
    "    else:\n",
    "        word_vocab = build_vocab_from_iterator(yield_tokens(train_data),\n",
    "                                               specials=[\"<unk>\", \"<start>\", \"<end>\"])\n",
    "        word_vocab.set_default_index(word_vocab[\"<unk>\"])\n",
    "\n",
    "        char_vocab = build_vocab_from_iterator(yield_chars(train_data), \n",
    "                                                specials=[\"<unk>\", \"<start>\", \"<end>\", \"<pad>\"])\n",
    "        char_vocab.set_default_index(char_vocab[\"<unk>\"])\n",
    "\n",
    "        pos_vocab = build_vocab_from_iterator(yield_pos(train_data), \n",
    "                                                specials=[\"<unk>\"])\n",
    "        pos_vocab.set_default_index(pos_vocab[\"<unk>\"])\n",
    "        \n",
    "    def split_char_sequence(chars, tokens):\n",
    "        word_lens = [len(w) for w in tokens]\n",
    "        chars = iter(chars)\n",
    "        return [list(islice(chars, elem)) for elem in word_lens]\n",
    "    \n",
    "    def collate_batch(batch):\n",
    "        pos_list, token_list, char_list, word_lens = [], [], [], []\n",
    "        for tokens, chars, pos in zip(yield_tokens(batch), \n",
    "                                      yield_chars(batch),\n",
    "                                      yield_pos(batch)):\n",
    "            # Your code here\n",
    "            token_tensor = torch.tensor(word_transform(tokens), dtype=torch.long).unsqueeze(1)\n",
    "            pos_tensor = torch.tensor(pos_transform(pos), dtype=torch.long).unsqueeze(1)\n",
    "            chars = split_char_sequence(chars, tokens)\n",
    "            chars = [char_transform(w) for w in chars]\n",
    "            char_tensors = [torch.tensor(cs, dtype=torch.long) for cs in chars]\n",
    "\n",
    "            pos_list.append(pos_tensor)\n",
    "            token_list.append(token_tensor)\n",
    "            char_list += char_tensors\n",
    "\n",
    "        return Example(token_list[0],\n",
    "                       pos_list[0],\n",
    "                       (pad_sequence(char_list, batch_first=True, padding_value=char_vocab[\"<pad>\"]).unsqueeze(0),\n",
    "                        len(token_list[0])-2,\n",
    "                        [len(w) for w in tokens]))\n",
    "\n",
    "    test_iter = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "    dev_iter = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_batch)\n",
    "    train_iter = DataLoader(train_data, batch_size=1, shuffle=True, collate_fn=collate_batch)\n",
    "    \n",
    "    return train_iter, dev_iter, test_iter, word_vocab, char_vocab, pos_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now read Guarani training, development and test data. The `read_ud_data()` function returns both the numericalized datasets and word, character and POS vocabularies which map tokens, characters and POS tags into ID numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter, word_vocab, char_vocab, pos_vocab = read_ud_data(\"gun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few assertions to check that everything is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of word types: 221\n",
      "Count of character types: 48\n",
      "Count of POS types: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of word types:\", len(word_vocab))\n",
    "print(\"Count of character types:\", len(char_vocab))\n",
    "print(\"Count of POS types:\", len(pos_vocab))\n",
    "\n",
    "assert len(word_vocab) == 221\n",
    "assert len(char_vocab) == 48\n",
    "assert len(pos_vocab) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1\n",
    "rubric={reasoning:1}\n",
    "\n",
    "The size of the word vocab is much larger than the size of character vocab.  Why would we even want to consider characters in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Given your answer from 1., which model do you think could best use data from another language - a word model, or a character model, and why?  Try to answer this question before moving on to the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy: To evaluate POS tagging, we will be using simple accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 \n",
    "rubric={reasoning:1}\n",
    "\n",
    "1. Why would F1-score be inappropriate for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to compute tagging accuracy is given. No need to modify anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(sys,gold):\n",
    "    \"\"\"\n",
    "    Function for evaluating tagging accuracy w.r.t. a gold standard test set (gold).\n",
    "    \"\"\"\n",
    "    assert(len(sys) == len(gold))\n",
    "    pos_itos = pos_vocab.get_itos()\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "    for s, g in zip(sys,gold):\n",
    "        g_pos = [pos_itos[t[0]] for t in g.pos.tolist()]\n",
    "        assert(len(s) == len(g_pos))\n",
    "        corr += sum([1 if x==y else 0 for x,y in zip(s,g_pos)])\n",
    "        tot += len(s)\n",
    "    return corr * 100.0 / tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: A Simple POS Tagger without a character-level model\n",
    "\n",
    "In this exercise, you will build run BiLSTM POS tagger. The tagger:\n",
    "\n",
    "1. Embeds word tokens in the input sentence.\n",
    "2. Passes the embeddings through a bidirectional LSTM layer.\n",
    "3. Predicts POS tags using a feed-forward network and log softmax layer.\n",
    "\n",
    "This will serve as a baseline against which to compare future models,\n",
    "and can simply be run as is.\n",
    "\n",
    "This is the model that you implemented previously, and you will not need to\n",
    "modify the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.functional import log_softmax, relu\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "from random import random, seed, shuffle\n",
    "\n",
    "# Ensure reproducible results.\n",
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM=50\n",
    "RNN_HIDDEN_DIM=50\n",
    "RNN_LAYERS=1\n",
    "BATCH_SIZE=10\n",
    "EPOCHS=5\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BidirectionalLSTM,self).__init__()\n",
    "        self.forward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "        self.backward_rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS)\n",
    "        \n",
    "    def forward(self,sentence):\n",
    "        fwd_hss, _ = self.forward_rnn(sentence)\n",
    "        bwd_hss, _ = self.backward_rnn(sentence.flip(0))\n",
    "        return torch.cat([fwd_hss, bwd_hss.flip(0)], dim=2)\n",
    "        \n",
    "def drop_words(sequence,word_dropout):\n",
    "    seq_len, _ = sequence.size()\n",
    "    dropout_sequence = sequence.clone()\n",
    "    for i in range(1,seq_len-1):\n",
    "        if random() < word_dropout:\n",
    "            dropout_sequence[i,0] = word_vocab[UNK]\n",
    "    return dropout_sequence\n",
    "        \n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceEncoder,self).__init__()\n",
    "\n",
    "        self.vocabulary = word_vocab\n",
    "        self.embedding = nn.Embedding(len(self.vocabulary),EMBEDDING_DIM)\n",
    "        self.rnn = BidirectionalLSTM()\n",
    "        \n",
    "    def forward(self,ex,word_dropout):\n",
    "        embedded = self.embedding(drop_words(ex.word,word_dropout))\n",
    "        hss = self.rnn(embedded)\n",
    "        return hss[1:-1]\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear1 = nn.Linear(input_dim,input_dim)\n",
    "        self.linear2 = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,tensor):\n",
    "        layer1 = relu(self.linear1(tensor))\n",
    "        layer2 = self.linear2(layer1).log_softmax(dim=2)\n",
    "        return layer2\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 Tagging sentences\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Now it's time to put together all the components that you built so far. `SimplePOSTagger` is a wrapper around a `SentenceEncoder` and `FeedForward` layer. It has a `forward` method which returns a tensor `res` where `res[i,j]` represents the log probability of tag `POS.itos[j]` for the word at position `i` in our input sentences. \n",
    "\n",
    "The function `tag` takes a dataset (development or test data) `data` as input and returns a list of tag sequences as output. For example:\n",
    "\n",
    "```\n",
    "[[\"DET\",\"NOUN\",\"VERB\"],[\"VERB\",\"ADV\"],[\"PRON\",\"VERB\",\"PRON\"]]\n",
    "```\n",
    "\n",
    "As with the previous code, you can simply run the WordPOSTagger \"as is\".  We will \n",
    "be modifying it in the coming sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPOSTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordPOSTagger,self).__init__()\n",
    "        self.tagset_size = len(pos_vocab)\n",
    "        \n",
    "        self.sentence_encoder = SentenceEncoder()\n",
    "        self.hidden2tag = FeedForward(2*RNN_HIDDEN_DIM,self.tagset_size)\n",
    "        \n",
    "    def forward(self,ex, word_dropout=0):\n",
    "        states = self.sentence_encoder(ex,word_dropout)\n",
    "        scores = self.hidden2tag(states)\n",
    "                \n",
    "        return scores\n",
    "\n",
    "    def tag(self,data):\n",
    "        with torch.no_grad():\n",
    "            pos_itos = pos_vocab.get_itos()\n",
    "            results = []\n",
    "            for ex in data:\n",
    "                scores = self(ex)\n",
    "                tags = scores.argmax(dim=2).squeeze(1)\n",
    "                results.append([pos_itos[i] for i in tags])\n",
    "            return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the `WordPOSTagger` class, \n",
    "you can now train your tagger using the following code. \n",
    "For Mbyá Guaraní, you should achieve tagging accuracy around 45-50% on the test set. \n",
    "On Garrett's computer the model trains in 15 seconds (Remember, we only have 50 sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 49 of 49\n",
      "Average loss per example: 2.3946\n",
      "Development accuracy: 25.22\n",
      "Epoch 2: Example 49 of 49\n",
      "Average loss per example: 1.7641\n",
      "Development accuracy: 39.77\n",
      "Epoch 3: Example 49 of 49\n",
      "Average loss per example: 1.3992\n",
      "Development accuracy: 38.27\n",
      "Epoch 4: Example 49 of 49\n",
      "Average loss per example: 1.1138\n",
      "Development accuracy: 44.67\n",
      "Epoch 5: Example 49 of 49\n",
      "Average loss per example: 0.8576\n",
      "Development accuracy: 50.19\n"
     ]
    }
   ],
   "source": [
    "tagger = WordPOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for i,ex in enumerate(train_iter):\n",
    "        print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "        tagger.zero_grad()\n",
    "        output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "        gold = ex.pos.squeeze(dim=1)\n",
    "        loss = loss_function(output,gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.detach().numpy()\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/len(train_iter)))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Character-level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Reversing a batch of words\n",
    "rubric={accuracy:1}\n",
    "\n",
    "We're now going to be extending the POS-tagger to use characters as well as words.\n",
    "\n",
    "This requires that we run a bidirectional LSTM for a padded batch of words, namely all of the words in our input sentence. Consider the following example having size `(4,3)`:\n",
    "\n",
    "$$\\begin{bmatrix} a_1 & b_1 & c_1 \\\\\n",
    "a_2 & b_2 & c_2 \\\\\n",
    "{\\rm PAD} & b_3 & c_3 \\\\\n",
    "{\\rm PAD} & b_4 & {\\rm PAD}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This batch consists of three words $a_1a_2$, $b_1b_2b_3b_4$ and $c_1c_2c_3$. They have been padded to equal length using the symbol ${\\rm PAD}$. In order run a bidirectional LSTM, we need to reverse the words in the batch. This gives the following tensor:\n",
    "\n",
    "$$\\begin{bmatrix} a_2 & b_4 & c_3 \\\\\n",
    "a_1 & b_3 & c_2 \\\\\n",
    "{\\rm PAD} & b_2 & c_1 \\\\\n",
    "{\\rm PAD} & b_1 & {\\rm PAD}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that reverse has the same size as the original tensor and that we haven't touched the PAD symbols. \n",
    "\n",
    "It is your task to implement a function `reverse_words` which takes a tensor as input and returns its reverse. The function also takes a vector of word lengths which you can use when reversing the strings. (**HINT**: you can a Python `range` object to index tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_words(tensor,lengths):\n",
    "    # rev_tensor has the same size as tensor but is filled with padding symbols. \n",
    "    rev_tensor = torch.zeros(tensor.size(),dtype=tensor.dtype) + char_vocab[PAD]\n",
    "\n",
    "    # Fill in the correct values in rev_tensor.\n",
    "    # your code here\n",
    "    for i,l in enumerate(lengths):\n",
    "        ...\n",
    "    # your code here\n",
    "\n",
    "    return rev_tensor\n",
    "\n",
    "# An assertion to check that your code works correctly.\n",
    "words = [[\"a\", \"e\", \"g\"],\n",
    "         [\"b\", \"f\", \"h\"],\n",
    "         [\"c\", PAD, \"j\"],\n",
    "         [PAD, PAD, \"k\"]]\n",
    "reversed_words = [[\"c\", \"f\", \"k\"],\n",
    "                  [\"b\", \"e\", \"j\"],\n",
    "                  [\"a\", PAD, \"h\"],\n",
    "                  [PAD, PAD, \"g\"]]\n",
    "words = torch.LongTensor([[char_vocab[c] for c in row] for row in words])\n",
    "reversed_words = torch.LongTensor([[char_vocab[c] for c in row] for row in reversed_words])\n",
    "word_lengths = torch.tensor([3,2,4])\n",
    "assert(torch.all(reverse_words(words,word_lengths)==reversed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Character-Level Model\n",
    "rubric={accuracy:4}\n",
    "\n",
    "Now you will implement the character-level model `CharacterModel`. You need to initialize the model with two LSTM networks: `self.forward_rnn` and `self.backward_rnn`. Both take a tensor of character embeddings (of dimension `EMBEDDING_DIM`) as input and generate final hidden states (having dimension  `RNN_HIDDEN_DIM`). The networks should be unidirectional and have layer count `RNN_LAYERS`.\n",
    "\n",
    "Your second task is to implement the function `self.forward`. It takes three arguments. The first two arguments `embs` and `rev_embs` are tensors of embeddings representing the input words in a sentence. Both have dimension `(N,sentence_length,EMBEDDING_DIM)`, where `N` is a large enough so that the tensor will fit all words in the input sentence. `emb[i,j,:]` is the embedding of the $i+1$th character in the $j$th word or `embeddin(PAD)` if the $j$th word has fewer than `i+1` characters. `rev_emb` is the reversed version of `emb` obtained using the function `reverse_words` which you just implemented. The third argument to `self.forward` is `word_lengths` which is an array of word lengths.\n",
    "\n",
    "The first thing you should do is to pack the input tensors `embs` and `rev_embs` using [pack_padded_sequence](https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence). This ensures that `self.forward_rnn` and `self.backward_rnn` will return the correct final state. Then yous should run `self.forward_rnn` and `self.backward_rnn` on your packed tensors and returns their final states. You should contatenate them and return a tensor of size `(1,sentence_length,2*RNN_HIDDEN_DIM)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterModel,self).__init__()\n",
    "        \n",
    "        # your code here\n",
    "        self.forward_rnn = ... # LSTM with EMBEDDING_DIM, RNN_HIDDEN_DIM, and RNN_LAYERS; \n",
    "        self.backward_rnn = ... # same: LSTM with EMBEDDING_DIM, RNN_HIDDEN_DIM, and RNN_LAYERS; \n",
    "        #your code here\n",
    "        \n",
    "    def forward(self,embs,rev_embs,word_lengths):\n",
    "        # your code here\n",
    "        embs = ... # requires `pack_padded_sequence` for embs with `enforce_sorted=False`\n",
    "        rev_embs = ... # same; requires `pack_padded_sequence` for rev_embs with `enforce_sorted=False`\n",
    "\n",
    "        _, (fwd_hs,_) = ... # `forward_rnn``\n",
    "        _, (bwd_hs,_) = ... # `backward_rnn`\n",
    "        \n",
    "        return ... # cat with `fwd_hs` `bwd_hs`; \n",
    "        # your code here\n",
    "        \n",
    "# Assertions to check that your code returns objects of the correct size (not a guarantee that your code works).\n",
    "assert(CharacterModel()(torch.zeros(100,10,EMBEDDING_DIM),\n",
    "                        torch.zeros(100,10,EMBEDDING_DIM),\n",
    "                        torch.tensor([1,2,3,4,5,6,7,8,9,10])).size() == \n",
    "       (1,10,2*RNN_HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Encoding words\n",
    "rubric={accuracy:3}\n",
    "\n",
    "The `WordEncoder` class encapsulates a character embedding `self.embedding` and a `CharacterModel`. Its job is to encode a batch of words such as the example below into a representation of size `(sentence_length,1,2*RNN_HIDDEN_DIM)`. \n",
    "\n",
    "$$C=\\begin{bmatrix} t & d & b \\\\\n",
    "h & o & a \\\\\n",
    "e & g & r \\\\\n",
    "{\\rm PAD} & s & k\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "It does this by first using `self.embedding` to embed $B$ and its reversal $D$ given by the function `reverse_words`. It then calls `self.char_model` on the embeddings. `self.char_model` returns a tensor of size `(1,sentence_len,2*RNN_HIDDEN_DIM)`. You need to rearrange the dimensions so that `Wordencoder.forward` can return a tensor of size `(sentence_length,1,2*RNN_HIDDEN_DIM)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordEncoder,self).__init__()  \n",
    "        self.embedding = nn.Embedding(len(char_vocab),EMBEDDING_DIM)\n",
    "        self.char_model = CharacterModel()\n",
    "    \n",
    "    def forward(self,ex):\n",
    "        words, sentence_len, word_lens = ex.char\n",
    "        \n",
    "        # We need to rearrange words and word_lens a bit here to be able \n",
    "        # to feed them to self.char_model. After rearranging, words will \n",
    "        # have size (N,sentence_len), where N is the length of the longest \n",
    "        # word in our input sentence.\n",
    "        words = words.squeeze(0).permute(1,0)\n",
    "        word_lens = torch.tensor(word_lens,dtype=torch.long)\n",
    "        \n",
    "        # Reverse the words in ex.\n",
    "        rev_words = reverse_words(words, word_lens)\n",
    "\n",
    "        # your code here\n",
    "        embedded = ... \n",
    "        rev_embedded = ... \n",
    "        hs = ... # requires `char_model` with embedded and rev_embedded; \n",
    "\n",
    "        # here... you may need `permute`  (print hs to see why)\n",
    "        hs = ... # permute\n",
    "        \n",
    "        return hs\n",
    "        # your code here\n",
    "\n",
    "sentence_len = ex.word.size()[0] - 2\n",
    "assert(WordEncoder()(ex).size() == (sentence_len,1,2*RNN_HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4 Tagging sentences\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Now it's time to put together all the components that you built so far. `CharacterPOSTagger` is a wrapper around a `SentenceEncoder`, `WordEncoder` and `FeedForward` layer. It has a `forward` method which returns a tensor `res` where `res[i,j]` represents the log probability of tag `POS.itos[j]` for the word at position `i` in our input sentences. \n",
    "\n",
    "The function `tag` takes a dataset (development or test data) `data` as input and returns a list of tag sequences as output. For example:\n",
    "\n",
    "```\n",
    "[[\"DET\",\"NOUN\",\"VERB\"],[\"VERB\",\"ADV\"],[\"PRON\",\"VERB\",\"PRON\"]]\n",
    "\n",
    "```\n",
    "\n",
    "Note that the \"tag\" function is identical to the function for the WordPOSTagger - it simply calls the modified tagger.\n",
    "**NOTE!** Again, `with torch.no_grad():` is required to prevent Pytorch from collecting gradients from the test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterPOSTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterPOSTagger,self).__init__()\n",
    "        self.tagset_size = len(pos_vocab)\n",
    "        \n",
    "        self.sentence_encoder = SentenceEncoder()\n",
    "        self.word_encoder = WordEncoder()\n",
    "        self.hidden2tag = FeedForward(4*RNN_HIDDEN_DIM,self.tagset_size)\n",
    "        \n",
    "    def forward(self,ex, word_dropout=0):\n",
    "        word_states = self.sentence_encoder(ex,word_dropout)\n",
    "        char_states = self.word_encoder(ex)\n",
    "        scores = self.hidden2tag(torch.cat([word_states, char_states],dim=2))\n",
    "        return scores\n",
    "       \n",
    "    def tag(self,data):\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            pos_itos = pos_vocab.get_itos()\n",
    "            for ex in data:\n",
    "                scores = self(ex)\n",
    "                tags = scores.argmax(dim=2).squeeze(1)\n",
    "                results.append([pos_itos[i] for i in tags])\n",
    "            return results\n",
    "        \n",
    "pos_size = len(pos_vocab)\n",
    "assert(CharacterPOSTagger()(ex).size() == (ex.word.size()[0]-2,1,pos_size))\n",
    "assert(len(CharacterPOSTagger().tag([ex])[0]) == ex.word.size()[0] -2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the `CharacterPOSTagger` class, you can now train your tagger. You should get accuracy about 65-70%. On Garrett's computer, the model trains in about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 49 of 49\n",
      "Average loss per example: 2.0911\n",
      "Development accuracy: 44.67\n",
      "Epoch 2: Example 49 of 49\n",
      "Average loss per example: 1.2655\n",
      "Development accuracy: 57.97\n",
      "Epoch 3: Example 49 of 49\n",
      "Average loss per example: 0.8390\n",
      "Development accuracy: 64.87\n",
      "Epoch 4: Example 49 of 49\n",
      "Average loss per example: 0.5694\n",
      "Development accuracy: 67.38\n",
      "Epoch 5: Example 49 of 49\n",
      "Average loss per example: 0.3773\n",
      "Development accuracy: 70.51\n"
     ]
    }
   ],
   "source": [
    "tagger = CharacterPOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for i,ex in enumerate(train_iter):\n",
    "        print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "        tagger.zero_grad()\n",
    "        output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "        gold = ex.pos.squeeze(dim=1)\n",
    "        loss = loss_function(output,gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.detach().numpy()\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/len(train_iter)))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4: Mixing Mbyá Guaraní and German Data\n",
    "\n",
    "#### Assignment 4.1\n",
    "rubric={accuracy:6}\n",
    "\n",
    "You now have a Mbyá Guaraní POS-tagger that's been trained on all the available\n",
    "data, and accuracy has improved over the word model. What next?\n",
    "In this section, we are going to add about 1000 Standard German \n",
    "sentences to the training data.  Although German and Guaraní are not closely\n",
    "related, it's possible they share some syntactic information that could help \n",
    "pre-train the model.\n",
    "\n",
    "Run the following cell to read in German data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Universal Depandencies v2 training, development and test sets for German(deu).\n",
    "# Next, create a training iterator for German\n",
    "# train_gua_iter, dev_gua_iter, test_gua_iter, word_vocab, char_vocab, pos_vocab = read_ud_data(\"gun\")\n",
    "train_deu_iter, dev_deu_iter, test_deu_iter, _, _, _ = read_ud_data(\"deu\", vocabs=[word_vocab, char_vocab, pos_vocab])\n",
    "assert(len(train_deu_iter) == 948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now need to re-train your CharacterPOSTagger to incorporate both the Standard and Mbyá Guaraní.  We will be using a method known as \"fine-tuning\", where we train a model on one language, and then re-train for a small number of epochs on a small amount of the data we're really interested in.  This method has wide applications in NLP, from MT, to Sentiment Analysis, to document classification, and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 948 of 948 for German\n",
      "Average loss per example: 0.9057\n",
      "Development accuracy: 30.11\n",
      "Epoch 2: Example 948 of 948 for German\n",
      "Average loss per example: 0.4422\n",
      "Development accuracy: 30.87\n",
      "Epoch 3: Example 948 of 948 for German\n",
      "Average loss per example: 0.3258\n",
      "Development accuracy: 27.35\n",
      "Epoch 4: Example 948 of 948 for German\n",
      "Average loss per example: 0.2543\n",
      "Development accuracy: 27.48\n",
      "Epoch 5: Example 948 of 948 for German\n",
      "Average loss per example: 0.2115\n",
      "Development accuracy: 26.85\n",
      "Epoch 1: Example 49 of 49 for Mbyá Guaraní\n",
      "Average loss per example: 1.9154\n",
      "Development accuracy: 59.72\n",
      "Epoch 2: Example 49 of 49 for Mbyá Guaraní\n",
      "Average loss per example: 0.5629\n",
      "Development accuracy: 69.26\n",
      "Epoch 3: Example 49 of 49 for Mbyá Guaraní\n",
      "Average loss per example: 0.3556\n",
      "Development accuracy: 68.88\n",
      "Epoch 4: Example 49 of 49 for Mbyá Guaraní\n",
      "Average loss per example: 0.2037\n",
      "Development accuracy: 71.52\n",
      "Epoch 5: Example 49 of 49 for Mbyá Guaraní\n",
      "Average loss per example: 0.1479\n",
      "Development accuracy: 73.53\n"
     ]
    }
   ],
   "source": [
    "tagger = CharacterPOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "import itertools\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "\n",
    "    #Your code here\n",
    "    # Since it's same as in Ex2, i just copy here with `train_deu_iter`\n",
    "    # for i,ex in enumerate(train_iter):\n",
    "    #     print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "    #     tagger.zero_grad()\n",
    "    #     output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "\n",
    "    for i,ex in enumerate(train_deu_iter):\n",
    "        print(\"Epoch %u: Example %u of %u for German\" % (epoch+1, i+1,len(train_deu_iter)),end=\"\\r\")\n",
    "        tagger.zero_grad()\n",
    "        output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "        gold = ex.pos.squeeze(dim=1)\n",
    "        loss = loss_function(output,gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += loss.detach().numpy()\n",
    "    #Your code here\n",
    "\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_deu_iter))))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev_iter))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    #Your code here\n",
    "    # It's also same as in Ex2, \n",
    "    \n",
    "    # for i,ex in enumerate(train_iter):\n",
    "    #     print(\"Epoch %u: Example %u of %u for Mbyá Guaraní\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "    #     tagger.zero_grad()\n",
    "    #     output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "    #Your code here\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_iter))))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev_iter))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4.2\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Why is it important that we iterate over Standard German first, and then Mbyá Guaraní?  What do you think would happen if we reversed this order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4.3\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Can you see any potential dangers in training on 50 Mbyá Guaraní sentences, and 1000 Standard German ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Annotation Projection (preparation)\n",
    "\n",
    "We see that just building a German POS tagger gives us very low accuracy, but the fine-tuning is able to recover, despite the fact that it is only 5% of the data of the German.  What if we could get some more Mbyá Guaraní annotated data without having to do it by hand?\n",
    "\n",
    "In this next section, we'll be taking advantage of an unsupervised word aligner to take advantage of the fact that translations of\n",
    "words often occur in the same environments, and usually have the same POS.\n",
    "\n",
    "\n",
    "\n",
    "We'll be using the ``translate'' module from the nltk to produce a word alignment for each sentence on a small parallel corpus: the Bible.  Make sure that you have the nltk installed.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5.1: Read in the German and Mbyá Guaraní, preserving parallelism.\n",
    "rubric={accuracy:4}\n",
    "\n",
    "A tagged version of the German Bible, as well as an\n",
    "untagged version of the Mbyá Guaraní Bible are in the data folder (deu_bible and gun_bible, respectively).\n",
    "\n",
    "The files are in CONLL format, as before.\n",
    "\n",
    "Each line of the file corresponds to a single word with its tag, and verses are separated by a blank line.\n",
    "\n",
    "You must read in each of the Bibles, storing both the tags and words in such \n",
    "a way that the parallel nature of the Bibles is preserved.  For now, you can assign \"_\" as the tag of every Mbyá Guaraní word.  Your function should return 4 lists: the source words, the target words, the source tags, and the target tags.\n",
    "\n",
    "I encourage you to make sure that the data is really parallel.  Although I don't speak Mbyá Guaraní, and most of you probably don't speak German, named Entities can provide clues that data is parallel.  Named Entities are often similar across languages.  If you see \"Jesu\" on the German side, but no \"Jesus\" on the Mbyá Guaraní side, you may have lost the parallelism."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data % head *_bible\n",
    "==> deu_bible <==\n",
    "1\tDAS\t_\tX\t_\t_\t_\t_\t_\t_\n",
    "2\tBuch\t_\tNOUN\t_\t_\t_\t_\t_\t_\n",
    "3\tder\t_\tDET\t_\t_\t_\t_\t_\t_\n",
    "4\tAbstammung\t_\tNOUN\t_\t_\t_\t_\t_\t_\n",
    "5\tJesu\t_\tPROPN\t_\t_\t_\t_\t_\t_\n",
    "6\tChristi\t_\tPROPN\t_\t_\t_\t_\t_\t_\n",
    "7\t,\t_\tPUNCT\t_\t_\t_\t_\t_\t_\n",
    "8\tdes\t_\tDET\t_\t_\t_\t_\t_\t_\n",
    "9\tSohnes\t_\tNOUN\t_\t_\t_\t_\t_\t_\n",
    "10\tDavids\t_\tPROPN\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "==> gun_bible <==\n",
    "1\tKova'e\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "2\tkuaxia\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "3\tre\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "4\tma\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "5\toĩ\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "6\tJesus\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "7\tCristo\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "8\tramoĩ\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "9\typy\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "10\tkuery\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "data % \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Kova'e\", 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', \"ha'e\", 'Davi', 'ma', 'Abraão', 'ramymino', \"raka'e\", '.']\n",
      "['DAS', 'Buch', 'der', 'Abstammung', 'Jesu', 'Christi', ',', 'des', 'Sohnes', 'Davids', ',', 'des', 'Sohnes', 'Abrahams', '.']\n",
      "['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "['X', 'NOUN', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT']\n",
      "7957\n",
      "7957\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def prepare_data(source_file, target_file):\n",
    "    \n",
    "    source_words = [] # list of lists; \n",
    "    target_words = []\n",
    "    source_tags = []\n",
    "    target_tags = []\n",
    "\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    ...  FUN to process data files ... \n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    return source_words, target_words, source_tags, target_tags\n",
    "\n",
    "german_words, guarani_words, german_tags, guarani_tags = prepare_data(\"deu_bible\", \"gun_bible\")\n",
    "\n",
    "print(guarani_words[0])\n",
    "print(german_words[0])\n",
    "print(guarani_tags[0])\n",
    "print(german_tags[0])\n",
    "print(len(german_tags))\n",
    "print(len(guarani_tags))\n",
    "# assert(len(guarani_words) == len(german_words))\n",
    "# assert(len(guarani_tags) == len(german_tags))\n",
    "# assert(guarani_words[0] == [\"Kova'e\", 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', \"ha'e\", 'Davi', 'ma', 'Abraão', 'ramymino', \"raka'e\", '.'])\n",
    "# assert(guarani_tags[0] == ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Kova'e\", 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', \"ha'e\", 'Davi', 'ma', 'Abraão', 'ramymino', \"raka'e\", '.'], ['Abraão', \"ra'y\", 'ma', 'Isaque', ',', 'Isaque', \"ra'y\", 'ma', 'Jacó', ',', 'Jacó', \"ra'y\", 'ma', 'Judá', \"ha'e\", 'tyvy', 'kuery', '.']]\n",
      "[['DAS', 'Buch', 'der', 'Abstammung', 'Jesu', 'Christi', ',', 'des', 'Sohnes', 'Davids', ',', 'des', 'Sohnes', 'Abrahams', '.'], ['Abraham', 'zeugte', 'den', 'Isaak', '.', 'Isaak', 'zeugte', 'den', 'Jakob', '.', 'Jakob', 'zeugte', 'den', 'Juda', 'und', 'seine', 'Brüder', '.']]\n",
      "[['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_']]\n",
      "[['X', 'NOUN', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT'], ['PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'DET', 'PROPN', 'PUNCT', 'PROPN', 'VERB', 'DET', 'NOUN', 'CCONJ', 'PRON', 'NOUN', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "print(guarani_words[:2])\n",
    "print(german_words[:2])\n",
    "print(guarani_tags[:2])\n",
    "print(german_tags[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5.2 - Align the two corpora\n",
    "rubric={accuracy:1}\n",
    "\n",
    "We will now use the IBMModel2 class from nltk.translate to align the two corpora.  You may recall that IBM Model 1 simply looks for words that co-occur in sentences, while Model 2 also learns an overarching re-ordering model.\n",
    "\n",
    "In this part of the assignment, you will need to assign the german_words and guarani_words from the previous section to a bitext, which IBMModel2 will use to produce an alignment.  I suggest you look at https://www.nltk.org/_modules/nltk/translate/ibm2.html for help in how to do so.\n",
    "\n",
    "The input to the function will be two lists: a list of source (German) sentences, and a list of target (Guarani) sentences:\n",
    "[[\"Kova'e\", 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', \"ha'e\", 'Davi', 'ma', 'Abraão', 'ramymino', \"raka'e\", '.'], ['Guarani', 'Sentence', 'number', '2', '.'], ['Guarani', 'Sentence', 'number', '3', '.'], etc]\n",
    "['DAS', 'Buch', 'der', 'Abstammung', 'Jesu', 'Christi', ',', 'des', 'Sohnes', 'Davids', ',', 'des', 'Sohnes', 'Abrahams', '.'], ['German', 'Sentence', 'number', '2', '.'], ['German', 'Sentence', 'number', '3', '.'], etc]\n",
    "\n",
    "Each of these sentences should be added to the bitext in parallel.\n",
    "\n",
    "The output will be the bitext, after you run it through IBMModel2 (you can use 5 iterations for the IBMModel2 function).\n",
    "\n",
    "After it goes through the IBMModel2 function, bitext will be transformed into an item with three parameters: words, mots, and alignment:\n",
    "\n",
    "bitext.words will be the source sentences\n",
    "bitext.mots will be the target sentences\n",
    "bitext.alignment will be the alignment\n",
    "\n",
    "\n",
    "This alignment is of the form:\n",
    "\n",
    "0 0 1 3 4 3 6 3 8 5 ..., where the first index corresponds to the source language in your bitext, and the second to the target.  Although we are using German as a source, and Mbyá Guaraní as a target, the algorithm is only concerned with how you present them to the function - be careful that you are consistent!\n",
    "\n",
    "This step takes a bit of time, but you should only have to learn the alignment once.  Wait for it to finish... at completion, it will print out the time it took to learn.  On Garrett's computer, it takes about 6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting!\n",
      "--- 158.92377614974976 seconds ---\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def align_source_target(source, target):\n",
    "    \n",
    "\n",
    "    from nltk.translate import AlignedSent\n",
    "    from nltk.translate import IBMModel2\n",
    "    import time\n",
    "\n",
    "    bitext = []\n",
    "    #Your code here\n",
    "    # iterate source or target legnth, \n",
    "    # append AlignedSent (source and target) to `bitext`\n",
    "    \n",
    "    #Your code here\n",
    "\n",
    "    print(\"Starting!\")\n",
    "    start_time = time.time()\n",
    "    # Then, IBMModel2 (given)\n",
    "    alignment = IBMModel2(bitext, 5)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Done!\")\n",
    "    return bitext\n",
    "    \n",
    "german_guarani_alignment = align_source_target(german_words, guarani_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AlignedSent(['DAS', 'Buch', 'der', 'Abstammung', 'Jesu', 'Christi', ',', 'des', 'Sohnes', 'Davids', ',', 'des', 'Sohnes', 'Abrahams', '.'], ['Kova'e', 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', 'ha'e', 'Davi', 'ma', 'Abraão', 'ramymino', 'raka'e', '.'], Alignment([(0, 7), (1, 1), (2, 4), (3, 7), (4, 10), (5, 6), (6, 13), (7, 10), (8, 7), (9, 19), (10, 16), (11, 10), (12, 7), (13, 21), (14, None)])),\n",
       " AlignedSent(['Abraham', 'zeugte', 'den', 'Isaak', '.', 'Isaak', 'zeugte', 'den', 'Jakob', '.', 'Jakob', 'zeugte', 'den', 'Juda', 'und', 'seine', 'Brüder', '.'], ['Abraão', 'ra'y', 'ma', 'Isaque', ',', 'Isaque', 'ra'y', 'ma', 'Jacó', ',', 'Jacó', 'ra'y', 'ma', 'Judá', 'ha'e', 'tyvy', 'kuery', '.'], Alignment([(0, 0), (1, 8), (2, 16), (3, 3), (4, None), (5, 3), (6, 8), (7, 8), (8, 8), (9, None), (10, 10), (11, 10), (12, 16), (13, 13), (14, 14), (15, 15), (16, 15), (17, 17)]))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_guarani_alignment[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [AlignedSent(['DAS', 'Buch', 'der', 'Abstammung', 'Jesu', 'Christi', ',', 'des', 'Sohnes', 'Davids', ',', 'des', 'Sohnes', 'Abrahams', '.'], \n",
    "#              ['Kova'e', 'kuaxia', 're', 'ma', 'oĩ', 'Jesus', 'Cristo', 'ramoĩ', 'ypy', 'kuery', 'rery', '.', 'Jesus', 'ma', 'Davi', 'ramymino', 'oiko', ',', 'ha'e', 'Davi', 'ma', 'Abraão', 'ramymino', 'raka'e', '.'], \n",
    "#               Alignment([(0, 7), (1, 1), (2, 4), (3, 7), (4, 10), (5, 6), (6, 13), (7, 10), (8, 7), (9, 19), (10, 16), (11, 10), (12, 7), (13, 21), (14, None)])), \n",
    "#               ... ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Annotation Projection\n",
    "\n",
    "#### Assignment 6.1\n",
    "rubric={accuracy:3}\n",
    "\n",
    "Now that we have an alignment, we can use it to project German tags onto the Mbyá Guaraní data.\n",
    "\n",
    "The alignment is now stored in the variables bitext[x].alignment, where x is the xth sentence of the bitext.  You should loop through the alignment for each sentence (to speed things up down the road, and to make things comparable with the Standard German results, I suggest limiting the sentences to 948), and for each alignment pair (ie, 0-3), find the tag from the source and assign it to the target (hint, remember german_tags and guarani_tags, from above?).  You will need to decide what to do if more than one German word aligns to a single Guarani word.  Some possible suggestions:  simply choose the\n",
    "tag of the first word that aligns to the Guarani word; randomly choose a tag from the words that align; if more than one word aligns, and the tags aren't the same, assign \"_\" as the tag.  All are reasonable, and all have repercussions.\n",
    "\n",
    "After assigning tags, the tagged Mbyá Guaraní information will be written to a pair of files.\n",
    "\n",
    "Your function will take five parameters - the bitext, the source(Standard German) tags, the target (Mbyá Guaraní) words, and the file names of your training and testing files that you will write.  It will not return anything.  Remember to close your files at the end of the function!\n",
    "\n",
    "An example projection might look like this:\n",
    "\n",
    "source_words[x] = [\"Eishockey\", \"ist\", \"ein\", \"wünderbarer\", \"Sport\", \"!\"]\n",
    "target_words[x] = [\"Hockey\", \"is\", \"a\", \"great\", \"game\", \"!\"]\n",
    "source_tags[x] = [\"NOUN\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"PUNCT\"]\n",
    "target_tags[x] = [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\"]\n",
    "bitext[x].alignment = [0 0, 1 1, 2 2, 5 5]\n",
    "\n",
    "output[x] (written to file. Don't worry about lining it up; just separate everything with tabs):\n",
    "\n",
    "1     Hockey    _     NOUN       _     _     _     _     _     _\n",
    "2     is        _     VERB       _     _     _     _     _     _\n",
    "3     a         _     DET        _     _     _     _     _     _\n",
    "4     great     _     _          _     _     _     _     _     _\n",
    "5     game      _     _          _     _     _     _     _     _\n",
    "6     !         _     PUNCT      _     _     _     _     _     _\n",
    "\n",
    "1     Sentence  _     ...\n",
    "2     2         _     ...\n",
    "3     goes      _     ...\n",
    "4     here      _     ...\n",
    "5     .         _     ...\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_tags(bitext, source_tags, target_tags, target_words, out_train, out_test):\n",
    "    #Your code here:\n",
    "    \n",
    "    # you can use the following: \n",
    "\n",
    "    # alignment = bitext[sentence].alignment\n",
    "    # for sourceID, targetID in alignment:\n",
    "    #   target_tags[sentence][targetID] = source_tags[sentence][sourceID]\n",
    "\n",
    "    # To create CONLLU files; \n",
    "\n",
    "    #Your code here\n",
    "                            \n",
    "# print(guarani_tags[936])\n",
    "project_tags(german_guarani_alignment, german_tags, guarani_tags, guarani_words, \"gun_bible_train\", \"gun_bible_test\")\n",
    "# print(len(german_tags))\n",
    "# print(len(guarani_tags))\n",
    "\n",
    "# #Check that we haven't introduced any extra lines, or removed any\n",
    "# assert(len(german_tags) == len(guarani_tags) and len(german_guarani_alignment) == len(guarani_tags))\n",
    "# #Check that we have a tag (or _) for each word in a sentence:\n",
    "# assert(len(german_tags[40]) == len(german_words[40]) and len(guarani_tags[40]) == len(guarani_words[40]))\n",
    "# #Check that we've gotten some tags projected       \n",
    "# assert('NOUN' in guarani_tags[936] or 'DET' in guarani_tags[936] or 'VERB' in guarani_tags[936] or 'ADP' in guarani_tags[936])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 7), (1, 1), (2, 4), (3, 7), (4, 10), (5, 6), (6, 13), (7, 10), (8, 7), (9, 19), (10, 16), (11, 10), (12, 7), (13, 21), (14, None)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_guarani_alignment[0].alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'NOUN', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'PROPN', 'PUNCT']\n",
      "['_', 'NOUN', '_', '_', 'DET', '_', 'PROPN', 'X', '_', '_', 'PROPN', '_', '_', 'PUNCT', '_', '_', 'PUNCT', '_', '_', 'PROPN', '_', 'PROPN', '_', '_', '_']\n"
     ]
    }
   ],
   "source": [
    "print(german_tags[0])\n",
    "print(guarani_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the files in as using `read_ud_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Universal Depandencies v2 training, development and test sets for English.\n",
    "train_gun_bible_iter, dev_gun_bible_iter, test_gun_bible_iter, word_vocab, char_vocab, pos_vocab = read_ud_data(\"gun_bible\")\n",
    "\n",
    "# # Print the first example in the Mbyá Guaraní training set.\n",
    "# ex = next(iter(iter(iter(train_gun_bible_iter))))\n",
    "# print(len(train_gun_bible_iter))\n",
    "# print(ex.word)\n",
    "# print(ex.char)\n",
    "# print(ex.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train a Mbyá Guaraní tagger from just the Bible data.  It's noisy,\n",
    "and missing a lot of tags, so it likely won't be as good as the one we trained previously, even though there's more data.  You will likely get below 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 948 of 948\n",
      "Average loss per example: 1.6749\n",
      "Development accuracy: 38.02\n",
      "Epoch 2: Example 948 of 948\n",
      "Average loss per example: 1.3689\n",
      "Development accuracy: 40.15\n",
      "Epoch 3: Example 948 of 948\n",
      "Average loss per example: 1.2191\n",
      "Development accuracy: 35.26\n",
      "Epoch 4: Example 948 of 948\n",
      "Average loss per example: 1.0856\n",
      "Development accuracy: 40.03\n",
      "Epoch 5: Example 948 of 948\n",
      "Average loss per example: 0.9434\n",
      "Development accuracy: 37.14\n"
     ]
    }
   ],
   "source": [
    "tagger = CharacterPOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "\n",
    "weights = torch.ones(len(pos_vocab))\n",
    "weights[pos_vocab['_']] = 0\n",
    "loss_function = nn.NLLLoss(weights)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #Your code here\n",
    "    tot_loss = 0\n",
    "\n",
    "    # Training is same as before except for `train_gun_bible_iter`; \n",
    "    # for i,ex in enumerate(train_gun_bible_iter):\n",
    "    #     print(\"Epoch %u: Example %u of %u\" % (epoch+1, i+1,len(train_gun_bible_iter)),end=\"\\r\")\n",
    "    #     tagger.zero_grad()\n",
    "    #     output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     #print(output)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "    #Your code here\n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_gun_bible_iter))))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % accuracy(sys_dev, dev_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Don't predict \"_\"!\n",
    "\n",
    "Ok, so we do about half as well as our tagger trained only on 50 Mbyá Guaraní sentences (but much better than the model trained on just German).  Not bad, but we can do better.  Some of you may have noticed that the alignment was missing a lot of words on the Guaraní side, so we had no choice but to use \"_\" as a tag.  This is ok, but \"_\" will never be a tag in the test data!  We can instead bias the model so that it never predicts \"_\".  This will involve 2 small changes to your character-based model:\n",
    "\n",
    "1.  In the tag function of your tagger, we need to make sure that \"_\" is never picked as the argmax.  This can be achieved by modifying the \"scores\" variable so that its value for \"_\" is very, very small (not 0!  These are log-likelihoods, so will be negative values).  Hint: remember vocab.stoi? \n",
    "\n",
    "2.  Make sure that the model ignores the loss for examples that have \"_\" in the testing data.  If we just do 1., then every time the model predicts something other than \"_\" when we have \"_\" in the data, the model will think we have huge loss, which will be back-propagated through the network.  Luckily, the NLLLoss() function allows you to set the \"weight\" parameter to a tensor specifying weights for each loss: https://pytorch.org/docs/stable/nn.html.  Why not just set the weight of \"_\" loss to 0?\n",
    "\n",
    "You may only observe a small gain from this step (or no gain at all), but it is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterPOSTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterPOSTagger,self).__init__()\n",
    "        self.tagset_size = len(pos_vocab)\n",
    "        \n",
    "        self.sentence_encoder = SentenceEncoder()\n",
    "        self.word_encoder = WordEncoder()\n",
    "        self.hidden2tag = FeedForward(4*RNN_HIDDEN_DIM,self.tagset_size)\n",
    "        \n",
    "    def forward(self,ex, word_dropout=0):\n",
    "        word_states = self.sentence_encoder(ex,word_dropout)\n",
    "        char_states = self.word_encoder(ex)\n",
    "        scores = self.hidden2tag(torch.cat([word_states, char_states],dim=2))\n",
    "        return scores\n",
    "       \n",
    "    def tag(self,data):\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            pos_itos = pos_vocab.get_itos()\n",
    "            for ex in data:\n",
    "                scores = self(ex)\n",
    "                #Your code here\n",
    "                # assign a very low value for `_`\n",
    "                ...\n",
    "                #Your code here\n",
    "\n",
    "                tags = scores.argmax(dim=2).squeeze(1)\n",
    "                results.append([pos_itos[i] for i in tags])\n",
    "            return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 7.1\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Even though the projected model is not as good as the gold model, can you think of any cases where we might want (or have) to use it on its own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also be fine-tuning on the gold data, as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Using both Gold and Silver data\n",
    "\n",
    "#### Assignment 8.1\n",
    "rubric={accuracy:3}\n",
    "\n",
    "As a final step, we will fine-tune the projected Mbyá Guaraní model with the gold data, though, fine-tuned on the gold-data.  In this section, we'll be combining the Silver data that you got from the projection and the Gold, hand-annotated data. This model can get up to 75-80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "49\n",
      "Epoch 1: Example 948 of 948 for Silver\n",
      "Average loss per example: 1.6665\n",
      "Development accuracy: 37.26\n",
      "Epoch 2: Example 948 of 948 for Silver\n",
      "Average loss per example: 1.3579\n",
      "Development accuracy: 42.53\n",
      "Epoch 3: Example 948 of 948 for Silver\n",
      "Average loss per example: 1.2133\n",
      "Development accuracy: 42.53\n",
      "Epoch 4: Example 948 of 948 for Silver\n",
      "Average loss per example: 1.0747\n",
      "Development accuracy: 37.77\n",
      "Epoch 5: Example 948 of 948 for Silver\n",
      "Average loss per example: 0.9388\n",
      "Development accuracy: 41.28\n",
      "Epoch 1: Example 49 of 49 for Gold\n",
      "Average loss per example: 1.3910\n",
      "Development accuracy: 60.60\n",
      "Epoch 2: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.7847\n",
      "Development accuracy: 69.89\n",
      "Epoch 3: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.5476\n",
      "Development accuracy: 73.02\n",
      "Epoch 4: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.3795\n",
      "Development accuracy: 73.27\n",
      "Epoch 5: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.2961\n",
      "Development accuracy: 73.90\n"
     ]
    }
   ],
   "source": [
    "tagger = CharacterPOSTagger()\n",
    "optimizer = Adam(tagger.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "weights = torch.ones(len(pos_vocab))\n",
    "\n",
    "#Your code here\n",
    "# assigne `weights`` for `_` 0.0\n",
    "\n",
    "#Your code here\n",
    "\n",
    "\n",
    "loss_function = nn.NLLLoss(weight = weights)\n",
    "\n",
    "print(len(dev_iter))\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model = None\n",
    "best_ft = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tot_loss = 0\n",
    "    for i,ex in enumerate(train_gun_bible_iter):\n",
    "    # #Your code here#\n",
    "    # #SAME as before (uncomment them to run)\n",
    "    #     print(\"Epoch %u: Example %u of %u for Silver\" % (epoch+1, i+1,len(train_gun_bible_iter)),end=\"\\r\")\n",
    "    #     tagger.zero_grad()\n",
    "    #     output = tagger(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "    # #Your code here#\n",
    "    \n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_gun_bible_iter))))\n",
    "    sys_dev = tagger.tag(dev_iter)\n",
    "    sys_acc = accuracy(sys_dev, dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % sys_acc)\n",
    "    if(sys_acc > best_acc):\n",
    "        best_acc = sys_acc\n",
    "        best_model = copy.deepcopy(tagger)\n",
    "    \n",
    " \n",
    "# This is effectively saving the model, so we don't have to re-train the silver\n",
    "# part every time\n",
    "fine_tuned = copy.deepcopy(best_model)\n",
    "optimizer = Adam(fine_tuned.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # #Your code here#\n",
    "    # #SAME as before (uncomment them to run)\n",
    "    # tot_loss = 0\n",
    "    # for i,ex in enumerate(train_iter):\n",
    "    #     print(\"Epoch %u: Example %u of %u for Gold\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "    #     fine_tuned.zero_grad()\n",
    "    #     output = fine_tuned(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "    # #Your code here#\n",
    "    \n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_iter))))\n",
    "    sys_dev = fine_tuned.tag(dev_iter)\n",
    "    sys_acc = accuracy(sys_dev, dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % sys_acc)\n",
    "    if(sys_acc > best_acc):\n",
    "        best_acc = sys_acc\n",
    "        best_ft = copy.deepcopy(fine_tuned)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Learning less from noise (optional)\n",
    "\n",
    "#### Assignment 9.1 Optional\n",
    "rubric={accuracy:2}\n",
    "\n",
    "We get a gain over the model trained on just gold data - an error reduction of about 10%-20%.  We might be able to do a little bit better, though.  We could fine-tune for more epochs - since we only have 50 gold sentences, fine-tuning is fast - but we're going to try something a little bit different.  We're going to instead vary the learning rate of the algorithm.\n",
    "\n",
    "Broadly speaking, the learning rate controls how much we want our network to \n",
    "move toward our current training data.  By default, we've been using a learning\n",
    "rate of 1.0, meaning that all of the loss is back-propagated through the network.  By changing the learning rate, we can either punish the network more\n",
    "for mistakes, or let it more slowly model the training data if we don't trust it.\n",
    "\n",
    "We can implement a learning rate by multiplying the loss by a constant value - values greater than 1.0 will speed up training (possibly at the cost of overfitting the data), while speeds less than 1.0 will slow it down (but possibly underfit the data).  You are free to experiment with various values of learning rates - I was able to get the final accuracy up to about 81%, which represents almost a 10% further error reduction over the original model.\n",
    "\n",
    "You're also free to tune some other parameters - the dropout rate is very low.  Higher dropouts tend to create more robust systems, \n",
    "but can underfit the data if there isn't a lot of it.  You might also want to compare your results from tuning the learning rate to increasing\n",
    "the number of fine-tuning epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Example 49 of 49 for Gold\n",
      "Average loss per example: 1.3507\n",
      "Development accuracy: 63.36\n",
      "Epoch 2: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.7706\n",
      "Development accuracy: 70.26\n",
      "Epoch 3: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.5319\n",
      "Development accuracy: 70.51\n",
      "Epoch 4: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.3914\n",
      "Development accuracy: 72.15\n",
      "Epoch 5: Example 49 of 49 for Gold\n",
      "Average loss per example: 0.2900\n",
      "Development accuracy: 72.77\n"
     ]
    }
   ],
   "source": [
    "fine_tuned = copy.deepcopy(best_model)\n",
    "optimizer = Adam(fine_tuned.parameters())\n",
    "\n",
    "\n",
    "## YOU CAN CHANGE the value;\n",
    "learning_rate = 1.0\n",
    "\n",
    "\n",
    "import numpy\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #Your code here#\n",
    "    # #SAME as before (uncomment them to run)\n",
    "\n",
    "    # tot_loss = 0\n",
    "    # best_acc = 0\n",
    "\n",
    "    # for i,ex in enumerate(train_iter):\n",
    "    #     print(\"Epoch %u: Example %u of %u for Gold\" % (epoch+1, i+1,len(train_iter)),end=\"\\r\")\n",
    "    #     fine_tuned.zero_grad()\n",
    "    #     output = fine_tuned(ex,word_dropout=0.05).squeeze(dim=1)\n",
    "    #     gold = ex.pos.squeeze(dim=1)\n",
    "    #     loss = loss_function(output,gold)\n",
    "    #     loss = loss * learning_rate\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     tot_loss += loss.detach().numpy()\n",
    "    #Your code here#\n",
    "    \n",
    "    print(\"\\nAverage loss per example: %.4f\" % (tot_loss/(len(train_iter))))\n",
    "    sys_dev = fine_tuned.tag(dev_iter)\n",
    "    sys_acc = accuracy(sys_dev, dev_iter)\n",
    "    print(\"Development accuracy: %.2f\" % sys_acc)\n",
    "    if(sys_acc > best_acc):\n",
    "        best_acc = sys_acc\n",
    "        best_ft = copy.deepcopy(fine_tuned)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 9.2 Optional\n",
    "rubric={reasoning:1}\n",
    "\n",
    "One of the problems with Projection is that many words don't align that well (particularly if we have a small parallel corpus).  Can you think of some ways that we could replace some of the \"_\"s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
