{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 581 Lab 4: Active learning for morphological segmentation\n",
    "\n",
    "In this lab, we will investigate different active learning strategies for the task of supervised morphological segmentation for English. We won't actually annotate any data in this assignment. Instead we will simulate active learning in the following way: We will start with a small annotated training set of 50 segmented words. We will then explore different criteria for incorporating additional training examples from a large annotated training set.\n",
    "\n",
    "In the first assignment, we will implement a segmentation system using the `pycrfsuite` toolkit. We'll will also train and evaluate a tagger on a small set of 50 training examples.\n",
    "\n",
    "In the second assignment, we'll explore the effect of adding 50 additional randomly sampled training examples to the training set. This can be seen as a baseline for our active learning experiment.\n",
    "\n",
    "In the third assignment, you will you will implement two active learning strategies belonging to the query-by-uncertainty family.\n",
    "\n",
    "In the fourth assignment, you will compare the active learning strategy query-by-uncertainty to query-by-committee.\n",
    "\n",
    "Let's start by reading three segementation datasets: \n",
    "\n",
    "* A small training set of 50 segmented word forms,\n",
    "* A larger training set containing 1350 segmented word forms, and\n",
    "* A development set which we will use for evaluation\n",
    "\n",
    "We use a function `read_data` from a module `data_handling` which is provided with the starter code. You are welcome to look at the module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### dadta_handling ####\n",
    "########################\n",
    "\n",
    "BEGIN = \"BEGIN\"\n",
    "INSIDE = \"INSIDE\"\n",
    "END = \"END\" \n",
    "SINGLE = \"SINGLE\"\n",
    "BOUNDARY=\"<BD>\"\n",
    "\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        input, output = line.split(\"\\t\")\n",
    "        data.append((input,output.split(\" \")))\n",
    "    return data\n",
    "\n",
    "def get_bies_notation(data):\n",
    "    result = []\n",
    "    for _, ex in data:\n",
    "        ex = [[[c,INSIDE] for c in morph] for morph in ex]\n",
    "        for morph in ex:\n",
    "            if len(morph) == 1:\n",
    "                morph[0][1] = SINGLE\n",
    "            else:\n",
    "                morph[0][1] = BEGIN\n",
    "                morph[-1][1] = END\n",
    "        result.append([c for morph in ex for c in morph])\n",
    "    return result\n",
    "\n",
    "def unbies(data):\n",
    "    result = []\n",
    "    for ex in data:\n",
    "        segmented = ['']\n",
    "        for char, tag in ex:\n",
    "            segmented[-1] += char\n",
    "            if tag == \"SINGLE\" or tag == \"END\":\n",
    "                segmented.append('')\n",
    "        segmented = [seg for seg in segmented if seg != '']\n",
    "        result.append([''.join(segmented), segmented])\n",
    "    return result\n",
    "\n",
    "def char2features(example, i):\n",
    "    char = example[i][0]\n",
    "    char_minus_1 = example[i-1][0] if i > 0 else BOUNDARY\n",
    "    char_plus_1 = example[i+1][0] if i + 1 < len(example) else BOUNDARY \n",
    "    char_minus_2 = example[i-2][0] if i - 1 > 0 else BOUNDARY\n",
    "    char_plus_2 = example[i+2][0] if i + 2 < len(example) else BOUNDARY \n",
    "    tag = example[i][1]\n",
    "    distance_from_start = i\n",
    "    distance_to_end = len(example) - i\n",
    "    word_prefix = \"\".join([c for c,tag in example[:i+1]])\n",
    "    \n",
    "    features=[\"CHAR=%s\" % char,\n",
    "              \"CHAR-1=%s\" % char_minus_1,\n",
    "              \"CHAR-2=%s\" % char_minus_2,\n",
    "              \"CHAR+1=%s\" % char_plus_1,\n",
    "              \"CHAR+2=%s\" % char_plus_2,\n",
    "              \"STR--=%s\" % (char_minus_2 + char_minus_1 + char),\n",
    "              \"STR++=%s\" % (char + char_plus_1 + char_plus_2),\n",
    "              \"STR-+=%s\" % (char_minus_1 + char + char_plus_1), \n",
    "              \"DIST_FROM_START=%u\" % distance_from_start]\n",
    " \n",
    "    return features\n",
    "\n",
    "def data2features(data):\n",
    "    \"\"\" Extract features for a data set in BIES format. \"\"\"\n",
    "    return [[char2features(example,i) for i in range(len(example))] for example in data]\n",
    "\n",
    "def data2labels(data):\n",
    "    \"\"\" Extract the tags from a data set in BIES format. \"\"\"\n",
    "    return [[tok[1] for tok in example] for example in data]\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def evaluate(sys_segmented_data,gold_segmented_data):\n",
    "    def boundaries(segmented):\n",
    "        token_lengths = [len(tok) for tok in segmented]\n",
    "        return set(np.cumsum(token_lengths)[:-1])\n",
    "\n",
    "    tp = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for sys_ex, (_, gold_ex) in zip(sys_segmented_data,\n",
    "                                    gold_segmented_data):\n",
    "        sys_bound = boundaries(sys_ex)\n",
    "        gold_bound = boundaries(gold_ex)\n",
    "        tp += len(sys_bound.intersection(gold_bound))\n",
    "        fp += len(sys_bound.difference(gold_bound))\n",
    "        fn += len(gold_bound.difference(sys_bound))\n",
    "    recall = 100 * tp / (tp + fn)\n",
    "    precision = 100 * tp / (tp + fp)\n",
    "    fscore = 2 * recall * precision / (recall + precision)\n",
    "\n",
    "    return precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"acupuncture's\", ['acupuncture', \"'s\"])\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import data_handling\n",
    "\n",
    "small_traindata = data_handling.read_data(open(path.join(\"data\", \"small_traindata\")))\n",
    "devdata = data_handling.read_data(open(path.join(\"data\", \"devdata\")))\n",
    "all_traindata = data_handling.read_data(open(path.join(\"data\", \"all_traindata\")))\n",
    "\n",
    "print(small_traindata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each segmented example is a pair `(word_form, segments)`, where `word_form` is a string and `segments` a list of morphemes as above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Implementing a segmentation system using `pycrfsuite`\n",
    "\n",
    "### Assignment 1.1: Training a tagger\n",
    "\n",
    "rubric={\"accuracy\":5}\n",
    "\n",
    "Your first task is to implement a training function `train_tagger` which trains a `pycrfsuite` tagger. Your function should take three parameters:\n",
    "\n",
    "* `traindata`, a list of training examples `(word_form, segments)`,\n",
    "* `epochs`, the number of epochs for the training algorithm, and\n",
    "* `fn` a file name where the trained tagger is stored\n",
    "\n",
    "Read the following [tutorial](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb) to see how a `pycrfsuite` tagger is trained. You can set the regularization parameter `c1` to `1.0` and `c2` to `1e-3`. You can set `feature.possible_transitions` to `False`. \n",
    "\n",
    "You can't use `traindata` directly to the `pycrfsuite.Trainer` class. You will first need to convert it into BIES (Begin-Inside-End-Singleton) format and then perform feature extraction. This can be accomplished using three functions which are provded to you in a `data_handling.py` which is provided with this notebook: \n",
    "\n",
    "* The first function `data_handling.get_bies_notation` takes a dataset as input and returns the same datset in BIES format. \n",
    "* The second function `data_handling.data2features` takes a dataset in BIES format and extracts features for each example (using a few simple feature functions).\n",
    "* The last function `data_handling.data2labels(bies)` takes a dataset in BIES format and returns the BIES labels for each example. \n",
    "\n",
    "Here is a small demonstration of using these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example:\t (\"acupuncture's\", ['acupuncture', \"'s\"])\n",
      "get_bies_notation:\t [[['a', 'BEGIN'], ['c', 'INSIDE'], ['u', 'INSIDE'], ['p', 'INSIDE'], ['u', 'INSIDE'], ['n', 'INSIDE'], ['c', 'INSIDE'], ['t', 'INSIDE'], ['u', 'INSIDE'], ['r', 'INSIDE'], ['e', 'END'], [\"'\", 'BEGIN'], ['s', 'END']]]\n",
      "data2features of bies:\t [[['CHAR=a', 'CHAR-1=<BD>', 'CHAR-2=<BD>', 'CHAR+1=c', 'CHAR+2=u', 'STR--=<BD><BD>a', 'STR++=acu', 'STR-+=<BD>ac', 'DIST_FROM_START=0'], ['CHAR=c', 'CHAR-1=a', 'CHAR-2=<BD>', 'CHAR+1=u', 'CHAR+2=p', 'STR--=<BD>ac', 'STR++=cup', 'STR-+=acu', 'DIST_FROM_START=1'], ['CHAR=u', 'CHAR-1=c', 'CHAR-2=a', 'CHAR+1=p', 'CHAR+2=u', 'STR--=acu', 'STR++=upu', 'STR-+=cup', 'DIST_FROM_START=2'], ['CHAR=p', 'CHAR-1=u', 'CHAR-2=c', 'CHAR+1=u', 'CHAR+2=n', 'STR--=cup', 'STR++=pun', 'STR-+=upu', 'DIST_FROM_START=3'], ['CHAR=u', 'CHAR-1=p', 'CHAR-2=u', 'CHAR+1=n', 'CHAR+2=c', 'STR--=upu', 'STR++=unc', 'STR-+=pun', 'DIST_FROM_START=4'], ['CHAR=n', 'CHAR-1=u', 'CHAR-2=p', 'CHAR+1=c', 'CHAR+2=t', 'STR--=pun', 'STR++=nct', 'STR-+=unc', 'DIST_FROM_START=5'], ['CHAR=c', 'CHAR-1=n', 'CHAR-2=u', 'CHAR+1=t', 'CHAR+2=u', 'STR--=unc', 'STR++=ctu', 'STR-+=nct', 'DIST_FROM_START=6'], ['CHAR=t', 'CHAR-1=c', 'CHAR-2=n', 'CHAR+1=u', 'CHAR+2=r', 'STR--=nct', 'STR++=tur', 'STR-+=ctu', 'DIST_FROM_START=7'], ['CHAR=u', 'CHAR-1=t', 'CHAR-2=c', 'CHAR+1=r', 'CHAR+2=e', 'STR--=ctu', 'STR++=ure', 'STR-+=tur', 'DIST_FROM_START=8'], ['CHAR=r', 'CHAR-1=u', 'CHAR-2=t', 'CHAR+1=e', \"CHAR+2='\", 'STR--=tur', \"STR++=re'\", 'STR-+=ure', 'DIST_FROM_START=9'], ['CHAR=e', 'CHAR-1=r', 'CHAR-2=u', \"CHAR+1='\", 'CHAR+2=s', 'STR--=ure', \"STR++=e's\", \"STR-+=re'\", 'DIST_FROM_START=10'], [\"CHAR='\", 'CHAR-1=e', 'CHAR-2=r', 'CHAR+1=s', 'CHAR+2=<BD>', \"STR--=re'\", \"STR++='s<BD>\", \"STR-+=e's\", 'DIST_FROM_START=11'], ['CHAR=s', \"CHAR-1='\", 'CHAR-2=e', 'CHAR+1=<BD>', 'CHAR+2=<BD>', \"STR--=e's\", 'STR++=s<BD><BD>', \"STR-+='s<BD>\", 'DIST_FROM_START=12']]]\n",
      "data2labels of bies:\t [['BEGIN', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'INSIDE', 'END', 'BEGIN', 'END']]\n",
      "unbies of bies:\t\t [[\"acupuncture's\", ['acupuncture', \"'s\"]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First training example:\\t\",small_traindata[0])\n",
    "bies = data_handling.get_bies_notation([small_traindata[0]])\n",
    "# list of list\n",
    "print(\"get_bies_notation:\\t\",bies)\n",
    "print(\"data2features of bies:\\t\",data_handling.data2features(bies))\n",
    "print(\"data2labels of bies:\\t\",data_handling.data2labels(bies))\n",
    "print(\"unbies of bies:\\t\\t\", data_handling.unbies(bies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "import data_handling\n",
    "\n",
    "def train_tagger(traindata,epochs,fn):\n",
    "    # # your code here\n",
    "    # https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb    \n",
    "    # 1. `data_handling.get_bies_notation`  --> bies\n",
    "    # 2. `data_handling.data2features`      --> X_train\n",
    "    # 3. `data_handling.data2labels(bies)`  --> y_train\n",
    "\n",
    "\n",
    "    # from `train model`: https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n",
    "    # trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    # for xseq, yseq in zip(X_train, y_train):\n",
    "    #     trainer.append(xseq, yseq)\n",
    "\n",
    "    # trainer.set_params({\n",
    "    #     'c1': 1.0,   # coefficient for L1 penalty\n",
    "    #     'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    #     'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    #     # include transitions that are possible, but not observed\n",
    "    #     'feature.possible_transitions': True\n",
    "    # })\n",
    "\n",
    "\n",
    "    # use `trainer.train(file_name)` for training\n",
    "\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a segmentation model on the small training set of 50 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagger(small_traindata,20,\"small_segmentation.model\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2: Segmenting data\n",
    "\n",
    "rubric={\"accuracy\":5}\n",
    "\n",
    "You will now implement a function `segment` which segments a dataset. The function takes two arguments:\n",
    "\n",
    "* `data` a dataset of examples in the same format which is returned by `data_handling.read_data`. The existing segmentations in `data` are ignored.\n",
    "* `fn` the file name of a stored segmentation model (trained using `train_tagger`)\n",
    "\n",
    "The `segment` function should return a list of pairs `(probability,segmentation)`, where `segmentation` is a list of morphemes like `[\"dog\",\"s\"]` and `probability` is the probability which the tagger gives to this segmentation. For example:\n",
    "\n",
    "```[(0.156, [\"dog\",\"s\"]), (0.223, [\"cat\",\"s\"]), ...]```\n",
    "\n",
    "You should first load the segmentation model from `fn`. Check the [tutorial](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb) to see how this is done. You should then convert `data` into BIES format (using `data_handling.get_bies_notation`) and extract features for each example (using `data_handling.data2features`). This gives you a list `data_features`.\n",
    "\n",
    "You should then iterate through the examples in `data_fatures` and segment each of them using the function [`pycrfsuite.Tagger.tag`](https://python-crfsuite.readthedocs.io/en/latest/pycrfsuite.html#pycrfsuite.Tagger.tag) which is a member function of the your segmentation model. This gives you the most likely BIES tags for each input example, for example: `[\"BEGIN\",\"INSIDE\",\"END\",\"SINGLE\"]`. \n",
    "\n",
    "You can use the function `pycrfsuite.Tagger.probability` to get the probability of the segmentation.\n",
    "\n",
    "You then need to transform each example into a list of pairs, for example:\n",
    "\n",
    "```[(\"d\",\"BEGIN\"),(\"o\",\"INSIDE\"),(\"g\",\"END\"),(\"s\",\"SINGLE\")]```\n",
    "\n",
    "You can then feed these to the the function `data_handling.unbies` which will return segmented word forms: `(\"dogs\",[\"dog\",\"s\"])`.\n",
    "\n",
    "Combine each of the segmentations with the appropriate probability and you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(data,fn):\n",
    "    # your code here\n",
    "    tagged_data = []\n",
    "    probs = []    \n",
    "\n",
    "    # from `make predictions`: https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n",
    "    # tagger = pycrfsuite.Tagger()\n",
    "    # tagger.open(fn)\n",
    "\n",
    "    # your data go through `get_bies_notation`  --> bies\n",
    "\n",
    "    # iterate `bies` by enumerating `data2features(bies)` which gives i, example\n",
    "    #   `tagger.tag`            --> append to `tagged_data` BUT...\n",
    "    #       [['s', 'BEGIN'], ['a', 'INSIDE'], ['l', 'INSIDE'], ['e', 'END'], ['s', 'SINGLE']]     <-- bies[i]\n",
    "    #       ['BEGIN', 'INSIDE', 'INSIDE', 'END', 'SINGLE']                                        <-- tags\n",
    "    \n",
    "\n",
    "    #    [('s', 'BEGIN'), ('a', 'INSIDE'), ('l', 'INSIDE'), ('e', 'END'), ('s', 'SINGLE')]         --> append to `tagged_data`\n",
    "    #   `tagger.probability`                                                                       --> append to `probs`\n",
    "\n",
    "    # Then, return following:\n",
    "    # [ \n",
    "    #   (   0.156,                  <--- probs\n",
    "    #       [\"dog\",\"s\"] ),          <--- by `unbies`` of `tagged_data`\n",
    "    #   (   0.223, [\"cat\",\"s\"]), \n",
    "    #   ... \n",
    "    # ]\n",
    "    return ...\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to evaluate a segmentation system, we don't need the probability for each segmentation. The following function simply throws away all the probabilites from the return value of `segment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentations(data,fn):\n",
    "    tokenized_data = [segmented for _,segmented in segment(data,fn)]\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the initial model trained on 50 examples using the function `data_handling.evaluate`. The f-score should be 56%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Development set precision: 73.05, recall: 45.58, f-score: 56.13\n"
     ]
    }
   ],
   "source": [
    "tokenized_dev = get_segmentations(devdata,\"small_segmentation.model\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % data_handling.evaluate(tokenized_dev,devdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: Annotating additional examples randomly\n",
    "\n",
    "rubric={\"accuracy\":5}\n",
    "\n",
    "You will now investigate the average improvement which can be gained by adding a number of additional training examples to `small_traindata`. This will be a baseline system where we randomly sample additional training examples from the large `all_traindata` dataset.\n",
    "\n",
    "Start by implementing the function `augment_data_randomly`. It takes three arguments:\n",
    "\n",
    "* `small_data` a small dataset of training example where we will add new examples,\n",
    "* `big_data` a large dataset where we will sample examples which are added to `small_data`, and\n",
    "* `n` the number of examples which are added to `small_data`.\n",
    "\n",
    "The function should return `augmented_data` which contains all examples in `small_data` together with the randomly sampled examples from `big_data`.\n",
    "\n",
    "You should sample `n` examples from `big_data` **without replacement**. This means that you should not add the same training example twice. You can use functions from the Python module [`random`](https://docs.python.org/3/library/random.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_data_randomly(small_data, big_data, n):\n",
    "    # your code here\n",
    "    augmented_data = small_data + # use `random.sample` for `big_data` with `n`\n",
    "    return augmented_data\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the average improvement which can be accomplished by adding random examples to `small_traindata`, you should train and evaluate `TRIALS` (= 20) segmentation systems which are trained on `small_traindata` augmented by `ADDED_EXAMPLES` (= 50) randomly sampled training examples from `all_traindata`. You can train each segmentation system for 20 epochs.\n",
    "\n",
    "Evaluate each of the taggers on `devdata` and store its f-score in the list `f-scores`. You can evaluate the segmentation performance using the function `data_handling.evaluate`. See the previous assignment for an example on how to call `data_handling.evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIALS=20\n",
    "ADDED_EXAMPLES=50\n",
    "\n",
    "fscores = []\n",
    "\n",
    "for i in range(TRIALS):\n",
    "    # # your score here\n",
    "    # 1. `augment_data_randomly`\n",
    "    # 2. `train_tagger`\n",
    "    # 3. `get_segmentations` with `devdata`\n",
    "    # 4. `evaluate`\n",
    "    # 5. `append` fscore to `fscores`\n",
    "    \n",
    "    # your score here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compute the mean and confidence interval (at the 99% level) for the the performance of the segmentaiton systems. Your mean should be around 67%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 65.02 Confidence interval at 99% level: [62.99, 67.06]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def mean_and_confidence_interval(data, confidence):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "mean, lower_boundary, upper_boundary = mean_and_confidence_interval(fscores,0.99)\n",
    "print(\"Mean: %.2f Confidence interval at 99%% level: [%.2f, %.2f]\" % (mean, lower_boundary, upper_boundary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Query-by-uncertainty\n",
    "\n",
    "You will now implement active learning. You'll investigate three strategies for choosing additional training examples from the large trainingset `all_traindata`. In each case, we choose an example to add the `small_traindata` using our existing segmentation system, then retrain our segmentation system, choose another example, retrain again, and so on. \n",
    "\n",
    "### Assignment 3.1: Least confidence strategy\n",
    "\n",
    "rubric={\"accuracy\":5}\n",
    "\n",
    "Our first active learning strategy will segment all words in `all_traindata` and find the word which results in the the least confident segmentation. This is measured by the segmentation probability $p(y|x)$, where $y$ is the segmentation and `x` is the input word. The function has two modes. If the parameter `length_normalize` is true, then it will return the example which maximizes the normalized probability $p(y|x)^{1/N}$, where $N$ is the length of the word form $x$. Otherwise, the function returns $p(y|x)$\n",
    "\n",
    "You should start by implementing the function `least_confident_example`. It finds the word form in `data` for which our segmentation system is maximally unsure about the segmentation. The function takes four arguments:\n",
    "\n",
    "* `data` a dataset of examples in the format returned by `data_handling.read_data`,\n",
    "* `skip_words` a set of words which should be filtered out when finding the output word of the function (there are words which either belong to `small_trainset` or have already been added),\n",
    "* `fn` the file name for a segmentation model,\n",
    "* `length_normalize` a boolean which tells us whether to normalize probabilities by length or not.\n",
    "\n",
    "You should start by calling `segment` on the input dataset `data`. Use the segmentation system stored in `fn`. This will give you the segmentation returned by our segmentation system and its probability for each example in `data`. If `length_normalize` ir `True`, you should normalize those probabilities by appying the transformation $p \\mapsto p^{1/N}$, where $N$ is the length od the input form.  \n",
    "\n",
    "Return a pair `(probability, example)`, where `example` is the example in `data` which gets the lowest segmentation probability, for example `(\"dogs\",[\"dog\",\"s\"])` and `probability` is the segmentation probability (possibly normalized by length if). Note that the segmentation `[\"dog\",\"s\"]` here is **the gold standard segmentation in `data`**. Not the segmentation given by our model!\n",
    "\n",
    "**Note that you should make sure that you don't return a word occurring in `skip_words`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confident_example(\n",
    "        data,\n",
    "        skip_words,                 # list of words\n",
    "        fn,\n",
    "        length_normalize):\n",
    "    \n",
    "    # # your code here\n",
    "    # 1. get  `confidences` using `segment`\n",
    "    confidences = ...\n",
    "    # 2. create a tuple of `(prob, ex)`\n",
    "    # 3. if normalize, then normalize ...\n",
    "    # 4. `sort()` confidences \n",
    "    # 5. then, return the first one (the least confident example)\n",
    "\n",
    "    return confidences[0]\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a new training set `active_traindata` which is initialized to the examples in `small_traindata`. You will then alternate between training a segmentation model stored in a file `active_segmentation.model` and adding training examples to `active_traindata`. Each time you will call `least_confident_example` to find the next training example to add to `active_traindata`. \n",
    "\n",
    "You should make sure that you never add the same word form twice to `active_traindata` by setting the `skip_words` parameter for `least_confident_example`. \n",
    "\n",
    "First you will run `least_confident_example` with `length_normalize = False`. This should give you an improvement over training on `small_traindata` alone but you will probably not beat `augment_data_randomly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query-by-uncertainty using least_confident_example without length normalization:\n",
      "Results:\n",
      "Development set precision: 88.71, recall: 48.67, f-score: 62.86\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "ADDED_EXAMPLES=50\n",
    "\n",
    "print(\"Query-by-uncertainty using least_confident_example without length normalization:\")\n",
    "active_traindata = deepcopy(small_traindata) \n",
    "\n",
    "# print(active_traindata)\n",
    "\n",
    "for i in range(ADDED_EXAMPLES):\n",
    "    # # your code here\n",
    "\n",
    "    # 1. `train_tagger` using `active_traindata`\n",
    "    # 2. `least_confident_example`\n",
    "    # 3. `append` example to `active_traindata`\n",
    "    # your code here\n",
    "\n",
    "\n",
    "train_tagger(active_traindata,20,\"active_segmentation.model\")    \n",
    "tokenized_dev = get_segmentations(devdata,\"active_segmentation.model\")\n",
    "print(\"Results:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % data_handling.evaluate(tokenized_dev,devdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then run `least_confident_example` with `length_normalize = True`. This should give you a clear improvement over `augment_data_randomly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query-by-uncertainty using least_confident_example with length normalization:\n",
      "Results for supervised segmentation:\n",
      "Development set precision: 90.85, recall: 57.08, f-score: 70.11\n"
     ]
    }
   ],
   "source": [
    "print(\"Query-by-uncertainty using least_confident_example with length normalization:\")\n",
    "active_traindata = deepcopy(small_traindata)\n",
    "\n",
    "for i in range(ADDED_EXAMPLES):\n",
    "    # your code here\n",
    "    \n",
    "    # SAME as before except for `length_normalize` = True\n",
    "\n",
    "    # your code here\n",
    "\n",
    "train_tagger(active_traindata,20,\"active_segmentation.model\")    \n",
    "tokenized_dev = get_segmentations(devdata,\"active_segmentation.model\")\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % data_handling.evaluate(tokenized_dev,devdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.2: Token entropy (optional)\n",
    "\n",
    "rubric={\"accuracy\":5}\n",
    "\n",
    "Our second active learning strategy will segment all words in `all_traindata` and find the word which results in the the segmentation with the highest *token entropy*. This is computed using the marginal probabilities $p(y_i = tag|x)$ of the BIES tag $tag$ at position $i$ in the input example $x$. The token entropy is computed as\n",
    "\n",
    "$$\\sum_{i = 0}^n \\sum_{tag \\in \\{B,I,E,S\\}} p(y_i = tag | x) \\cdot -\\log p(y_i=tag | x)$$\n",
    "\n",
    "You should implement a function `entropy_maximizing_example` which takes three arguments:\n",
    "\n",
    "* `data` a dataset of examples in the format returned by `data_handling.read_data`,\n",
    "* `skip_words` a set of words which should be filtered our when finding the output word of the function (there are words which either belong to `small_trainset` or have already been added), and\n",
    "* `fn` the file name for a segmentation model.\n",
    "\n",
    "You should start by extracting features for the examples in `data` using `data_handling.get_bies_notation` and `data_handling.data2features`. \n",
    "\n",
    "The function [`pycrfsuite.Tagger.marginal`](https://python-crfsuite.readthedocs.io/en/latest/pycrfsuite.html#pycrfsuite.Tagger.marginal) gives you the marginal probability $p(y_i=tag | x)$ at position `i` in an input example $x$. You will need to call either `pycrfsuite.Tagger.tag` or `pycrfsuite.Tagger.set` before `marginal` in order to initialize the tagger to the input example `x`. \n",
    "\n",
    "You should sum $p(y_i = tag | x) \\cdot -\\log p(y_i=tag | x)$ for each tag and position in the example and normalize by dividing with the length of the example $x$. Looping over each example in `data` will give you a list `marginals` which contains pairs `(entropy, example)` where `example` is an example like `(\"dogs\",[\"dog\",\"s\"])` and `entropy` is the token entropy for this example. You should return the pair where the token entropy is maximal. Note that the segmentation `[\"dog\",\"s\"]` here is **the gold standard segmentation in `data`**. Not the segmentation given by our model!   \n",
    "\n",
    "**Note that you should make sure that you don't return a word occurring in `skip_words`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from data_handling import BEGIN, INSIDE, END, SINGLE\n",
    "\n",
    "TAGS = [BEGIN, INSIDE, END, SINGLE]\n",
    "\n",
    "def entropy_maximizing_example(data,skip_words,fn):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "   return entropies[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then run `entropy_maximizing_example` with `length_normalize = True`. You should get a slight improvement over `augment_data_randomly`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query-by-uncertainty using entropy_mazimizing_example with length normalization:\n",
      "Results for supervised segmentation:\n",
      "Development set precision: 89.21, recall: 54.87, f-score: 67.95\n"
     ]
    }
   ],
   "source": [
    "active_traindata = deepcopy(small_traindata)\n",
    "\n",
    "print(\"Query-by-uncertainty using entropy_mazimizing_example with length normalization:\")\n",
    "for i in range(ADDED_EXAMPLES):\n",
    "    # your code here\n",
    "    ...\n",
    "    # your code here\n",
    "\n",
    "train_tagger(active_traindata,20,\"active_segmentation.model\")    \n",
    "tokenized_dev = get_segmentations(devdata,\"active_segmentation.model\")\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % data_handling.evaluate(tokenized_dev,devdata))\n",
    "\n",
    "# Query-by-uncertainty using entropy_mazimizing_example with length normalization:\n",
    "# Results for supervised segmentation:\n",
    "# Development set precision: 81.51, recall: 42.92, f-score: 56.23\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4: Query-by-committee\n",
    "\n",
    "rubric={\"accuracy\":7}\n",
    "\n",
    "Our final active learning strategy is a version of the query-by-committee strategy. Here we train a number of models on subsets of our original training set. We will then add examples to the training set based on how strongly the models agree on the segmentation for the examples. We want to add examples where there is maximal disagreement between the models beacues these are likely to be examples which our training data doesn't sufficiently cover.\n",
    "\n",
    "You should start by implementing a function `train_committee` which trains `COMMITTEE_SIZE` models (defined below). Each model is trained on a 70% subset of the input dataset `traindata`. Every model is trained for `epoch` epochs and is stored in a file `fn_prefix.n` where `fn_prefix` is given as argument to `train_committee` and `n` is an index number between `0` and `COMMITTEE_SIZE`. You should sample examples from traindata **without replacement** which means that the same training example should not occur twice in the sampled set. You can use functions from the Python module `random` to sample training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "COMMITTEE_SIZE=20\n",
    "\n",
    "def train_committee(traindata,epochs,fn_prefix):\n",
    "    # your code here\n",
    "    for i in range(COMMITTEE_SIZE):\n",
    "        # 1. get 70% of sample data from `traindata`\n",
    "        # 2. `train_tagger` and save it as model_i\n",
    "        \n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now implement a function `most_controversial_example`. It takes three arguments:\n",
    "\n",
    "* `data` a dataset of examples in the format returned by `data_handling.read_data`,\n",
    "* `skip_words` a set of words which should be filtered our when finding the output word of the function (there are words which either belong to `small_trainset` or have already been added), and\n",
    "* `fn_prefix` the file name prefix for the segmentation models belonging to a committee.\n",
    "\n",
    "The function will segment the dataset `data` using each of the segmentation models belonging to a committee. It will then return the example in `data` which results in the largest disagreement among the models in the committee. To measure disagreement, we will use *vote entropy* which is defined in the following way: Let $V(tag|i)$ be the number of models which predict BIES tag $tag$ at position $i$ in the input example, then the vote entropy is given by\n",
    "\n",
    "$$ -\\frac{1}{T}\\cdot \\sum_{i=0}^N \\sum_{tag \\in \\{B,I,E,S\\}} \\frac{V(tag,i) + \\alpha}{C + 4 \\cdot \\alpha} \\cdot -\\log \\frac{V(tag,i) + \\alpha}{C + 4\\cdot \\alpha},$$\n",
    "\n",
    "where $C$ is the number of models which belong to the committee and $\\alpha$ is a small smoothing constant which we set to `0.1`.\n",
    "\n",
    "You should start by segmenting `data` using each of the segmentation models in the committee stored in the model files `fn_prefix.0`,  ..., `fn_prefix.COMMITTEE_SIZE-1`. This will give you a list `committee_segmented` with `COMMITTEE_SIZE` elements.\n",
    "\n",
    "You should then use this list to compute the vote entropy for each example in `data`. Looping over each example in `data` will give you a list `vote_entropies` which contains pairs `(entropy, example)` where `example` is an example like `(\"dogs\",[\"dog\",\"s\"])` and `entropy` is the vote entropy for this example. You should return the pair where the vote entropy is maximal. Note that the segmentation `[\"dog\",\"s\"]` here is **the gold standard segmentation in `data`**. Not the segmentation given by our model!   \n",
    "\n",
    "**Note that you should make sure that you don't return a word occurring in `skip_words`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from data_handling import BEGIN, INSIDE, END, SINGLE\n",
    "from math import log\n",
    "\n",
    "TAGS = [BEGIN, INSIDE, END, SINGLE]\n",
    "\n",
    "def most_controversial_example(data,skip_words,fn_prefix):\n",
    "    # your code here\n",
    "    committee_segmented = []\n",
    "    for i in range(COMMITTEE_SIZE):\n",
    "        # 1. `get_segmentations` through models\n",
    "        segmentations =  ...                            # --> ['neg', 'ative']\n",
    "        # 2. get `segmented_data` with `data`           \n",
    "        segmented_data = ...                            # --> ('negative', ['neg', 'ative'])\n",
    "        # 3. `get_bies_notation` of `segmented_data`\n",
    "        bies_segmentations = ...\n",
    "        # 4. `append` it to `committee_segmented` \n",
    "        ...\n",
    "\n",
    "        # therefore, `committee_segmented` is a list of the list of segmentations\n",
    "\n",
    "    vote_entropies = []\n",
    "    for i, ex in enumerate(data):\n",
    "        if not ex[0] in skip_words:\n",
    "            # 1. initialize tag_counts with 0.1\n",
    "            tag_counts = ...        \n",
    "            # for a word `n e g a t i v e`\n",
    "            # tag_counts should be as follows:\n",
    "            # [{'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- n\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- e\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- g\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- a\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- t\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- i\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1},    <- v\n",
    "            #  {'BEGIN': 0.1, 'INSIDE': 0.1, 'END': 0.1, 'SINGLE': 0.1}]    <- e\n",
    "\n",
    "            # 2. then update (+1) `tag_counts` by iterating  `COMMITTEE_SIZE`\n",
    "            for j in range(COMMITTEE_SIZE):\n",
    "                for k in range(len(ex[0])):\n",
    "                    tag_counts[k][...] += 1\n",
    "            \n",
    "            # 3. calculate probs (entropy)\n",
    "            probs = ...\n",
    "            # [(\n",
    "            #   0.44075178913300456,            <- probs\n",
    "            #   ('negative', ['negat', 'ive'])  <- ex   \n",
    "            # )]                                                <-- vote_entropies \n",
    "            vote_entropies.append(...)\n",
    "\n",
    "    vote_entropies.sort(reverse=True)\n",
    "    return vote_entropies[0]\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then run `most_controversial_example` with `length_normalize = True`. You should get a clear improvement over `augment_data_randomly`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query-by-committee:\n",
      "Results for supervised segmentation:\n",
      "Development set precision: 90.07, recall: 60.18, f-score: 72.15\n"
     ]
    }
   ],
   "source": [
    "active_traindata = deepcopy(small_traindata)\n",
    "\n",
    "print(\"Query-by-committee:\")\n",
    "for i in range(ADDED_EXAMPLES):\n",
    "    # your code here\n",
    "    # 1. train_committee\n",
    "    # 2. get `most_controversial_example` \n",
    "    # 3. append example \n",
    "\n",
    "    # your code here\n",
    "\n",
    "train_tagger(active_traindata,20,\"active_segmentation.model\")    \n",
    "tokenized_dev = get_segmentations(devdata,\"active_segmentation.model\")\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % data_handling.evaluate(tokenized_dev,devdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
