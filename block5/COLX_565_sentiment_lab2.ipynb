{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 565 Lab Assignment 2: Polarity and Aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will\n",
    "- Perform Exploratory analysis on a multi-tagged corpus for emotion and sentiment (Exercise 1)\n",
    "- Create a CNN-based classifier to predict sentiment from text (Exercise 2)\n",
    "- Create a multi-task learner that tries to jointly learn sentiment and emotion (Exercise 3)\n",
    "- Add on to the classifier using speech-extracted features. (Exercise 4)\n",
    "- Create an ensemble that combines the previously-trained models to predict sentiment and emotion. (Exercise 5 - Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to access relevant modules (you can add to this as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "from nltk import word_tokenize\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "###from torchtext.data import Field, RawField, NestedField, TabularDataset,BucketIterator\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchtext.vocab import Vocab\n",
    "from torchdata.datapipes.iter import IterableWrapper, FileOpener\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the marks for tidy submission:\n",
    "\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)\n",
    "- The corpus will be provided to you - don't include it in your submission. Modify the path below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring the MELD Corpus\n",
    "rubric={accuracy:3, viz:1, reasoning:1}\n",
    "\n",
    "**MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations (https://aclanthology.org/P19-1050) (Poria et al., *ACL 2019*)**\n",
    "\n",
    "![MELD](https://affective-meld.github.io/emotion_shift.jpeg \"MELD\")\n",
    "\n",
    "\n",
    "The MELD (Multimodal EmotionLines Dataset) is a corpus of short video snippets from the TV series \"Friends\".  Each line has been annotated with the speaker (one of about 250 different characters - although most of them only speak a few times), whether the sentiment in the line is positive, negative, or neutral, and the emotion carried in the line (one of 7 emotions: neutral, surprise, joy, anger, sadness, fear, or disgust).\n",
    "\n",
    "I've extracted the audio from the video files (if you're interested in how to do so, let me know), and provided you with both the annotations (which will serve as our \"y\" variables), and the sound and text data, which will serve as our \"x\" data.  Don't worry about the audio data at this point - just concentrate on the text data included in the csvs.\n",
    "\n",
    "We're first going to do some exploratory data analysis (EDA).  Load up the training data, and provide some information regarding interesting statistics.  You should calculate and graph:\n",
    "\n",
    "* the number of times that each of the 6 main characters (Monica, Chandler, Ross, Rachel, Phoebe, and Joey) speak.\n",
    "* what percentage of each of their speech is positive / neutral / negative\n",
    "* what percentage of each of their speech is associated with each of the 7 emotions\n",
    "\n",
    "To get full marks, you must also provide a short (one paragraph) written analysis of what you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MELD dataset: https://github.com/declare-lab/MELD/tree/master/data/MELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-26 20:41:12--  https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/train_sent_emo.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1119290 (1,1M) [text/plain]\n",
      "Saving to: ‘train_sent_emo.csv’\n",
      "\n",
      "train_sent_emo.csv  100%[===================>]   1,07M  7,07MB/s    in 0,2s    \n",
      "\n",
      "2024-02-26 20:41:12 (7,07 MB/s) - ‘train_sent_emo.csv’ saved [1119290/1119290]\n",
      "\n",
      "--2024-02-26 20:41:12--  https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/dev_sent_emo.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121640 (119K) [text/plain]\n",
      "Saving to: ‘dev_sent_emo.csv’\n",
      "\n",
      "dev_sent_emo.csv    100%[===================>] 118,79K  --.-KB/s    in 0,02s   \n",
      "\n",
      "2024-02-26 20:41:13 (5,29 MB/s) - ‘dev_sent_emo.csv’ saved [121640/121640]\n",
      "\n",
      "--2024-02-26 20:41:13--  https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/test_sent_emo.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 294526 (288K) [text/plain]\n",
      "Saving to: ‘test_sent_emo.csv’\n",
      "\n",
      "test_sent_emo.csv   100%[===================>] 287,62K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2024-02-26 20:41:13 (6,18 MB/s) - ‘test_sent_emo.csv’ saved [294526/294526]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/train_sent_emo.csv\n",
    "!wget https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/dev_sent_emo.csv\n",
    "!wget https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/test_sent_emo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "data_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>10474</td>\n",
       "      <td>You or me?</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:48,173</td>\n",
       "      <td>00:00:50,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>10475</td>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:51,009</td>\n",
       "      <td>00:00:53,594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>10476</td>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>Joey</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:00,518</td>\n",
       "      <td>00:01:03,520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>10477</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>All</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:05,398</td>\n",
       "      <td>00:01:07,274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>10478</td>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:08,401</td>\n",
       "      <td>00:01:12,071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sr No.                                          Utterance  \\\n",
       "0          1  also I was the point person on my company’s tr...   \n",
       "1          2                   You must’ve had your hands full.   \n",
       "2          3                            That I did. That I did.   \n",
       "3          4      So let’s talk a little bit about your duties.   \n",
       "4          5                             My duties?  All right.   \n",
       "...      ...                                                ...   \n",
       "9984   10474                                         You or me?   \n",
       "9985   10475  I got it. Uh, Joey, women don't have Adam's ap...   \n",
       "9986   10476               You guys are messing with me, right?   \n",
       "9987   10477                                              Yeah.   \n",
       "9988   10478  That was a good one. For a second there, I was...   \n",
       "\n",
       "              Speaker   Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  \\\n",
       "0            Chandler   neutral   neutral            0             0       8   \n",
       "1     The Interviewer   neutral   neutral            0             1       8   \n",
       "2            Chandler   neutral   neutral            0             2       8   \n",
       "3     The Interviewer   neutral   neutral            0             3       8   \n",
       "4            Chandler  surprise  positive            0             4       8   \n",
       "...               ...       ...       ...          ...           ...     ...   \n",
       "9984         Chandler   neutral   neutral         1038            13       2   \n",
       "9985             Ross   neutral   neutral         1038            14       2   \n",
       "9986             Joey  surprise  positive         1038            15       2   \n",
       "9987              All   neutral   neutral         1038            16       2   \n",
       "9988             Joey       joy  positive         1038            17       2   \n",
       "\n",
       "      Episode     StartTime       EndTime  \n",
       "0          21  00:16:16,059  00:16:21,731  \n",
       "1          21  00:16:21,940  00:16:23,442  \n",
       "2          21  00:16:23,442  00:16:26,389  \n",
       "3          21  00:16:26,820  00:16:29,572  \n",
       "4          21  00:16:34,452  00:16:40,917  \n",
       "...       ...           ...           ...  \n",
       "9984        3  00:00:48,173  00:00:50,799  \n",
       "9985        3  00:00:51,009  00:00:53,594  \n",
       "9986        3  00:01:00,518  00:01:03,520  \n",
       "9987        3  00:01:05,398  00:01:07,274  \n",
       "9988        3  00:01:08,401  00:01:12,071  \n",
       "\n",
       "[9989 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "train_path = data_dir + \"train_sent_emo.csv\"\n",
    "train_data = pandas.read_csv(train_path)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sentence length (chars): 40.52838121934128\n",
      "average sentence length (words): 8.03543898288117\n",
      "number of utterances per f.r.i.e.n.d.s: [0.18167589694196967, 0.1756561521791476, 0.172766674692993, 0.15904165663375872, 0.15639296893811702, 0.15446665061401396]\n",
      "f.r.i.e.n.d.s: ['Joey', 'Ross', 'Rachel', 'Phoebe', 'Monica', 'Chandler']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive, negative, neutral = red, blue, green\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surprise, fear, sadness, neutral, joy, anger, disgust = purple, yellow, blue, black, orange, red, green\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHMCAYAAAAH0Kh7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7vklEQVR4nO3deVxV1f7/8TegghM4XUERI9Mc0kTFAXNIw9Rrg1NRmRo5VJZp/PJ6bXC8hqaZlZapmamZWjlkdtHCvFaSJkTlVGrlDDiCoDKd9fvDLyfJERzWOfp6Ph77Ue6zzzkf1mOfs99n7bXX9jDGGAEAAFjiabsAAABwYyOMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMCqIrYLuBQOh0P79+9X6dKl5eHhYbscAABwCYwxOn78uCpXrixPz/P3f7hFGNm/f7+CgoJslwEAAAphz549qlKlynkfd4swUrp0aUmn/xhfX1/L1QAAgEuRlpamoKAg53H8fNwijOSdmvH19SWMAADgZi42xIIBrAAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwqVBiZOnWqgoOD5ePjo6ZNm2rDhg3n3Xb27Nny8PDIt/j4+BS6YAAAcH0pcBhZuHChoqKiNGLECCUkJKh+/fpq3769UlJSzvscX19fHThwwLns2rXrsooGAADXjwKHkUmTJqlfv36KjIxUnTp1NG3aNJUoUUKzZs0673M8PDwUEBDgXPz9/S+raAAAcP0oUBjJyspSfHy8wsPD/3oBT0+Fh4crLi7uvM9LT0/XTTfdpKCgIN1///3avHnzBd8nMzNTaWlp+RYAAHB9KlAYOXTokHJzc8/q2fD391dSUtI5n1OzZk3NmjVLy5Yt07x58+RwONS8eXPt3bv3vO8THR0tPz8/5xIUFFSQMgEAcDt/H1955nK9u+pX04SFhalXr14KCQlR69attXjxYv3jH//Qu+++e97nDBs2TKmpqc5lz549V7tMAABgSZGCbFyhQgV5eXkpOTk53/rk5GQFBARc0msULVpUDRo00I4dO867jbe3t7y9vQtSGgAAcFMF6hkpVqyYGjVqpNjYWOc6h8Oh2NhYhYWFXdJr5Obm6pdfflGlSpUKVikAALguFahnRJKioqLUu3dvhYaGqkmTJpo8ebIyMjIUGRkpSerVq5cCAwMVHR0tSRo9erSaNWum6tWr69ixY5owYYJ27dqlvn37Xtm/BAAAuKUCh5GIiAgdPHhQw4cPV1JSkkJCQhQTE+Mc1Lp79255ev7V4XL06FH169dPSUlJKlu2rBo1aqR169apTp06V+6vwHXpYoO2jDHXqBIAwNXkYdzgGz0tLU1+fn5KTU2Vr6+v7XJwjRBGANxILvSd567fd5d6/ObeNAAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAq4rYLuB65+HhccHHjTHXqBIAAFwTPSMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwiungAeAM3MIBuPboGQEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFVFbBcAALg+eHh4XPBxY8w1qgTuhp4RAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBUzsALXIWbCBOBO6BkBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYFWhwsjUqVMVHBwsHx8fNW3aVBs2bLik5y1YsEAeHh7q3LlzYd4WAABchwocRhYuXKioqCiNGDFCCQkJql+/vtq3b6+UlJQLPu/PP//U888/r5YtWxa6WAAAcP0pcBiZNGmS+vXrp8jISNWpU0fTpk1TiRIlNGvWrPM+Jzc3Vz169NCoUaNUrVq1i75HZmam0tLS8i0AAOD6VKAwkpWVpfj4eIWHh//1Ap6eCg8PV1xc3HmfN3r0aFWsWFF9+vS5pPeJjo6Wn5+fcwkKCipImQAAwI0UKIwcOnRIubm58vf3z7fe399fSUlJ53zOt99+q/fee08zZsy45PcZNmyYUlNTncuePXsKUiYAAHAjV/VGecePH1fPnj01Y8YMVahQ4ZKf5+3tLW9v76tYGQAAcBUFCiMVKlSQl5eXkpOT861PTk5WQEDAWdvv3LlTf/75p+69917nOofDcfqNixTRr7/+qltuuaUwdQMAgOtEgU7TFCtWTI0aNVJsbKxzncPhUGxsrMLCws7avlatWvrll1+UmJjoXO677z61adNGiYmJjAUBAAAFP00TFRWl3r17KzQ0VE2aNNHkyZOVkZGhyMhISVKvXr0UGBio6Oho+fj4qG7duvmeX6ZMGUk6az0AALgxFTiMRERE6ODBgxo+fLiSkpIUEhKimJgY56DW3bt3y9OTiV0BAMCl8TDGGNtFXExaWpr8/PyUmpoqX19f2+UUiIeHxwUfd4Pmt4a2KzzarvBou8Kj7S7PhdrPXdvuUo/fdGEAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwqlBhZOrUqQoODpaPj4+aNm2qDRs2nHfbxYsXKzQ0VGXKlFHJkiUVEhKiuXPnFrpgAABwfSlwGFm4cKGioqI0YsQIJSQkqH79+mrfvr1SUlLOuX25cuX04osvKi4uTj///LMiIyMVGRmplStXXnbxAADA/XkYY0xBntC0aVM1btxYU6ZMkSQ5HA4FBQVp4MCB+ve//31Jr9GwYUN16tRJY8aMuaTt09LS5Ofnp9TUVPn6+hakXOs8PDwu+HgBm/+GQtsVHm1XeLRd4dF2l+dC7eeubXepx+8C9YxkZWUpPj5e4eHhf72Ap6fCw8MVFxd30ecbYxQbG6tff/1VrVq1Ou92mZmZSktLy7cAAIDrU4HCyKFDh5Sbmyt/f/986/39/ZWUlHTe56WmpqpUqVIqVqyYOnXqpLfeekvt2rU77/bR0dHy8/NzLkFBQQUpEwAAuJFrcjVN6dKllZiYqB9++EFjx45VVFSU1qxZc97thw0bptTUVOeyZ8+ea1EmAACwoEhBNq5QoYK8vLyUnJycb31ycrICAgLO+zxPT09Vr15dkhQSEqKtW7cqOjpad9555zm39/b2lre3d0FKAwAAbqpAPSPFihVTo0aNFBsb61zncDgUGxursLCwS34dh8OhzMzMgrw1AAC4ThWoZ0SSoqKi1Lt3b4WGhqpJkyaaPHmyMjIyFBkZKUnq1auXAgMDFR0dLen0+I/Q0FDdcsstyszM1BdffKG5c+fqnXfeubJ/CQAAcEsFDiMRERE6ePCghg8frqSkJIWEhCgmJsY5qHX37t3y9PyrwyUjI0MDBgzQ3r17Vbx4cdWqVUvz5s1TRETElfsrAACA2yrwPCM2MM/IjYm2KzzarvBou8Kj7S4P84wAAABYQhgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYUKI1OnTlVwcLB8fHzUtGlTbdiw4bzbzpgxQy1btlTZsmVVtmxZhYeHX3B7AABwYylwGFm4cKGioqI0YsQIJSQkqH79+mrfvr1SUlLOuf2aNWv08MMP6+uvv1ZcXJyCgoJ09913a9++fZddPAAAcH8exhhTkCc0bdpUjRs31pQpUyRJDodDQUFBGjhwoP79739f9Pm5ubkqW7aspkyZol69el3Se6alpcnPz0+pqany9fUtSLnWeXh4XPDxAjb/DYW2KzzarvBou8Kj7S7PhdrPXdvuUo/fBeoZycrKUnx8vMLDw/96AU9PhYeHKy4u7pJe48SJE8rOzla5cuXOu01mZqbS0tLyLQAA4PpUoDBy6NAh5ebmyt/fP996f39/JSUlXdJrDB06VJUrV84XaP4uOjpafn5+ziUoKKggZQIAADdyTa+mGTdunBYsWKAlS5bIx8fnvNsNGzZMqampzmXPnj3XsEoAAHAtFSnIxhUqVJCXl5eSk5PzrU9OTlZAQMAFnztx4kSNGzdOX331lW6//fYLbuvt7S1vb++ClAYAANxUgXpGihUrpkaNGik2Nta5zuFwKDY2VmFhYed93quvvqoxY8YoJiZGoaGhha8WAABcdwrUMyJJUVFR6t27t0JDQ9WkSRNNnjxZGRkZioyMlCT16tVLgYGBio6OliSNHz9ew4cP1/z58xUcHOwcW1KqVCmVKlXqCv4pAADAHRU4jEREROjgwYMaPny4kpKSFBISopiYGOeg1t27d8vT868Ol3feeUdZWVnq3r17vtcZMWKERo4ceXnVAwAAt1fgeUZsYJ6RGxNtV3i0XeHRdoVH210e5hkBAACwhDACAACsIowAAACrCCMAAMCqAl9Ngyts/oUHfOkR9xy0BABn4fsO50HPCAAAsIowAgAArOI0DdwXXb4AcF2gZwQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWMXVNABQEFzFBVxx9IwAAACr6BkBbkT8ugfgQugZAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFjFpGcAALi663yiQnpGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFjFPCOABaM8Rl3w8RFmxDWqBADso2cEAABYRRgBAABWEUYAAIBVhBEAAGAVA1gB4AbC4Gm4InpGAACAVYQRAABgFWEEAABYRRgBAABW3fADWBnMBeCK8vC48OPGXJs6ADdCzwgAALCKMAIAAKwijAAAAKsIIwAAwKobfgArAMBFMPj3hkUYAeB2LnQVHFfAAe6H0zQAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCqupgEA4BJwL7OrhzDi6rjuHjaw3wG4hggjuH5xQAUAt8CYEQAAYBVhBAAAWEUYAQAAVhUqjEydOlXBwcHy8fFR06ZNtWHDhvNuu3nzZnXr1k3BwcHy8PDQ5MmTC1srAAC4DhU4jCxcuFBRUVEaMWKEEhISVL9+fbVv314pKSnn3P7EiROqVq2axo0bp4CAgMsuGMDpsbkXWgDAnRQ4jEyaNEn9+vVTZGSk6tSpo2nTpqlEiRKaNWvWObdv3LixJkyYoIceekje3t6X9B6ZmZlKS0vLtwAAgPNw818oBQojWVlZio+PV3h4+F8v4Omp8PBwxcXFXbGioqOj5efn51yCgoKu2GsDAADXUqB5Rg4dOqTc3Fz5+/vnW+/v769t27ZdsaKGDRumqKgo57/T0tIIJC6I2QgBAFeCS0565u3tfcmndAAAgHsr0GmaChUqyMvLS8nJyfnWJycnMzgVAAAUSoHCSLFixdSoUSPFxsY61zkcDsXGxiosLOyKFwcAAK5/BT5NExUVpd69eys0NFRNmjTR5MmTlZGRocjISElSr169FBgYqOjoaEmnB71u2bLF+f/79u1TYmKiSpUqperVq1/BPwUAALijAoeRiIgIHTx4UMOHD1dSUpJCQkIUExPjHNS6e/dueXr+1eGyf/9+NWjQwPnviRMnauLEiWrdurXWrFlz+X8BAABwa4UawPrMM8/omWeeOedjfw8YwcHBMtwdFQAAnAf3pgEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABglUveKA8ArhYPD9sVAPg7ekYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFVcTQNruKoBACDRMwIAACwjjAAAAKsIIwAAwCrCCAAAsIoBrACAS8bAc1wN9IwAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCou7b1MXOYGAMDloWcEAABYRc8IAADXAD3p50fPCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwiqtpAAC4Ii52uYy5JlW4I8IIAMAteIy68MHejOBg7644TQMAAKwijAAAAKsIIwAAwCrGjAAAcJ1z9fE29IwAAACrCCMAAMAqwggAALCKMSO4ipgAqPBoOwA3DsIIAFxDrj6QELCB0zQAAMAqekbcHL+yYAP7HYAriTByUZy7BwDgauI0DQAAsIowAgAArOI0DQDgDJyaxrVHzwgAALCKnhHcsLgi5HrFL3vA3dAzAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMCqQoWRqVOnKjg4WD4+PmratKk2bNhwwe0//vhj1apVSz4+PqpXr56++OKLQhULAACuPwUOIwsXLlRUVJRGjBihhIQE1a9fX+3bt1dKSso5t1+3bp0efvhh9enTRz/++KM6d+6szp07a9OmTZddPAAAcH8FDiOTJk1Sv379FBkZqTp16mjatGkqUaKEZs2adc7t33jjDXXo0EFDhgxR7dq1NWbMGDVs2FBTpky57OIBAID7K9B08FlZWYqPj9ewYcOc6zw9PRUeHq64uLhzPicuLk5RUVH51rVv315Lly497/tkZmYqMzPT+e/U1FRJUlpaWkHKvSSndOqCj1/8LS+vprQTl/V0XaT8q9Jmf701bVf4t6btLu/tz18AbXext7e379F2N17b5b2uMRe5DYMpgH379hlJZt26dfnWDxkyxDRp0uSczylatKiZP39+vnVTp041FStWPO/7jBgxwuj0DSRYWFhYWFhY3HzZs2fPBfOFS94ob9iwYfl6UxwOh44cOaLy5cvLw+NiN8FyXWlpaQoKCtKePXvk6+truxy3QtsVHm1XeLRd4dF2hXc9tZ0xRsePH1flypUvuF2BwkiFChXk5eWl5OTkfOuTk5MVEBBwzucEBAQUaHtJ8vb2lre3d751ZcqUKUipLs3X19ftdzBbaLvCo+0Kj7YrPNqu8K6XtvPz87voNgUawFqsWDE1atRIsbGxznUOh0OxsbEKCws753PCwsLybS9JX3755Xm3BwAAN5YCn6aJiopS7969FRoaqiZNmmjy5MnKyMhQZGSkJKlXr14KDAxUdHS0JGnQoEFq3bq1XnvtNXXq1EkLFizQxo0bNX369Cv7lwAAALdU4DASERGhgwcPavjw4UpKSlJISIhiYmLk7+8vSdq9e7c8Pf/qcGnevLnmz5+vl156SS+88IJq1KihpUuXqm7dulfur3AT3t7eGjFixFmnoHBxtF3h0XaFR9sVHm1XeDdi23kYc7HrbQAAAK4e7k0DAACsIowAAACrCCMAAMAqwggAALCKMGIRY4cBAFeKw+GwXUKhEUYscuep7eHezgzChGLAfeV9fk+dOpVvWg13476Vu6G8nSY+Pl7//ve/ZYyRw+HgYIBr7sMPP9TGjRvlcDicoZj98PLRhriW8j6/q1evVuPGjZWSkmK7pEIjjFxDeV/63333nWbMmKH//e9/8vT0pIekAHJzcyVJ+/bt09GjRy1X4542b96sfv36aejQoXrvvff0xx9/SKKn7kqgDXEt5fWEzJ8/Xx07dlTFihUtV1R4hBELnn32WT366KN6/PHHtXr1akl/HWRxfsYYeXl5yeFw6Omnn9b777+vw4cPn7UNLuy2227Tjz/+qIoVK2rUqFF64YUXtGzZsrPaEueX93k9fvy4Vq5cqZEjR2r8+PFas2aN3cLcmDuPd7Ahr71++eUXFS1aVNWrV5fkvscSZmC9xrKzs1W0aFElJSVpwIABMsbok08+kZeXl+3SXF5ubq68vLz09NNPKz4+Xp988omqVKkiSfrxxx91++23046XwOFwOH9RxcTEaNSoUTp48KDuvfdedevWTaGhofLx8ZF0Otzxa//8unfvruTkZBUtWlSpqanOG4eWK1fOdmkuK+9znJaWpuTkZG3atEnt2rVTqVKlJLHPFVRERIS++uortWrVSkuWLHGuP/Nz7hYMrrrc3Nxzrv/1119N7dq1TceOHc2+ffuMMcY4HI5rWZrb2bt3rylbtqzZsGGDMcaYrVu3mp49e5oKFSqY8uXLm2+++cZyhe4hKysr379ff/11U716ddOwYUMzfvx4s2nTJvbF88j7PC9ZssSUK1fO7Nq1yxhjTHBwsHnllVeMMcZs377d/PDDD9ZqdFVn7lOPPPKIqVatmmnQoIHx9fU148aNy7dfsv9dXE5Ojpk+fbp54IEHTJkyZUyXLl3Ml19+mW8bd2lHN4pN7svT01M7d+5U+/bt9cYbb+jtt9/WTz/9pMDAQL366qsqXry4Pv/8c0mcc76YTZs2KTAwUIGBgdq3b59GjBihvXv36uOPP1ZISIg+/fRT2yW6tLyuXS8vL504cUIbNmyQJA0ePFg//PCD7rjjDk2dOlUDBw7U3LlzOe11Dnm/Nj/77DP17dtXVatW1eTJk+Xl5aWBAwdKkuLi4rRkyRKlpqbaLNXl5O1/Y8aM0c8//6x58+Zp7ty5ysjI0MiRI3XzzTdr8eLFkvguvBReXl7q16+fpk6dqrFjxyojI0MjRozQ888/ry1btkhyn3YkjFwjO3bskJeXl+bNm6dFixYpNDRUHTp00Msvv6wlS5boySef1DvvvCNjDAeACwgLC5Ofn58effRRNWjQQD4+Pnrttdd05513qmXLltq+fTvnni8g70A6ZswYhYeH69FHH5Wfn58++ugjlSlTRm+++aaWLVumkydP6tixY27zRXYt5X0+q1Spoj179ig7O1v/+c9/NHr0aOephvj4eG3btk1+fn42S3U5Xl5eysjI0HvvvacxY8YoLCxMb7/9tlq2bKl169bJ399f3bt3V926dZWenm67XJeWnp6uFStW6Pvvv5fD4dCAAQM0YcIEtWjRQgkJCXrooYecYxLdgt2Omevf37vITpw4YVJTU83u3bvNhx9+aBYvXmyGDh1qunTpYlq1amV+//13S5W6vqSkJGOMMUuXLjUvvfSSefHFF53te+LECVOzZk3z+uuvW6zQteXk5BhjjPnggw9McHCwmT17ttm5c6fx8PAwkydPNsacbkdcmlWrVpmGDRua9u3bm/bt2zvX//bbb6Z06dJmzZo1FqtzXWvXrjURERHm6NGjZufOnSYwMNCsW7fOGGPMuHHjTNeuXc3bb79tuUrXlJ2dbYwx5rvvvjOtWrUy1apVM6VLlza33XZbvtOCK1asMAMGDDCnTp2yVWqBEUaukjNDyJ9//mnee+89s2TJEnPw4EGTmZl51vYHDhwwzZo1M+3ateOAcIa8D9+CBQvMkCFDTGJiojHmrwOrMcb88ssvZuDAgea2226zUqO7qVWrljO0vfLKK6ZevXrm5MmTJjc31/zrX/8yMTExxhj3Odd8Lf29TZ577jnj7e1tWrRoYVatWmWGDRtm7rrrLtOlSxdLFbq+U6dOmeXLl5v09HSzcOFC06JFC3P48GFjjDGLFy82ERER5/yOxF9q1Khh/t//+3/GGGOmTJliAgMDzdGjR40xxqSnpxtjzh4X5uo4TXOV5J0qeOedd9S5c2dNmzZNERERat++vZKSkpyPZ2dny+FwKCAgQJMmTdLevXt17Ngxi5W7DofDoSJFiujEiRMaMGCAqlatqqCgIElSWlqa0tLSJEkbN25URkaGZs2aZbNct3Ds2DFVrVpVbdu2VU5OjsaPH69Ro0bJx8dH2dnZOnDggBISEiS5z7nmayHv82qMUW5urrZv3y5JmjRpkhYvXqzMzEz17NlTK1eu1F133aX333/fZrkuKyUlRd7e3rrnnntUsmRJlS9fXt9//71++OEH/fnnnxo+fLhuvvlmFStWzHapLmvJkiVyOByaOHGisrKyNG7cOI0cOVJlypTRunXrNHToUP3xxx8qWrSo7VILxnYauh7l/Xo6fPiwKV26tJk3b54xxpi+ffuaNm3aGGOMycjIOOtX1vPPP2+aNGlybYt1YXntM2DAANOuXTtjzOlfVd9884254447TFBQkJk4caIxxpj9+/dbq9OdZGVlmebNm5uhQ4eaHj16mI4dOzof27p1qylfvrz58ccf7RXognJycpz74gsvvGBCQ0NN9erVTXBwsJk5c6Zzu127dpm0tDRbZbqsvN7Nzz//3HTu3NmsWLHC+ZjD4TD9+/c3Hh4eJiAgwISFhdkq0+Xl7YNff/21ad26tTHGmCFDhphmzZo52zg+Pt7cfvvtZvPmzbbKLDTCyFU0evRo506TkJBgSpcu7TyvN2fOHNOrVy/nOAhjjElOTjYHDx60UarLSk9PN+Hh4Wbs2LHGGGOmTp1qOnToYLp162ZefvllU6tWLXPo0CHLVbqX7777zoSFhRkfHx/nwXTTpk2mXbt2plu3bparcy1nhouZM2eacuXKmZEjR5oFCxaYwYMHmxIlSpiWLVua5ORkYwyntv7uzPaoVKmSmTBhgvOHQ94l0ocPHzbx8fHmv//9Lz8q/iYnJ8ekpqbmW7dp0yZTuXJlM3bsWFO6dGkTHx/vfKx3796mQ4cO17rMK4IwchXNmTPH3H///cYYY9q0aWP69+/vfOzjjz82zZo1c37Z5X0w+TI726hRo0zjxo1Nnz59TFBQkHnzzTdNVlaW2bVrl6lXrx4DBS/gXPtTbm6umTlzpqlRo4Zp0KCBqV69uqlWrZrp0KGD87wzjImJiTGtW7c2H3/8sTHGmCeeeMK8+eab+bb58ccfTUhIiHnmmWdslOjy8va/iRMnmnr16pns7Gznurz/HjlyxFp9rm7AgAHm3nvvNRs3bsw3GHX69OkmODjY3HLLLWbdunVmz549Jjo62vj5+Zlff/3VYsWFV8T2aaLrWdWqVbVp0yYNGDBAP//8s1asWCHp9AyEr7zyijp16qTSpUvnmymP8/R/Mf83E2P79u21detWHTlyRKNHj9Zjjz0m6fQ9fg4fPqyWLVvaLdRFmTNmsvzqq6/01Vdf6eabb9aDDz6oPn36qEOHDvr0009VokQJVapUSa1atVLp0qUtV+06jhw54rx0/Msvv9Thw4d14sQJSXLe5DIkJETdu3fXvHnzlJSUpICAAMtVu4a8fS9v/0tPT1ft2rVVpEiRfI/n5ubq/fffV5EiRTRgwADn4zgtPDxczz77rO6++249++yzeuSRR1StWjVFREQoOztbn3zyie677z6lp6erefPmGj9+vG699VbbZReO1Sh0Axg9erSpXLmyufPOO83q1avN2rVrTb9+/UxwcLBzG3pDzu187ZKbm2vWrVtnqlWrZqZOnXqNq3Ifeb1t06ZNM+XLlzcNGzY0ZcuWNQEBAWbSpEluddmfLX/88YcZO3as6dixo6lUqZK5/fbbzR9//JFvm/Xr15uKFSuaP//8006RLuZcn9tZs2aZihUrnnMsw/3332+GDh16LUpzK2deMRgdHW18fHxMrVq1zOzZs83x48eNMadn8Y6PjzerVq1yu6tn/o4wcpWdOnXKvPXWW6ZZs2YmODjYFCtWzPTq1ct5aiFv4BFOy/sAxsTEmP79+5uAgADTsWNHM3LkSPPLL78YY4xJTEw0jz76qOnTp4/NUl3WZ599ZmJjY53/rl69uvnggw/M8ePHTWZmphk2bJgpVqyYadiwoVm+fDmDLs/h7wfU7777zkRFRZng4GDTvHlzM3HiRJOenm7Wrl1runbtau69915LlbqePn36mK+//jrfui1btpgaNWqYdu3amYSEBJOenm6OHj1qZs+ebYoXL+68tBenrV271rzxxhvmt99+c65LT083PXv2NB4eHuauu+4ysbGxZwUQd/5hSxi5QvIOorGxsWbw4MGmYcOG5rHHHjOzZ882mZmZZv/+/WbLli3OAyrOduaAtnLlypmePXuaCRMmmB49epg2bdqY+++/3zkA+Pfff+cL7Dy6du1qPDw8TJ8+fcyWLVvMoEGDzppMb9euXeaBBx4wHh4ezsHBuDCHw2FWrVplevToYYKDg42vr6+pXr26+c9//uOc2+FGl5ycbJ566innoMuNGzc6H0tMTDQhISGmWLFipn379qZy5comJCSEiQrP4amnnjKlS5c2jzzyiPn000+d9y4z5vQ4pSZNmpgiRYqYAQMGmB9++OG6+FHLXXuvgLwxH3v37lWDBg3UunVrBQYGatu2bUpPT9ett96qsWPHqnLlyrZLdQtPPPGE9u3b57xfjyT997//1YsvvqiKFSvq888/59zyRSxfvlzPP/+8UlJSlJmZqbfeekt9+vRxTmWedy5/9erVuuWWW3TTTTfZLNdlbdmyRUuWLFGJEiXUpEkThYWFKTU1VUuXLtWiRYu0fft2zZgxQ23atLFdqstIT09XqVKltGbNGnXr1k1dunTRkCFDVLNmTUnS4sWLFRcXp5tuuklhYWFq1KiR5Ypd06pVqzRy5EgdO3ZM99xzj+655x41atRIJUuWlCQtWLBAgwcP1rFjx/T777+7//HFchi6rjz44IMmIiLC+Qv/xIkTZsaMGebmm282AwcOtFyde8jIyDAPPvigeeKJJ4wx+c+b5p2b37Jli63y3Ep2drZ5/fXXTWBgoKlSpYpZsGBBvpkt3blL92rK+5U5Z84cU6dOHdOmTRtTvXp1U65cOfP99987t9u6dSu/6i9g27Zt5j//+Y9p06aNue2228yoUaNMRkaG7bJcXk5OjvN7b/v27aZZs2bGy8vL3HzzzWb8+PEmMTEx32d30aJFtkq9oggjV0Bubq45evSoadu2rfMW4mceROfOnWsqVapkDhw4YKtEtzJ+/HhTp06ds+ZuOHDggKlWrVq+SZOQX95+l5aW5my3ffv2mT59+hhPT0/ToUOHfF3nODeHw2HKly/vnIdlwoQJJiQkxOTm5pqTJ0+anTt3Wq7QPWRlZZmvv/7aDBkyxISEhJhmzZqZuXPn2i7LpeV9hr/77jtz++23myFDhpiPPvrI9OrVy5QpU8a0aNHCzJkzx2zfvt1ypVcWYeQypKWl5Ztw65lnnjGtWrVyXqWQ10OydetWU61atXy/qvCXrVu3moULF5opU6aYAwcOmKSkJBMUFGTq169vvv32W5OTk2P27dtnpkyZYv7xj3/YLtctPPvss2bixIn59s/169ebVq1ameLFi5uePXsyzuEC5s2bZ0JCQowxp8cn+fr6mpUrVxpjjFmzZo3p1q2b8z5JOC3v+y47O9ukpKSYn376yfnY0aNHzeLFi81jjz1matSoYTp27GhOnjxpq1S30KZNm7Pmr4mPjzd16tQxVapUMd26dXPbOUXOhTByGYYOHWpatmxp5s+fb4w5vaMUL17c3HfffWb37t3GmNOBZdKkSaZq1ao2S3VZcXFxJjQ01JQpU8ZUrVrV1KxZ02zfvt0kJyebzp07Gw8PD1O3bl1z0003mVtuucUsXLjQdskuK+8X1auvvmpq1qxp1q9ff87t3nvvPXP33Xdfy9Lczvr1603jxo2NMcY88sgj+W589+2335ratWubPXv22CrPJeX1xD3zzDMmJCTElCxZ0oSGhjpDnDGnB06//vrrZvr06bbKdAt5M08PGjTIGGNMZmam88qZjz76yFSpUuW6+wwTRgopNzfXTJ8+3XTr1s00a9bMPP7442bbtm1m06ZNpk2bNsbLy8u0a9fO1KpVy9xyyy1m2bJlxhgu5f27mjVrmiFDhphff/3VvPvuu8bT09MMHjzYGHN6ZsYDBw6Y0aNHm0WLFp11RQjOdurUKRMQEGAWLFjgXJf3izUrK8t5zp7xIueXm5trdu/ebWrVqmXuu+8+U6JECedtGnJyckzbtm25rPxv8vanL774wpQpU8a8/fbbZunSpeaBBx4wXl5epnPnztfdaYWr7YUXXjC33XbbWdPB79ixw/Tv3/+6u3UIV9Ncpt27d+ujjz7Sl19+qYyMDIWHh6tLly5KTU3VsmXLFBQUpMaNG6tVq1a2S3U5kydP1ltvvaWdO3c611WuXFm+vr5q1aqV9u7dq7S0NN166636448/tHLlSu7meREJCQl6/PHH9d577511lcKmTZv08ccfKzIyUsHBwXYKdCMrVqzQyy+/rIyMDA0aNEhBQUH6+OOPtWbNGm3btk0lSpSwXaLLeffdd5WSkqKXX35Z0unZpmNjY/Xiiy9q69at6tmzp9566y15eXkx2/RF7N+/X//85z+1b98+TZw4Ub1799bmzZv19ttv63//+582bdpku8QrijByhWzcuFHz58/Xxo0bVaxYMUVGRqp79+7y9vZ2bmPOmJ77RudwOFS5cmW1bdtWH3zwgYoWLao5c+ZowIABioqKUp06dXTq1CmlpKRo+/btqlu3rgYNGmS7bJd37NgxNWrUSM8++6wGDRqknJwc52XQa9as0VNPPaVvvvlGFSpUsFypa8rNzZWHh4dSU1NVtmxZrV27Vu+++67Wr1+v5ORk9ejRQw8++KDatm1ru1SXkfe9tmjRIn355Zcyxmj69OnOW1xIUkZGhmbMmKHly5crNjbWYrWu6e/HhhMnTqhEiRI6fPiwRowYodmzZ6t48eIqWbKkihYtqgULFlx3l0QTRi7T33ei5cuX65NPPtHmzZt18803q2fPnrrvvvssVuiaDh8+rP79+yslJUUBAQF6+OGHNXDgQL322mt66KGHJBHeCsrhcCgnJ0fPPPOMPvnkE82cOVNdu3aVMUbJycnq3LmzGjZsqLffftt2qS4lL7B9+eWX+uijj7Ry5UrVrl1bdevWVVRUlKpUqaL09HRlZGSoUqVKtst1WYMGDdLbb7+t8uXLa9myZWratKmk/J/jrKwsejfPIa+N5s2bpzVr1sjHx0d16tRR7969VbJkSe3fv19ffvmlypcvr7p1616XPZuEkSvkzA/csWPHtGjRIi1fvlwnT57U0KFD1a5dO8sVuh6Hw6HVq1frrbfe0pYtW3TkyBEtW7ZMLVq0cD6em5urokWLWq7U/fTr108ffPCB6tatq6pVq2rHjh0qXry41q9fn+8X640u73N78uRJBQUFqXPnzqpRo4aSkpK0ZcsWHT9+XCNHjtTdd99tu1SXd/z4cSUkJGjMmDH6+uuv1b9/f7366qvcfPEi8ibNXLNmjbp3766GDRvKw8NDx44dk5+fn/r27asHH3zQdplXHWHkCjszlOzcudM56+XKlSvl4+NjuTrXdPLkSc2fP19z586VMUZ33nmn+vTpo6pVq9ouzW2kpaXp5MmTSk9P1y233CJJSkxM1DvvvCMvLy/nuKW8x3Ba3uf1+eef18aNG7V69WpnWPvmm28UHR2trKwsrVixIt8pV5xfdna2PvzwQ40ZM0bHjh3TSy+9pOeee852WS6vT58+CgwM1OjRo5Wenq7PPvtMy5cv17Zt21SnTh316dPnuj49SBi5SvK+5NavX6/HHntM//3vf6/LrrUrKSkpSW+88Ya++eYblS5dWh07dtTAgQM5VXMeubm58vLy0sKFCzV79mytX79ezZs312233aZHHnlE9evXl0TX+MXkndpKT0/XvHnz8j2WmJiotm3basWKFQoLC7NUoWv79ddf9dlnn6lEiRIqV66c7rzzTlWqVEnJycmaNm2aRo0apddff50xX+eQ1yvy888/a+bMmQoNDVWvXr2cj//5559avny5Fi1apPLly2vp0qX2ir3ars1FOzeuKVOmmHvuucd2GW7lp59+Mvfee6+JioqyXYrLOvOmgn5+fmbs2LFm3bp1pn79+qZKlSqmefPm5tVXX2XW30s0a9Ysc/PNN5sff/wx3/q0tDRTu3Zt5rf5m7wpCpYtW2Zq1KhhatSoYW666SbTsGFD061bN+csyadOnTKbNm3iUvKL6N69uylXrpzp3r37OR+Pi4u7riY4Oxd6Rq6y3NxcZWRkyNfX13YpbiU3N1eZmZlcPnke5v963vr376+DBw9qyZIl2r9/v2rWrKlx48Zp4cKF2rRpk2699VaNHz9erVu3tl2yS8prx+3bt6tbt24yxmjcuHFq0qSJsrOztXz5cg0dOlRHjhxhrM05BAYG6umnn9agQYNUsmRJzZ8/X3PmzFFaWppWrFihsmXL2i7R5eXm5mru3LlauXKl1qxZoxYtWqhv375q37697dKuKcII4KYOHTqkXr166eGHH1bPnj3VoUMHBQUFacaMGfrhhx/Uo0cPNW3aVFOmTJGfn5/tcl1eWlqaHn/8cS1evFiNGjXS7t27VbFiRQ0ZMiRf1/mNLi/ArV69Wk8++aS+/fZbVaxY0fn4kSNHFBISos6dO+vNN9+0WKl7SU5O1oIFC7Ry5UodPXpUYWFhioyMVL169WyXdk1wH3bATVWoUEEvvviiSpcurf379yslJUUvvfSSJKlSpUoKDQ3V4MGDCSJnMMbI4XDIy8tL3377rdavX69t27apU6dO6ty5sz755BP99NNPzjFetWvXdo69uZHljW3Izs52Xt1WpUoVZWRkaPXq1XrooYeUnZ0tLy8vlStXTj179tSOHTsYr3QJTp48qZycHPn7+2vQoEFq1aqVFi1apA0bNuiRRx7RG2+8cV0PXM1DGAHcQN6v0W3btunnn3/WoUOH1KVLF91xxx2SpKNHjyozM1PLly9X3bp19cUXX2jt2rWaP3++5cpdi4eHh7y8vHTo0CF1795dAQEBKl++vHr06KF69erp3XffVf369Qkgf5N3imrw4MG688471blzZ916661q1KiRxo0bp5CQENWqVcu5/ebNm1WqVCmCyDnkzWuzdetWzZw5Uxs3blRgYKBat26tvn37qkGDBmrQoIE+//xzrVq1yjnVwXXP1mAVAJcmb7DqunXrzrqpYFxcnDHm9D1Thg0bZho1amSqVatmypUrZ+bNm2ezbJeSnJxsBg4c6Lw3z7/+9S/TtWtXk5uba1JSUsyqVatMeHi48fDwMI899hh3ND6HN99803h4eJh27dqZAQMGmISEBJOWlmbuvfde4+PjY/r06WOio6NNRESEKV++vNm3b5/tkl3a7bffbrp3725Gjx5t7rrrLlOmTBnTqlUrs3TpUuc2N9K9zBgzAriJmjVrqnPnzurTp4/WrFmjAQMGaPDgwZo4caLz19b06dNVpEgR1a5dm0tRz7Bq1So98sgjKlGihIYMGaLAwEAlJSVpwIABkk73PB08eFAxMTEaMWKEDh06pJSUFBUvXtxy5a5j9uzZevzxxxUaGqqAgAClpaWpU6dOateunf744w9NmDBBmZmZqlu3rh588EF16tTJdskuJ+9015QpU/TWW29p27Zt8vDwUMWKFdW1a1dt2bJFW7Zs0R133KGxY8eqbt26tku+ZggjgAvLm0vkXDcVDAwMlK+vr+644w7t3btXx44dU506dbRz504m2fub7Oxsbd26VYsWLdLChQu1f/9+tWjRQitXrpT012mw3Nxc7dixQykpKWrZsqXlql3P9OnTtXjxYnXq1Em//fab4uLiFBgYqIiICOdtHLjq6MJyc3PVqVMnderUSQMHDtSQIUO0fv16rV27VjExMc4bWb755ptq3Lix7XKvGfYawIV5eXnJ4XBo3Lhxatq0qbKzsyVJc+bMUWpqqh544AG1a9dODz30kLp16yYvLy917dqVIPJ/8n5rpaSk6MCBA/L399dLL72kgQMH6n//+5/uuece7dq1yzmxnpeXl2rWrEkQ+Zu8/a5z586qXLmyEhMTNXnyZI0bN05FixbV66+/7rwSSfqr3XE2Ly8v9erVS5UqVVJGRoZiY2P19NNPS5IaNGigO++8UyNHjryhgohEzwjg8ripYOHkdYlv2LBBgwYN0g8//ODsTerXr5/Kly/v7Cp/8skn9corr/Cr/hIkJSWpS5cuqlWrlmbOnKn09HTNnTtXs2fPVvPmzbmc9xLl5OQoLS1Nbdu2VUREhIYNG6YNGzaoa9euSkhIyHe59I2AMAK4AW4qWHi1atXSXXfdpR49euiPP/7Qa6+9prJly2rFihXas2ePli5dqvfff1+7du3Sp59+qg4dOtgu2bq804Nr1qzRypUrdffdd8vb21tlypRRnTp1tH79ek2cOFH333+/Hn30UUnS9u3b5evrK39/f8vVu5dBgwZp7dq1Klu2rPbs2aO7775bU6dOtV3WNUcYAdwINxW8NBcaa7NkyRI9+eSTWrlypUJCQnTy5En99NNPmjVrlp577jnVrl3bYuWuIzc3V5UqVdKhQ4fk5+enjh07Kj4+XmXKlNEdd9zh7BVJTExUvXr16Jk7j7y5VrZs2aLPPvtMiYmJ6tq1qxo1aqRq1app9+7dmjVrln7//XfVqFFDL7/88g3ZloQRwA1xU8GLczgcqly5stq2basPP/zQ2TarVq1S586ddfTo0Xx34j1x4gS3HzhD3unBY8eOqWzZsqpTp4769u2r2NhYHT16VDt37tTRo0eZy+Y88gJxnuDgYFWpUkU5OTlKTExUWFiYnnrqKd17771ctSXCCODWfv75Z7300kuqUaOGXnvtNdvluJQzx9pUrFhRPXr0UNeuXXXHHXeoefPmmjBhwlkHDOTncDj01Vdfafr06dq/f78aN26sp59+WrfeeqskOWdkZczS2f71r3/pgQceUOPGjTV37lxNnTpVq1atkq+vr3bs2KHBgwfrm2++0X333aeePXsqLCxMpUuXtl22NYQRwM1xU8HzO3OszeHDh+Xl5aVNmzZp37598vHxkcPhkMTlqBdz8uRJzZkzRx999JFycnLUtm1bPf744woODrZdmssxxui3337TbbfdpnLlyum5555TcHCwtm7dqtGjR+fb9osvvtCQIUO0e/dubd26VVWqVLFUtX2EEQDXvZMnTzqv+Dh69Kh69Oih3r17KygoyHZpboXTgwUzc+ZMPfPMMypWrJgqVKigL774It+0+XliYmJu+IHThBEAN4zk5GRNnjyZg+ll4vTgueVdTv7jjz9qxYoVuvvuu1WzZk0NGzZM06ZNU5s2bfTyyy8rNDRUpUqVsl2uSyGMALjhcDC9fJwezC9v3ExycrK6du2q4OBg9ejRQ//85z8lSQkJCerXr582b96s/v3764knnlCNGjW4meD/IYwAuCFxMMWVlBdG7rvvPpUpU0ZTpkyRr6+vcnJy5Onp6RyXNGfOHL300ktKSUnRlClT1LdvX8uVuwbCCAAAlyEviHz//ffq1KmT4uLinFcc5cm78ijPkCFD1Lx5c3Xp0uVal+uSitguAAAAd5Y35mjt2rUKDQ1VxYoVz7rcOS+IvPjii0pOTtbMmTOt1OqquJ4NAIAroGzZstqyZYvKlCkjDw+Pc94wsG7dusrOztbJkyctVOi6CCMAAFwBVatW1b59+zR37lxlZ2c7e0bODCWffPKJypcvz6yrf8NpGgAAroAWLVqoadOmGj16tHx9fdW6dWv5+fnJw8NDJ06c0LJlyxQTE6N9+/bZLtXlMIAVAIArZMeOHXrooYe0detWdevWTa1atVKlSpX00UcfKTExUY8//riioqJsl+lyCCMAAFxBR44c0bRp0zR16lQdPnxYHh4eqlevnqKiovTQQw/ZLs8lEUYAALhK1q1bp+DgYPn5+alkyZK2y3FZhBEAAK4y7mx8YVxNAwDAVUYQuTDCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKz6/x4iFMIyKWr6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code;\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: \n",
    "\n",
    "-- Your answer;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sentiment prediction using text\n",
    "\n",
    "Now that you've gotten a feel for the data, it's time to build a classifier.\n",
    "\n",
    "There are multiple ways to build a sentiment analyser, but we're going to be using a convolutional neural network (CNN).  We'll take a look at CNNs in class, but the \"magic\" of CNNs lies in something called a \"convolutional\" layer.  A convolutional layer passes a window over the entire data example, and learns to extract information; the window is then moved slightly, and the same information is extracted again, and again, and again.\n",
    "\n",
    "For this task, we're going to be building a 1-dimensional convolutional layer of length 3.  This will move over words in the text, and then extract an embedding for those three words.  It will then shift one word, and extract an embedding from the next 3 words, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "I really like this movie.\n",
    "```\n",
    "                        |\n",
    "                        V\n",
    "\"I really like\"    -> [0.1,  0.5,  0.3] \n",
    "\"really like this\" -> [-0.2, -0.3, 0.4] \n",
    "\"like this movie\"  -> [-0.3, 0.1,  0.5] \n",
    "```\n",
    "\n",
    "After extracting n-2 embeddings for the sequence, we will apply a max-pooling layer, which will grab the maximum value from each column (we could also do mean-pooling, or 2x2 pooling, or any other type of pooling, but we'll stick with max pooling).\n",
    "\n",
    "Conv(\"I really like this movie\") = [0.1, 0.5, 0.5]\n",
    "\n",
    "You'll be provided with the CNN - you don't need to modify it in this exercise.\n",
    "\n",
    "Also provided below is code that uses torchtext to build a representation of the MELD corpus, both input tokens and labels. You also don't need to know how this works (you are exploring torchtext in more detail in other courses of this program). Note that this code requires that you have set your data_dir correctly, and it actually creates some intermediate files in that directory, so the directory must be writable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Images are from https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d\n",
    "\n",
    "![Conv](https://miro.medium.com/v2/resize:fit:1000/1*YvlCSNzDEBGEWkZWNffPvw.gif)\n",
    "\n",
    "\n",
    "![Conv](https://miro.medium.com/v2/resize:fit:1000/1*WpOcRWlofm0Z0EDUTKefzg.gif)\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "![Max Pooling](https://miro.medium.com/v2/resize:fit:1000/1*LjXV6eQKTQcg-PJnBRE0VA.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.0: Prepare the data\n",
    "\n",
    "Pytorch requires that its data be in a very specific format - tensors of x values and y values that are then converted by the neural models into data we're interested in.   Data must first go through a pre-processing pipeline, including generating a vocabulary from the training data.  The following code converts the CSV provided into the appropriate data structure.  This code was modified from [here](https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L15/packed_lstm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torchtext pipeline first wraps the file in an iterator, then opens it and parses it.  We add a final filter that limits the data to just the first and 4th columns (ie, text and sentiment).  No modifications needed here, but you'll need them in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system.', 1]\n",
      "['Oh my God, he\\x92s lost it. He\\x92s totally lost it.', 2]\n"
     ]
    }
   ],
   "source": [
    "# provided code; \n",
    "\n",
    "### Code modified from https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L15/packed_lstm.ipynb\n",
    "VOCABULARY_SIZE = 20000\n",
    "sentiments = {\"neutral\":\"1\", \"positive\":\"0\", \"negative\":\"2\"}\n",
    "    \n",
    "datapipe_train = IterableWrapper([\"/Users/jungyeul/Downloads/565_Lab_2_2023/train_sent_emo.csv\"])\n",
    "datapipe_train = FileOpener(datapipe_train, mode='b')  ### Open the file\n",
    "datapipe_train = datapipe_train.parse_csv(skip_lines=1) ### Parse, skipping the header\n",
    "datapipe_train = datapipe_train.map(lambda row: [row[1], int(sentiments[row[4]])]) ###Limit data to 1th column (text) and 4th column (sentiment)\n",
    "\n",
    "for sample in datapipe_train:\n",
    "     print(sample)\n",
    "     break\n",
    "\n",
    "datapipe_dev = IterableWrapper([\"/Users/jungyeul/Downloads/565_Lab_2_2023/dev_sent_emo.csv\"])\n",
    "datapipe_dev = FileOpener(datapipe_dev, mode='b')\n",
    "datapipe_dev = datapipe_dev.parse_csv(skip_lines=1)\n",
    "datapipe_dev = datapipe_dev.map(lambda row: [row[1], int(sentiments[row[4]])])\n",
    "\n",
    "for sample in datapipe_dev:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we need to create a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def get_vocab(train_datapipe):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(train_datapipe), specials=[\"<SOS>\", \"<EOS>\", \"<UNK>\", \"<PAD>\"], max_tokens=VOCABULARY_SIZE)\n",
    "    vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  6018\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(datapipe_train)\n",
    "print(\"Vocabulary size: \", len(vocab))\n",
    "# PADDING_VALUE=vocab['<PAD>']\n",
    "# vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to define a short function that transforms a sequence of words into vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(input_seq, vocab):\n",
    "    seq = [vocab[token] for token in tokenizer(input_seq)]\n",
    "    seq = [vocab[\"<SOS>\"]] + seq + [vocab[\"<EOS>\"]] ### Add in start / end of sequence markers\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to the text_transform: here is an example\n",
      "output of the text_transform: [0, 53, 19, 128, 3781, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"input to the text_transform:\", \"here is an example\")\n",
    "print(\"output of the text_transform:\", text_transform(\"here is an example\", vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code will implement batching - finding sequences of similar length and grouping them together, with padding, if necessary.  It first requires a collation function, that indicates how instances should be collected together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    text_list, label_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        processed_text = torch.tensor(text_transform(_text, vocab))\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(_label)\n",
    "    return pad_sequence(text_list, padding_value=PADDING_VALUE).to(DEVICE), torch.tensor(label_list).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then requires code that samples the data and finds items with similar length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own sampler, to ensure we function with multiple worker threads\n",
    "# See https://discuss.pytorch.org/t/using-distributedsampler-in-combination-with-batch-sampler-to-make-sure-batches-have-sentences-of-similar-length/119824/3\n",
    "class BatchSamplerSimilarLength(Sampler):\n",
    "    def __init__(self, dataset, batch_size, indices=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # get the indices and length\n",
    "        self.indices = [(i, len(tokenizer(s[0]))) for i, s in enumerate(dataset)]\n",
    "        # if indices are passed, then use only the ones passed (for ddp)\n",
    "        if indices is not None:\n",
    "            self.indices = torch.tensor(self.indices)[indices].tolist()\n",
    "\n",
    "    def __iter__(self):\n",
    "        # if self.shuffle:\n",
    "        #     random.shuffle(self.indices)\n",
    "\n",
    "        pooled_indices = []\n",
    "        # create pool of indices with similar lengths\n",
    "        for i in range(0, len(self.indices), self.batch_size * 100):\n",
    "            pooled_indices.extend(sorted(self.indices[i:i + self.batch_size * 100], key=lambda x: x[1]))\n",
    "            self.pooled_indices = [x[0] for x in pooled_indices]\n",
    "        \n",
    "        # Comment in for validation\n",
    "        #self.pooled_lengths = [x[1] for x in pooled_indices]\n",
    "        #print(self.pooled_lengths)\n",
    "        #print(self.pooled_indices)\n",
    "\n",
    "        # yield indices for current batch\n",
    "        batches = [self.pooled_indices[i:i + self.batch_size] for i in\n",
    "                   range(0, len(self.pooled_indices), self.batch_size)]\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(batches)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pooled_indices) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our sampler, we can create our batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "datapipe_train_list = list(datapipe_train)\n",
    "datapipe_dev_list = list(datapipe_dev)\n",
    "\n",
    "train_loader = DataLoader(datapipe_train_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = datapipe_train_list, \n",
    "                                                                  batch_size=BATCH_SIZE), collate_fn=collate_batch)\n",
    "dev_loader = DataLoader(datapipe_dev_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = datapipe_dev_list, \n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=False), collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      "torch.Size([7, 64])\n",
      "input sample:\n",
      "tensor([  0, 131,   8,  23, 155,   4,   1])\n",
      "label shape:\n",
      "torch.Size([64])\n",
      "label sample:\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "batch = next(iter(train_loader))\n",
    "print(\"input shape:\")\n",
    "print(batch[0].shape)\n",
    "print(\"input sample:\")\n",
    "print(batch[0][:, 0])\n",
    "# print(batch[0][:])\n",
    "print(\"label shape:\")\n",
    "print(batch[1].shape)\n",
    "print(\"label sample:\")\n",
    "print(batch[1][0])\n",
    "# print(batch[1].shape, batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information about the tensors: \n",
    "\n",
    "* The default batch size is 64 - this means we gather together 64 instances (of similar length) at a time for training and inference - that's why these tensors have 64 as the second dimension.  The CNN expects the batch dimension to be first - we'll be doing some manipulation of the dimensions inside the CNN.  \n",
    "* The input is of size [14,64].  This means that we have 64 instances, each of length 14 (including padding).  The CNN requires that the batch dimension be first, but we'll fix that later.  It's much easier to read for us when we do it this way.\n",
    "* Batches are created by sorting all of the training examples by length, so that all instances in a batch have the same length (in this case, 14).  If we couldn't find 64 instances of length 4, we take some shorter ones, and add the padding tokens.\n",
    "* The output is just our ordinary labels that we had originally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 - creating the CNN\n",
    "rubric={accuracy:1}\n",
    "\n",
    "We're going to be using a convolutional neural network (CNN) to train our sentiment classifier.  This is typical for sequential prediction tasks.  A CNN passes a sliding window over a sentence (we'll be working with a window of length 3).  It takes the embeddings of the words in that window, and performs a linear transformation on them.  We then end up n-k embedding for the entire sentence:\n",
    "\n",
    "[0->2] <br>\n",
    "[1->3] <br>\n",
    "[2->4] <br>\n",
    "[...] <br>\n",
    "[n-2->n] <br>\n",
    "\n",
    "Each vector is the same length, let's say m.\n",
    "\n",
    "If we stack these separate vectors, then we get a matrix of size [n-k, m]:\n",
    "\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "[0 1 2 3 4 ...  m] <br>\n",
    "\n",
    "For each column, we just take the maximum value, giving us a new vector length \"m\".\n",
    "\n",
    "This is our \"convolved\" vector that will represent the sentence.  It's essentially a fuzzy n-gram detector.  We're also going to use dropout of 0.3.  I suggest you take a hard look at the provided code - you will be modifying it in the next exercise, so make sure you know exactly what the code is doing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This first bit of code represents the CNN - your job is to implement the forward function.\n",
    "\n",
    "You will need to run the input sequence through:\n",
    "\n",
    "        1. the embedding layer (`input_seq`)\n",
    "        2. the dropout layer (`dropout_layer`)\n",
    "        3. `permute` it so that the \"sequence\" dimension is last (where is it, currently?) \n",
    "                => we will put the sequence last. 1,2,3 -> 2,3,1;  to put the sequence last, as expected by the convolutional layers\n",
    "                After dropout: (INPUT_SEQ, BATCH_SIZE, HIDDEN_DIM) -> (BATCH_SIZE, HIDDEN_DIM, INPUT_SEQ)\n",
    "        4. run the result through the convolutional layer (`conv`)\n",
    "        5. apply max pooling across the second dimension (for this, you can call `torch.max`)\n",
    "            Hint: torch.max returns a tuple of indices and values - you want the values (see below)\n",
    "        6. Call `F.relu` on the result, to activate it\n",
    "        7. Apply the `dropout_layer` again\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> a = torch.rand(3,3,3)\n",
    ">>> a\n",
    "\n",
    "                           axis = 1\n",
    "                              |\n",
    "        * axis = 0            V\n",
    "tensor([[[*0.1292, 0.4502, 0.7906],  <---  axis = 2\n",
    "         [0.5469, 0.6681, 0.5831],\n",
    "         [0.9125, 0.6430, 0.7398]],\n",
    "\n",
    "        [[*0.8346, 0.9417, 0.6099],\n",
    "         [0.7979, 0.0380, 0.0628],\n",
    "         [0.9640, 0.5319, 0.3050]],\n",
    "\n",
    "        [[*0.0024, 0.7601, 0.4194],\n",
    "         [0.0086, 0.1827, 1.0000],\n",
    "         [0.0379, 0.2279, 0.8576]]])\n",
    ">>> torch.max(a, axis=0)\n",
    "torch.return_types.max(\n",
    "values=tensor([[0.8346, 0.9417, 0.7906],\n",
    "        [0.7979, 0.6681, 1.0000],\n",
    "        [0.9640, 0.6430, 0.8576]]),\n",
    "indices=tensor([[1, 1, 0],\n",
    "        [1, 0, 2],\n",
    "        [1, 0, 2]]))\n",
    ">>> torch.max(a, axis=1)\n",
    "torch.return_types.max(\n",
    "values=tensor([[0.9125, 0.6681, 0.7906],\n",
    "        [0.9640, 0.9417, 0.6099],\n",
    "        [0.0379, 0.7601, 1.0000]]),\n",
    "indices=tensor([[2, 1, 0],\n",
    "        [2, 0, 0],\n",
    "        [2, 0, 1]]))\n",
    ">>> torch.max(a, axis=2)\n",
    "torch.return_types.max(\n",
    "values=tensor([[0.7906, 0.6681, 0.9125],\n",
    "        [0.9417, 0.7979, 0.9640],\n",
    "        [0.7601, 1.0000, 0.8576]]),\n",
    "indices=tensor([[2, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 2, 2]]))\n",
    "\n",
    "# to get `values` of torch.max\n",
    ">>> torch.max(a, axis=2).values\n",
    "tensor([[0.7906, 0.6681, 0.9125],\n",
    "        [0.9417, 0.7979, 0.9640],\n",
    "        [0.7601, 1.0000, 0.8576]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "class CNN(nn.Module):\n",
    "    '''Converts a sentence into a fixed length vector representation using convolution and max pooling. Input to forward\n",
    "    should be a matrix of token indices of shape (sequence length, batch size), output is a vector which represents the\n",
    "    sentence, of shape (batch size, hidden_dim)'''\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_dim)        \n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim, 3) # window size is 3\n",
    "        self.dropout_layer = nn.Dropout(p=0.3) \n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        ### Your code here\n",
    "        \n",
    "        ### Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Now that you have a CNN that does max-pooling, you can add it as a layer  a regular FF NN.\n",
    "\n",
    "The classifier should take the input, pass it through a CNN layer, and then a final Linear layer.  We'll be doing logistic regression, so you'll also want to pass it through a log softmax, so what we are returning are log probabilities.\n",
    "\n",
    "    1. input go through `CNN`\n",
    "    2. `linear`, \n",
    "    3. `F.log_softmax` (which `dim=?`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "class CNNClassifier(nn.Module):\n",
    "    '''Uses a CNN and a linear layer to provide (pre-Sigmoid) score for binary classification'''\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.CNN = CNN(vocab_size, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        ### Your code here\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train and evaluate our classifier.  We'll run the model for 30 epochs (I recommend when debugging, set the value lower), and then evaluate.  You will know that your code from above was correct if the loss is decreasing with each epoch.  My loss was ~100 after 30 epochs (about 60 below where it started).  Training 30 epochs is pretty fast (our hidden layer is only 64 nodes, after all). It finishes training in less than a little more than a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch: 0 Loss is: 159.8611958026886\n",
      "After epoch: 1 Loss is: 144.7844859957695\n",
      "After epoch: 2 Loss is: 140.0794302225113\n",
      "After epoch: 3 Loss is: 137.9580042362213\n",
      "After epoch: 4 Loss is: 135.0211929678917\n",
      "After epoch: 5 Loss is: 133.32490891218185\n",
      "After epoch: 6 Loss is: 131.42105853557587\n",
      "After epoch: 7 Loss is: 128.17739355564117\n",
      "After epoch: 8 Loss is: 127.10968834161758\n",
      "After epoch: 9 Loss is: 124.95486617088318\n",
      "After epoch: 10 Loss is: 124.2073672413826\n",
      "After epoch: 11 Loss is: 121.68130320310593\n",
      "After epoch: 12 Loss is: 120.92535150051117\n",
      "After epoch: 13 Loss is: 119.351034283638\n",
      "After epoch: 14 Loss is: 118.6497352719307\n",
      "After epoch: 15 Loss is: 116.35184115171432\n",
      "After epoch: 16 Loss is: 113.86126059293747\n",
      "After epoch: 17 Loss is: 113.24722456932068\n",
      "After epoch: 18 Loss is: 111.37214380502701\n",
      "After epoch: 19 Loss is: 109.7791477739811\n",
      "After epoch: 20 Loss is: 108.65479332208633\n",
      "After epoch: 21 Loss is: 106.49020951986313\n",
      "After epoch: 22 Loss is: 104.43540605902672\n",
      "After epoch: 23 Loss is: 103.61190596222878\n",
      "After epoch: 24 Loss is: 103.29255211353302\n",
      "After epoch: 25 Loss is: 102.10659347474575\n",
      "After epoch: 26 Loss is: 99.86243323981762\n",
      "After epoch: 27 Loss is: 98.56664070487022\n",
      "After epoch: 28 Loss is: 96.6684011220932\n",
      "After epoch: 29 Loss is: 95.9851341471076\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "\n",
    "torch.backends.cudnn.deterministic = True  ### Try to ensure replicability\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "epochs = 30 ### Train in a reasonable time, at the cost of overall accuracy\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# weights.to(device)\n",
    "\n",
    "hidden_dim = 64 ### Keep models small.  Feel free to experiment with this value\n",
    "\n",
    "# loss_function = nn.NLLLoss(weight=weights)  ### Since we are returning log_softmax, we are working with log-probabilities\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "model = CNNClassifier(vocab_size, hidden_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    batch_counter = 0\n",
    "    for data in train_loader:\n",
    "        train_seq = data[0]\n",
    "        train_labels = data[1]    \n",
    "        model.zero_grad()\n",
    "        train_seq, train_labels = train_seq.to(device), train_labels.to(device).float()\n",
    "        score = model(train_seq)\n",
    "        gold = torch.tensor([int(i.item()) for i in train_labels])\n",
    "        loss = loss_function(score, gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('After epoch:', epoch, 'Loss is:', epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to evaluate your model.  We'll use the development data (which was already converted into an appropriate format).  Normally, we wouldn't evaluate on the development data - it's there to tune the model, and make sure we got it working (and to pick the best model).  After development, we evaluate on test - hopefully, the findings we got on our development data hold.\n",
    "\n",
    "For your evaluation, you'll want to do several things.  First, set your model to \"eval\" mode (model.eval()).  Otherwise, the model will apply dropout during inference, which we definitely don't want!  Then, iterate through the development data, and compare the predictions made by your model to the gold labels.  I get about 81%, which is a tough baseline to beat - we've already beaten the \"majority baseline\" (ie, our model is learning, and not just picking the majority class).  Your numbers may be a little different because of some inherent randomness in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6321009918845807\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "predictions = defaultdict(list)\n",
    "golds = defaultdict(list)\n",
    "model.eval() #needed because our model has dropout, otherwise it will be applied when predicting!\n",
    "total = 0\n",
    "correct = 0\n",
    "counts = {}\n",
    "pred_counts = {}\n",
    "# confusion = {}\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for data in dev_loader:\n",
    "        dev_seq = data[0]\n",
    "        dev_labels = [x.item() for x in data[1]]\n",
    "        \n",
    "        dev_seq = dev_seq.to(device)\n",
    "        scores = model(dev_seq)\n",
    "        preds = [np.argmax(x) for x in scores.detach().cpu().numpy()]\n",
    "        for i in preds:\n",
    "            pred_counts[i] = pred_counts.get(i, 0) + 1\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] == dev_labels[i]:\n",
    "                correct += 1\n",
    "            counts[dev_labels[i]] = counts.get(dev_labels[i], 0) + 1\n",
    "            counts[\"total\"] = counts.get(\"total\", 0) + 1     \n",
    "            # confusion[str(dev_labels[i]) + \":\" + str(preds[i])] = confusion.get(str(dev_labels[i]) + \":\" + str(preds[i]), 0) + 1\n",
    "\n",
    "        total += len(preds)\n",
    "        \n",
    "print(\"Accuracy: \", correct / total)  \n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "Perform a high-level error analysis of the results.  You should modify the evaluation code above to create a confusion matrix of the results (ie, how many times did the model predict X, when the true value was Y?).  Do the results make sense, given what you know about sentiment?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 (Optional)\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "We didn't do a check for the class distribution in the data, but from Exercise 1, we saw that it is not equal - there are far more neutral instances than anything else.  We can influence the model to work around this bias by providing class weights to the loss function.\n",
    "\n",
    "We'll just do a simple calculation of the weights - find out how many of each class there are, and inversely weight the classes by the proportion.  The weights should be a tensor of floats that are then supplied to the [weight parameter in the loss function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html).\n",
    "\n",
    "Once you've generated the weights, you'll need to send them to your device, pass them to the loss function, and re-train / evaluate.  Write a short paragraph explaining any changes you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2798, 2.1208, 3.3919])\n",
      "Counter({1: 4710, 2: 2945, 0: 2334})\n"
     ]
    }
   ],
   "source": [
    "class_counter = Counter()\n",
    "\n",
    "### Your code here\n",
    "\n",
    "\n",
    "### Your code here\n",
    "print(weights)\n",
    "print(class_counter)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Joint training\n",
    "\n",
    "In exercise 2, we were just using the utterance to learn a classification model - a pretty standard X-> Y mapping.  But we have other information available that seems very closely-related: we know that emotion and sentiment are related to each other.  We could add in the emotion as an \"X\" value during training, but then, we would need to get it for every example during inference, which would defeat the purpose of training a model at all.  Instead, we're going to do something a bit different - we're going to use the emotion as another task to learn simultaneously with sentiment.\n",
    "\n",
    "This task is typically called \"Multi-Task learning\" (MTL), or sometimes, \"Joint learning\".  The idea is that learning two related tasks will result in a stronger model than just learning one for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "The first thing we need to do is modify our data again - instead of just one class, we are now going to be predicting 2: sentiment and emotion.  You'll need to modify your Data loader so that on the label side, it produces a 1x2 tensor instead of a 1x1 one.\n",
    "\n",
    "Feel free to copy the code from 2.0 here - you'll need to modify it to handle the new setting.\n",
    "This will require modifications to the data pipeline and collate_batch function - everything else can stay the same, but you need to run the pipeline again.  The main thing is to transform the labels from a tensor of ints to a tensor of tuples of ints.\n",
    "\n",
    "eg. [2, 1, 0] -> [(2,4), (1,3), (0,0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system.', 1, 0]\n",
      "['Oh my God, he\\x92s lost it. He\\x92s totally lost it.', 2, 4]\n"
     ]
    }
   ],
   "source": [
    "### Code modified from https://github.com/andrei-radulescu-banu/stat453-deep-learning-ss21/blob/main/L15/packed_lstm.ipynb\n",
    "\n",
    "sentiments = {\"neutral\":\"1\", \"positive\":\"0\", \"negative\":\"2\"}\n",
    "emotions = {\"neutral\":\"0\", \"fear\":\"1\", \"anger\":\"2\", \"disgust\":\"3\", \"sadness\":\"4\", \"surprise\":\"5\", \"joy\":\"6\"}\n",
    "\n",
    "datapipe_train = IterableWrapper([\"train_sent_emo.csv\"])\n",
    "datapipe_train = FileOpener(datapipe_train, mode='b')  ### Open the file\n",
    "datapipe_train = datapipe_train.parse_csv(skip_lines=1) ### Parse, skipping the header\n",
    "datapipe_train = datapipe_train.map(...)\n",
    "\n",
    "for sample in datapipe_train:\n",
    "     print(sample)\n",
    "     break\n",
    "\n",
    "datapipe_dev = IterableWrapper([\"dev_sent_emo.csv\"])\n",
    "datapipe_dev = FileOpener(datapipe_dev, mode='b')\n",
    "datapipe_dev = datapipe_dev.parse_csv(skip_lines=1)\n",
    "datapipe_dev = datapipe_dev.map(...)\n",
    "\n",
    "for sample in datapipe_dev:\n",
    "    print(sample)\n",
    "    break\n",
    "\n",
    "# From Ex2:\n",
    "# ['also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system.', 1]\n",
    "# ['Oh my God, he\\x92s lost it. He\\x92s totally lost it.', 2]\n",
    "\n",
    "# For Ex3:\n",
    "# ['also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system.', 1, 0]\n",
    "# ['Oh my God, he\\x92s lost it. He\\x92s totally lost it.', 2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From Ex2:\n",
    "# def collate_batch(batch):\n",
    "#     text_list, label_list = [], []\n",
    "#     for (_text, _label) in batch:\n",
    "#         processed_text = torch.tensor(text_transform(_text, vocab))\n",
    "#         text_list.append(processed_text)\n",
    "#         label_list.append(_label)\n",
    "#     return pad_sequence(text_list, padding_value=PADDING_VALUE).to(DEVICE), torch.tensor(label_list).to(DEVICE)\n",
    "\n",
    "# For Ex 3:\n",
    "# def collate_batch(batch):\n",
    "#     text_list, label_list = [], []\n",
    "#     for (_text, _label) in batch:                                                     <---  your batch should contains _sentiment and _emotion instead of _label\n",
    "#         processed_text = torch.tensor(text_transform(_text, vocab))\n",
    "#         text_list.append(processed_text)\n",
    "#         label_list.append(_label)                                                     <--- append  _sentiment and _emotion;\n",
    "#     return pad_sequence(text_list, padding_value=PADDING_VALUE).to(DEVICE), torch.tensor(label_list).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as in Ex2:\n",
    "datapipe_train_list = list(datapipe_train)\n",
    "datapipe_dev_list = list(datapipe_dev)\n",
    "\n",
    "train_loader_joint = DataLoader(datapipe_train_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = datapipe_train_list, \n",
    "                                                                  batch_size=BATCH_SIZE), collate_fn=collate_batch)\n",
    "dev_loader_joint = DataLoader(datapipe_dev_list, \n",
    "                          batch_sampler=BatchSamplerSimilarLength(dataset = datapipe_dev_list, \n",
    "                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                  shuffle=False), collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      "torch.Size([4, 64])\n",
      "input sample:\n",
      "tensor([  0, 187,   4,   1])\n",
      "label shape:\n",
      "torch.Size([64, 2])\n",
      "label sample:\n",
      "tensor([0, 6])\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "batch = next(iter(train_loader_joint))\n",
    "print(\"input shape:\")\n",
    "print(batch[0].shape)\n",
    "print(\"input sample:\")\n",
    "print(batch[0][:, 0])\n",
    "print(\"label shape:\")\n",
    "print(batch[1].shape)\n",
    "print(\"label sample:\")\n",
    "print(batch[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous results:\n",
    "# -----------------------------\n",
    "# input shape:\n",
    "# torch.Size([17, 64])\n",
    "# input sample:\n",
    "# tensor([   0,   17,   75,   39,   44,    4,   38,  157,  831,  135,   50, 1731,\n",
    "#         4959, 3350,    4,    1,    3])\n",
    "# label shape:\n",
    "# torch.Size([64])\n",
    "# label sample:     \n",
    "# tensor(1)             <---        previously one result, now TWO results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Now that we have multiple output classes, we'll need to modify our CNN so it produces not one output distribution, but several.\n",
    "\n",
    "You don't need to modify the CNN itself, but you'll need to modify the classifier.\n",
    "\n",
    "Modify the CNNClassifier class (I highly recommend naming it something like \"JointCNNClassifier\") so that it takes a number of classifiers, so that it has 2 linear layers - one that transforms the embedded input into a sentiment distribution (as before), and one that transforms it into an emotion distribution (very similar).  The forward function should return both distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Ex2, class CNNClassifier(nn.Module):\n",
    "\n",
    "# class JointCNNClassifier(nn.Module):\n",
    "#     '''Uses a CNN and a linear layer to provide (pre-Sigmoid) score for binary classification'''\n",
    "#     def __init__(self, vocab_size, hidden_dim):\n",
    "#         super(CNNClassifier, self).__init__()                                 ---> super(JointCNNClassifier, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim                                          ---> idem\n",
    "#         self.CNN = CNN(vocab_size, hidden_dim)                                ---> idem\n",
    "#         self.linear1 = nn.Linear(hidden_dim, 3)                                ---> one for sentiment\n",
    "#         self.linear2 = nn.Linear(hidden_dim, 7)                                ---> one for emotion\n",
    "#         self.dropout_layer = nn.Dropout(p=0.3)                                ---> idem\n",
    "\n",
    "#     def forward(self, input_seq):\n",
    "#         ### Your code here\n",
    " \n",
    "    # 1. input go through `CNN`\n",
    "    # 2. `linear`, \n",
    "    # 3. `F.log_softmax` (which `dim=1`) --> one for semitment; another for emotiion; \n",
    "    # 4. return log_softmax values of sentiment and emotion;\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Now, you need to modify the training algorithm to handle two different predictions.\n",
    "\n",
    "This will require 2 loss functions: one for emotion, and one for sentiment (they can both be NLLLoss). There are many ways that we can combine these functions to modify the model, but we're just going to take the easiest way - add them together, and back-propagate through the model.\n",
    "\n",
    "The steps you'll need to modify: \n",
    "\n",
    "1. Create a separate loss function for the sentiment and emotion\n",
    "2. Separate out the gold sentiment labels and gold emotion labels\n",
    "3. Separate predictions returned by the model\n",
    "4. Calculate separate loss values for sentiment and emotion\n",
    "5. Combine losses\n",
    "6. Back-propagate (no modification is required - once the losses are added, calling loss.backward will do back-propagation.\n",
    "\n",
    "You may also want to keep track of the epoch loss for sentiment and emotion separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Ex2:\n",
    "\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True  ### Try to ensure replicability\n",
    "# RANDOM_SEED = 42\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# epochs = 30 ### Train in a reasonable time, at the cost of overall accuracy\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# # weights.to(device)\n",
    "\n",
    "# hidden_dim = 64 ### Keep models small.  Feel free to experiment with this value\n",
    "\n",
    "# loss_function = nn.NLLLoss() \n",
    "    # -> You need to define two NLLoss()s for emotion and senti; \n",
    "    # -> emot_loss_function = nn.NLLLoss()\n",
    "    # -> sent_loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "# vocab_size = len(vocab)\n",
    "# model = CNNClassifier(vocab_size, hidden_dim)                         ---> modelJoint = JointCNNClassifier(vocab_size, hidden_dim)\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "    # ->     emot_epoch_loss = 0\n",
    "    # ->     sent_epoch_loss = 0\n",
    "\n",
    "#     batch_counter = 0\n",
    "#     for data in train_loader:\n",
    "#         train_seq = data[0]\n",
    "#         train_labels = data[1]    \n",
    "#         model.zero_grad()\n",
    "#         train_seq, train_labels = train_seq.to(device), train_labels.to(device).float()\n",
    "#         score = model(train_seq)\n",
    "    # ->    scores = modelJoint(train_seq)\n",
    "    # ->    sent_scores = \n",
    "    # ->    emot_scores = \n",
    "\n",
    "#         gold = torch.tensor([int(i.item()) for i in train_labels])\n",
    "        # -> gold_sent = \n",
    "        # -> gold_emot = \n",
    "\n",
    "#         loss = loss_function(score, gold)\n",
    "            # -> emot_loss = \n",
    "            # -> sent_loss = \n",
    "            # -> loss =                                                 ---> your loss is sum of sent_loss and emot_loss\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "            # -> emot_epoch_loss +=             \n",
    "            # -> sent_epoch_loss += \n",
    "#     print('After epoch:', epoch, 'Loss is:', epoch_loss)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch: 0 Emotion Loss is: 236.7262768149376\n",
      "After epoch: 0 Sentiment Loss is: 158.3451641201973\n",
      "After epoch: 1 Emotion Loss is: 214.9758431315422\n",
      "After epoch: 1 Sentiment Loss is: 144.53474378585815\n",
      "After epoch: 2 Emotion Loss is: 209.38474696874619\n",
      "After epoch: 2 Sentiment Loss is: 141.24635016918182\n",
      "After epoch: 3 Emotion Loss is: 203.79699611663818\n",
      "After epoch: 3 Sentiment Loss is: 138.09752088785172\n",
      "After epoch: 4 Emotion Loss is: 200.99668502807617\n",
      "After epoch: 4 Sentiment Loss is: 135.88494670391083\n",
      "After epoch: 5 Emotion Loss is: 198.16450834274292\n",
      "After epoch: 5 Sentiment Loss is: 133.626459389925\n",
      "After epoch: 6 Emotion Loss is: 196.18797743320465\n",
      "After epoch: 6 Sentiment Loss is: 132.77064526081085\n",
      "After epoch: 7 Emotion Loss is: 193.36775296926498\n",
      "After epoch: 7 Sentiment Loss is: 130.68901360034943\n",
      "After epoch: 8 Emotion Loss is: 191.80103766918182\n",
      "After epoch: 8 Sentiment Loss is: 129.2412519454956\n",
      "After epoch: 9 Emotion Loss is: 188.57674032449722\n",
      "After epoch: 9 Sentiment Loss is: 126.67758268117905\n",
      "After epoch: 10 Emotion Loss is: 185.6322296857834\n",
      "After epoch: 10 Sentiment Loss is: 124.936846524477\n",
      "After epoch: 11 Emotion Loss is: 185.4841577410698\n",
      "After epoch: 11 Sentiment Loss is: 124.68343967199326\n",
      "After epoch: 12 Emotion Loss is: 182.37255257368088\n",
      "After epoch: 12 Sentiment Loss is: 121.56502383947372\n",
      "After epoch: 13 Emotion Loss is: 181.09980142116547\n",
      "After epoch: 13 Sentiment Loss is: 120.41783341765404\n",
      "After epoch: 14 Emotion Loss is: 179.44259589910507\n",
      "After epoch: 14 Sentiment Loss is: 119.52110683917999\n",
      "After epoch: 15 Emotion Loss is: 178.00372052192688\n",
      "After epoch: 15 Sentiment Loss is: 118.1177886724472\n",
      "After epoch: 16 Emotion Loss is: 177.01549363136292\n",
      "After epoch: 16 Sentiment Loss is: 117.49291563034058\n",
      "After epoch: 17 Emotion Loss is: 175.05939239263535\n",
      "After epoch: 17 Sentiment Loss is: 115.80977001786232\n",
      "After epoch: 18 Emotion Loss is: 172.30573707818985\n",
      "After epoch: 18 Sentiment Loss is: 114.05349171161652\n",
      "After epoch: 19 Emotion Loss is: 170.13349932432175\n",
      "After epoch: 19 Sentiment Loss is: 111.5632611811161\n",
      "After epoch: 20 Emotion Loss is: 167.88709244132042\n",
      "After epoch: 20 Sentiment Loss is: 109.73667764663696\n",
      "After epoch: 21 Emotion Loss is: 168.64650654792786\n",
      "After epoch: 21 Sentiment Loss is: 110.16978850960732\n",
      "After epoch: 22 Emotion Loss is: 164.48976516723633\n",
      "After epoch: 22 Sentiment Loss is: 107.10751375555992\n",
      "After epoch: 23 Emotion Loss is: 165.02336013317108\n",
      "After epoch: 23 Sentiment Loss is: 107.44218111038208\n",
      "After epoch: 24 Emotion Loss is: 163.47157561779022\n",
      "After epoch: 24 Sentiment Loss is: 105.54416701197624\n",
      "After epoch: 25 Emotion Loss is: 159.9994882941246\n",
      "After epoch: 25 Sentiment Loss is: 103.37905248999596\n",
      "After epoch: 26 Emotion Loss is: 159.6914823949337\n",
      "After epoch: 26 Sentiment Loss is: 102.58066141605377\n",
      "After epoch: 27 Emotion Loss is: 158.84442007541656\n",
      "After epoch: 27 Sentiment Loss is: 102.25848695635796\n",
      "After epoch: 28 Emotion Loss is: 157.57646742463112\n",
      "After epoch: 28 Sentiment Loss is: 101.10587315261364\n",
      "After epoch: 29 Emotion Loss is: 155.36162513494492\n",
      "After epoch: 29 Sentiment Loss is: 99.00356659293175\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.4 Evaluating joint training\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "You will also need to modify your evaluation code to correctly evaluate your system.  Again, produce confusion matrices and briefly discuss what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From Ex2:\n",
    "\n",
    "# #provided code\n",
    "# predictions = defaultdict(list)\n",
    "# golds = defaultdict(list)\n",
    "# model.eval()                                        ---> modelJoint.eval() #needed because our model has dropout, otherwise it will be applied when predicting!\n",
    "# total = 0\n",
    "# correct = 0\n",
    "#             ---> emot_correct = 0\n",
    "#             ---> sent_correct = 0\n",
    "# counts = {}\n",
    "# pred_counts = {}\n",
    "        \n",
    "        \n",
    "# with torch.no_grad():\n",
    "#     for data in dev_loader:\n",
    "#         dev_seq = data[0]\n",
    "#         dev_labels = [x.item() for x in data[1]]\n",
    "#             ---> sent_dev_labels = \n",
    "#             ---> emot_dev_labels = \n",
    "#         dev_seq = dev_seq.to(device)\n",
    "\n",
    "#         scores = model(dev_seq)\n",
    "#             ---> sent_scores = \n",
    "#             ---> emot_scores = \n",
    "\n",
    "#         preds = [np.argmax(x) for x in scores.detach().cpu().numpy()]\n",
    "#             ---> emot_preds = [...  emot_scores.detach().cpu().numpy()]\n",
    "#             ---> sent_preds = \n",
    "\n",
    "#         # go through emot_preds and sent_preds to get corret numbers; \n",
    "#         for i in range(len(preds)):\n",
    "#             if preds[i] == dev_labels[i]:\n",
    "#                 correct += 1\n",
    "\n",
    "#         total += len(emot_preds)                  --> One total is enough; \n",
    "        \n",
    "print(\"Emotion Accuracy: \", emot_correct / total) \n",
    "print(\"Sentiment Accuracy: \", sent_correct / total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Accuracy:  0.5500450856627592\n",
      "Sentiment Accuracy:  0.6293958521190262\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.5 (Optional):\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "Perform some tuning on your model to try to improve your accuracy.  \n",
    "Some suggestions - try adding in extra tasks that might have more information (do the characters get more positive / negative by season?)  Try re-weighting the classes like in Exercise 1.\n",
    "Tune several parameters: hidden size and dropout size are good candidates, but you're free to tune whatever you feel like.  Make one of the tasks easier - what if we only predict positive / negative, instead of neutral?  What if we balance the classes (in any of our tasks?).  Report your findings.  Don't worry if you can't improve the quality that much (or at all).  An effort to investigate at least 2 of these ideas (or others you come up with) will get you your marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Sentiment from speech\n",
    "\n",
    "As we all know, text is not the best mode for detecting sentiment / emotion.  Sarcasm, enthusiasm, boredom, and other emotions just don't come through in text like they do in speech.\n",
    "\n",
    "You may have noticed, way back in Exercise 1, that MELD is a *multi-modal* data-set.  This means that along with textual data, it includes some other `mode` of data.  The original dataset contained video (another great mode for emotion!), but since we haven't worked with video, I won't be springing it on you here.\n",
    "\n",
    "Instead, I've extracted the audio from the video files.  Remember that we've seen audio before, back in COLX 521.  We'll be extracting audio_features from some audio files using OpenSmile (I know that some of you cannot use OpenSmile - don't worry, I'll provide the features for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MacOS users, just \"load\" dev/train audio npz files; \n",
    "\n",
    "\n",
    "# !python3 -m pip install opensmile\n",
    "# !python3 -m pip install --upgrade audresample\n",
    "# !python3 -m pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Provided code - \n",
    "### this will load the audio features into dictionaries, and combine them into a single dictionary\n",
    "full_data = {}\n",
    "dev_audio = np.load(data_dir + \"dev_audio.npz\")\n",
    "train_audio = np.load(data_dir + \"train_audio.npz\")\n",
    "full_data.update(dev_audio)\n",
    "full_data.update(train_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    "rubric{accuracy:1}\n",
    "\n",
    "We're going to make a small change to the dataloader here - we'll include the dialogue number (column 5) and utterance number (column 6) - this will make things a lot easier when we get to training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my companys tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You mustve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So lets talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>10474</td>\n",
       "      <td>You or me?</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:48,173</td>\n",
       "      <td>00:00:50,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>10475</td>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:00:51,009</td>\n",
       "      <td>00:00:53,594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>10476</td>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>Joey</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:00,518</td>\n",
       "      <td>00:01:03,520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>10477</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>All</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1038</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:05,398</td>\n",
       "      <td>00:01:07,274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>10478</td>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1038</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:08,401</td>\n",
       "      <td>00:01:12,071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sr No.                                          Utterance  \\\n",
       "0          1  also I was the point person on my companys tr...   \n",
       "1          2                   You mustve had your hands full.   \n",
       "2          3                            That I did. That I did.   \n",
       "3          4      So lets talk a little bit about your duties.   \n",
       "4          5                             My duties?  All right.   \n",
       "...      ...                                                ...   \n",
       "9984   10474                                         You or me?   \n",
       "9985   10475  I got it. Uh, Joey, women don't have Adam's ap...   \n",
       "9986   10476               You guys are messing with me, right?   \n",
       "9987   10477                                              Yeah.   \n",
       "9988   10478  That was a good one. For a second there, I was...   \n",
       "\n",
       "              Speaker   Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  \\\n",
       "0            Chandler   neutral   neutral            0             0       8   \n",
       "1     The Interviewer   neutral   neutral            0             1       8   \n",
       "2            Chandler   neutral   neutral            0             2       8   \n",
       "3     The Interviewer   neutral   neutral            0             3       8   \n",
       "4            Chandler  surprise  positive            0             4       8   \n",
       "...               ...       ...       ...          ...           ...     ...   \n",
       "9984         Chandler   neutral   neutral         1038            13       2   \n",
       "9985             Ross   neutral   neutral         1038            14       2   \n",
       "9986             Joey  surprise  positive         1038            15       2   \n",
       "9987              All   neutral   neutral         1038            16       2   \n",
       "9988             Joey       joy  positive         1038            17       2   \n",
       "\n",
       "      Episode     StartTime       EndTime  \n",
       "0          21  00:16:16,059  00:16:21,731  \n",
       "1          21  00:16:21,940  00:16:23,442  \n",
       "2          21  00:16:23,442  00:16:26,389  \n",
       "3          21  00:16:26,820  00:16:29,572  \n",
       "4          21  00:16:34,452  00:16:40,917  \n",
       "...       ...           ...           ...  \n",
       "9984        3  00:00:48,173  00:00:50,799  \n",
       "9985        3  00:00:51,009  00:00:53,594  \n",
       "9986        3  00:01:00,518  00:01:03,520  \n",
       "9987        3  00:01:05,398  00:01:07,274  \n",
       "9988        3  00:01:08,401  00:01:12,071  \n",
       "\n",
       "[9989 rows x 11 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "train_path = data_dir + \"train_sent_emo.csv\"\n",
    "train_data = pandas.read_csv(train_path)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `datapipe_train.map` requires row[1], row[4] = sentiments, row[3] = emotions, row[5] and row[6];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also I was the point person on my company\\x92s transition from the KL-5 to GR-6 system.', 1, 0, 0, 0]\n",
      "['Oh my God, he\\x92s lost it. He\\x92s totally lost it.', 2, 4, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "### Your code here (reading CSV)\n",
    "### Your code here (modified collate_batch)\n",
    "### Your code to load the dataset here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:\n",
      "torch.Size([23, 64])\n",
      "input sample:\n",
      "tensor([   0,   47,   50, 2136,   53,    4,   70,    5,    8,   10,  148,  203,\n",
      "          13,  553, 2571,   29, 1135,   31,  354,  617,    4,    1,    3])\n",
      "label shape:\n",
      "torch.Size([64, 2])\n",
      "label sample:\n",
      "tensor([1, 0])\n",
      "audio sample:\n",
      "tensor([459,   9])\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "batch = next(iter(train_loader_audio))\n",
    "print(\"input shape:\")\n",
    "print(batch[0].shape)\n",
    "print(\"input sample:\")\n",
    "print(batch[0][:, 0])\n",
    "print(\"label shape:\")\n",
    "print(batch[1].shape)\n",
    "print(\"label sample:\")\n",
    "print(batch[1][0])\n",
    "print(\"audio sample:\")\n",
    "print(batch[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2\n",
    "rubric{accuracy:2, quality: 1}\n",
    "\n",
    "We're going to modify the CNN class to accept audio low-level features (25 of them) as an \"embedding\" of the audio signal.  This will be similar to how we are processing text, except that we won't be running the data through an embedding layer.\n",
    "\n",
    "We'll need a second convolutional layer in the model, that converts from the size 25 embedding to the hidden dimension.  You'll need to permute the audio file so that it fits the convolutional layer (hint: the sequence size should be last.).  After running the text and audio through their separate convolutional layers and applying activation and dropout, you should return both values.  They'll be dealt with by the classifier module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "class AudioCNN(nn.Module):\n",
    "    '''Converts a sentence into a fixed length vector representation using convolution and max pooling. Input to forward\n",
    "    should be a matrix of token indices of shape (sequence length, batch size), output is a vector which represents the\n",
    "    sentence, of shape (batch size, hidden_dim)'''\n",
    "    def __init__(self, vocab_size, hidden_dim):   \n",
    "        ...\n",
    "\n",
    "    def forward(self, input_seq, audio):\n",
    "        ...\n",
    "        return (return_val_text, return_val_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.3\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Now that we've passed everything through the CNN, we need to modify the audio classifier to work on this new data.\n",
    "\n",
    "We're just going to be concatenating the text and audio output from the convolutional layer.  This data is {batch x hidden}, so when we concatenate it, we'll have {batch x hidden * 2}, which will be passed to the final linear layers, as before (this means you'll have to modify the size of those layers in the init function...)\n",
    "\n",
    "You can return the prediction distributions for emotion and sentiment, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "class AudioCNNClassifier(nn.Module):\n",
    "    '''Uses a CNN and a linear layer to provide (pre-Sigmoid) score for binary classification'''\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        ...\n",
    "\n",
    "    def forward(self, input_seq,audio):\n",
    "        ...\n",
    "        \n",
    "        return sent, emot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.4\n",
    "rubric{accuracy:1, quality:1}\n",
    "\n",
    "Now, we again need to modify the training algorithm.  There are two main things that we need to do - some of the data is corrupted, so it needs to be replaced.  You'll need to loop through the batch, and if any of the dialogue / utterance pairs are not in full_data, then you need to add an empty matrix (100x25 zeros is fine).  You'll then need to grab the matrix from full_audio for each item in the batch.\n",
    "\n",
    "Secondly, you'll need to pad the audio data (batches are matched on text, not audio).  You can use the provided function:\n",
    "\n",
    "You'll provide the padded audio to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(to_pad):\n",
    "    ''' Takes a batch of data, and pads any shorter sequences so that all items are the same length'''\n",
    "    max_cols = max([len(row) for instance in to_pad for row in instance])  ### Maximum number of columns in the batch\n",
    "    max_rows = max([len(instance) for instance in to_pad]) ### Maximum number of rows in the batch\n",
    "    padded = []\n",
    "    for i in to_pad:\n",
    "        instance = [[0] * (max_cols)] * (max_rows - len(i))  ### Create a bunch of zeros to add to the end of the sequence\n",
    "        if(max_rows != len(i)):\n",
    "            instance = np.concatenate([i, instance]) ### Add the rows to the end of the sequence\n",
    "        else:\n",
    "            instance = i\n",
    "        padded.append(instance)\n",
    "    padded = torch.tensor(np.array(padded))\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch: 0 Emotion Loss is: 232.16262447834015\n",
      "After epoch: 0 Sentiment Loss is: 153.03723645210266\n",
      "After epoch: 1 Emotion Loss is: 212.78395926952362\n",
      "After epoch: 1 Sentiment Loss is: 143.70107811689377\n",
      "After epoch: 2 Emotion Loss is: 206.28683656454086\n",
      "After epoch: 2 Sentiment Loss is: 139.5942583680153\n",
      "After epoch: 3 Emotion Loss is: 202.87514758110046\n",
      "After epoch: 3 Sentiment Loss is: 137.01324808597565\n",
      "After epoch: 4 Emotion Loss is: 199.15678256750107\n",
      "After epoch: 4 Sentiment Loss is: 134.38190406560898\n",
      "After epoch: 5 Emotion Loss is: 197.2995782494545\n",
      "After epoch: 5 Sentiment Loss is: 132.84787422418594\n",
      "After epoch: 6 Emotion Loss is: 193.43419164419174\n",
      "After epoch: 6 Sentiment Loss is: 130.61021852493286\n",
      "After epoch: 7 Emotion Loss is: 191.27298551797867\n",
      "After epoch: 7 Sentiment Loss is: 128.77414482831955\n",
      "After epoch: 8 Emotion Loss is: 189.12424558401108\n",
      "After epoch: 8 Sentiment Loss is: 127.03430980443954\n",
      "After epoch: 9 Emotion Loss is: 188.4457894563675\n",
      "After epoch: 9 Sentiment Loss is: 126.64188754558563\n",
      "After epoch: 10 Emotion Loss is: 186.17132633924484\n",
      "After epoch: 10 Sentiment Loss is: 124.56225633621216\n",
      "After epoch: 11 Emotion Loss is: 184.14511913061142\n",
      "After epoch: 11 Sentiment Loss is: 122.90430581569672\n",
      "After epoch: 12 Emotion Loss is: 180.99203652143478\n",
      "After epoch: 12 Sentiment Loss is: 120.60483276844025\n",
      "After epoch: 13 Emotion Loss is: 178.8903045654297\n",
      "After epoch: 13 Sentiment Loss is: 118.73037606477737\n",
      "After epoch: 14 Emotion Loss is: 177.70265197753906\n",
      "After epoch: 14 Sentiment Loss is: 117.97852438688278\n",
      "After epoch: 15 Emotion Loss is: 176.62753772735596\n",
      "After epoch: 15 Sentiment Loss is: 115.9629425406456\n",
      "After epoch: 16 Emotion Loss is: 174.60831290483475\n",
      "After epoch: 16 Sentiment Loss is: 115.06579232215881\n",
      "After epoch: 17 Emotion Loss is: 171.46558618545532\n",
      "After epoch: 17 Sentiment Loss is: 112.488355204463\n",
      "After epoch: 18 Emotion Loss is: 170.80258178710938\n",
      "After epoch: 18 Sentiment Loss is: 112.39783644676208\n",
      "After epoch: 19 Emotion Loss is: 169.07396519184113\n",
      "After epoch: 19 Sentiment Loss is: 110.66695868968964\n",
      "After epoch: 20 Emotion Loss is: 165.97458773851395\n",
      "After epoch: 20 Sentiment Loss is: 108.6640567034483\n",
      "After epoch: 21 Emotion Loss is: 165.642903983593\n",
      "After epoch: 21 Sentiment Loss is: 107.45736147463322\n",
      "After epoch: 22 Emotion Loss is: 164.88099819421768\n",
      "After epoch: 22 Sentiment Loss is: 106.93471562862396\n",
      "After epoch: 23 Emotion Loss is: 163.14965867996216\n",
      "After epoch: 23 Sentiment Loss is: 105.38589495420456\n",
      "After epoch: 24 Emotion Loss is: 160.49914383888245\n",
      "After epoch: 24 Sentiment Loss is: 103.36194276809692\n",
      "After epoch: 25 Emotion Loss is: 157.45070204138756\n",
      "After epoch: 25 Sentiment Loss is: 100.19028544425964\n",
      "After epoch: 26 Emotion Loss is: 157.48042938113213\n",
      "After epoch: 26 Sentiment Loss is: 100.41007241606712\n",
      "After epoch: 27 Emotion Loss is: 157.64740014076233\n",
      "After epoch: 27 Sentiment Loss is: 100.96089765429497\n",
      "After epoch: 28 Emotion Loss is: 153.72164458036423\n",
      "After epoch: 28 Sentiment Loss is: 98.25261701643467\n",
      "After epoch: 29 Emotion Loss is: 154.0500944852829\n",
      "After epoch: 29 Sentiment Loss is: 98.59171704947948\n"
     ]
    }
   ],
   "source": [
    "### Your modified training code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.5\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Finally, we need to do the same thing with the evaluation code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your modified evaluation code.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Ensembling (Optional)\n",
    "rubric:{accuracy: 1, reasoning: 1}\n",
    "\n",
    "Another way of combining multiple pieces of data is to train separate models, and then combine their predictions in some way - this is known as \"ensembling\".  Each model makes predictions, and then they are combined in some way - one common way is to use the probabilities of the models as a \"confidence\" score for the selections, and then to just add them up.  In this exercise, you'll be modifying the evaluation code to create an ensemble.  You are free to combine the score any way you feel works, but make sure to provide your motivation in the reasoning part of this answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Ensembling (Optional)¶\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
