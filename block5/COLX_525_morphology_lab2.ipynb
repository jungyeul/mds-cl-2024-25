{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colx 525 Lab Assignment 2: Morphological Segmentation (Cheat sheet)\n",
    "\n",
    "## Assignment objectives\n",
    "\n",
    "In this assignment, you will develop a supervised morphological segmentation system using the `pycrfsuite` tagger toolkit. You will:\n",
    "\n",
    "1. Read in training, development and test data.\n",
    "1. Convert datasets into BIES format.\n",
    "1. Train a baseline BPE segmentation system using the Python toolkit `bpe`.\n",
    "1. Implement an evaluation function for morphologial segmentation.\n",
    "1. Implement a feature extraction function for your supervised segmentation system.\n",
    "1. Train your segmentation system and apply it to test data.  \n",
    "\n",
    "All parts of this assignment are mandatory.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "You will need to install the Python modules `bpe`, `numpy` and `pycrfsuite`. The easiest way to do this is using `pip`:\n",
    "\n",
    "```\n",
    "pip install python-crfsuite bpe numpy\n",
    "```\n",
    "\n",
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "* Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "* Be sure to follow the general lab instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Reading in data\n",
    "\n",
    "Start by running the following code. It will create a directory `.data` and download English segmentation data into the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .data\n",
      "Downloading segmentation_data.tgz into .data\n",
      "Extracting .data/segmentation_data.tgz\n"
     ]
    }
   ],
   "source": [
    "import os, urllib.request, tarfile\n",
    "\n",
    "URL = \"http://mpsilfve.github.io/assets/segmentation_data.tgz\"\n",
    "DATADIR = \".data\"\n",
    "FN = \"segmentation_data\"\n",
    "TGZ = os.path.join(DATADIR,\"%s.tgz\" % FN)\n",
    "\n",
    "try:\n",
    "    print(\"Creating\",DATADIR)\n",
    "    os.mkdir(DATADIR)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "print(\"Downloading %s.tgz into .data\" % FN)\n",
    "urllib.request.urlretrieve(URL,TGZ)\n",
    "\n",
    "print(\"Extracting %s\" % TGZ)\n",
    "tf = tarfile.open(TGZ)\n",
    "tf.extractall(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 1.1: Reading data files into lists\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "In the sub-directory `segmentation_data` of the directory `.data`, there are three files: `traindata`, `devdata` and `testdata`. These files consists of two tab-separated columns:\n",
    "\n",
    "**1.**\n",
    "```\n",
    "vouchsafed      vouch saf ed\n",
    "negative        negat ive\n",
    "annotations     an not at ion s\n",
    "torpedos        torpedo s\n",
    "coxswain        coxswain\n",
    "lofted          loft ed\n",
    "...\n",
    "```\n",
    "\n",
    "The first column contains a word form and the second column contains the same word form but segmented into morphemes. Please read these files into three lists: `traindata`, `devdata` and `testdata`. Each list element should be a pair like:\n",
    "\n",
    "```\n",
    "[\"vouchsafed\",[\"vouch\",\"saf\",\"ed\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** **traindata | devdata | testdat**:\n",
    "```\n",
    "[   ['vouchsafed', ['vouch', 'saf', 'ed']], \n",
    "    ['negative', ['negat', 'ive']], \n",
    "    ['annotations', ['an', 'not', 'at', 'ion', 's']], \n",
    "    ['torpedos', ['torpedo', 's']], \n",
    "    ['coxswain', ['coxswain']], \n",
    "    ['lofted', ['loft', 'ed']], ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "devdata = []\n",
    "testdata = []\n",
    "\n",
    "# your code here\n",
    "\n",
    "# your code here\n",
    "\n",
    "# A few assertions to make sure that your code is working properly.\n",
    "assert(traindata[0] == [\"vouchsafed\",[\"vouch\", \"saf\", \"ed\"]])\n",
    "assert(devdata[0] == [\"sales\",[\"sale\", \"s\"]])\n",
    "assert(testdata[0] == [\"bunkhouses\",[\"bunk\",\"house\",\"s\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define `trainwords`, `devwords` and `testwords` which simply contain the unsegmented word forms in the data sets: \n",
    "\n",
    "**3.** **trainwords | devwords | testwords**:\n",
    "```\n",
    "['vouchsafed', 'negative', 'annotations', 'torpedos', 'coxswain', 'lofted', ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainwords\t ['vouchsafed', 'negative']\n",
      "devwords\t ['sales', 'oppressor']\n",
      "testwords\t ['bunkhouses', 'beheld']\n"
     ]
    }
   ],
   "source": [
    "trainwords = [wf for wf, _ in traindata]\n",
    "devwords = [wf for wf, _ in devdata]\n",
    "testwords = [wf for wf, _ in testdata]\n",
    "\n",
    "print(\"trainwords\\t\", trainwords[:2])\n",
    "print(\"devwords\\t\", devwords[:2])\n",
    "print(\"testwords\\t\", testwords[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 1.2: BIES notation\n",
    "\n",
    "rubric={accuracy:10}\n",
    "\n",
    "We will use a sequential tagger for morphological segmentation which means that we need to convert our segmented word forms into so called BIES (Begin-Inside-End-Singleton) format which looks like this (for the word form \"bunkhouses\" segmented as \"bunk\" \"house\" \"s\"):\n",
    "\n",
    "```\n",
    "BEGIN   INSIDE   INSIDE   END   BEGIN   INSIDE   INSIDE   INSIDE   END   SINGLE\n",
    "\n",
    "b       u        n        k     h       o        u        s        e     s\n",
    "```\n",
    "\n",
    "The first character of each morpheme receives a `BEGIN` tag, the last one an `END` tag and the remaining characters receive an `INSIDE` tag. As a special case, morphemes consisting of a single character (like the plural \"s\" ending above), receive the tag `SINGLE`. \n",
    "\n",
    "It is your task to convert `traindata`, `devdata` and `testdata` into BIES format. You should store the result in the lists `trainbies`, `devbies` and `testbies`. Each example in these lists should have the following format:\n",
    "\n",
    "```\n",
    "[[\"b\",\"BEGIN\"],\n",
    " [\"u\",\"INSIDE\"],\n",
    " [\"n\",\"INSIDE\"],\n",
    " [\"k\",\"END\"],\n",
    " [\"h\",\"BEGIN\"],\n",
    " [\"o\",\"INSIDE\"],\n",
    " [\"u\",\"INSIDE\"]\n",
    " [\"s\",\"INSIDE\"],\n",
    " [\"e\",\"END\"],\n",
    " [\"s\",\"SINGLE\"]]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO/BIO/IOE/BIOES\n",
    "\n",
    "|       | IO | BIO | IOE | BIOES |\n",
    "|-------|-------|-------|-------|-------|\n",
    "| U.N.  | I-NP  | B-NP  | I-NP  | B-NP  |\n",
    "| official  |    I-NP  | I-NP  | I-NP  | I-NP  | \n",
    "| Ekeus  | I-NP  | I-NP  | E-NP  | E-NP  | \n",
    "| heads  |  O  |  O  |  O  |  O  | \n",
    "| for  |  O  |  O  |  O  |  O  | \n",
    "| Baghdad  |  I-NP  | B-NP  |  E-NP  |  S-NP  | \n",
    "| .  |  O  |  O  |  O  |  O  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** **trainbies | devbies | testbies**:\n",
    "```\n",
    "[[  ['v', 'BEGIN'], ['o', 'INSIDE'], ['u', 'INSIDE'], ['c', 'INSIDE'], ['h', 'END'], \n",
    "    ['s', 'BEGIN'], ['a', 'INSIDE'], ['f', 'END'], \n",
    "    ['e', 'BEGIN'], ['d', 'END']    ], \n",
    " [  ['n', 'BEGIN'], ['e', 'INSIDE'], ['g', 'INSIDE'], ['a', 'INSIDE'], ['t', 'END'], \n",
    "    ['i', 'BEGIN'], ['v', 'INSIDE'], ['e', 'END']   ], \n",
    " [  ['a', 'BEGIN'], ['n', 'END'], \n",
    "    ['n', 'BEGIN'], ['o', 'INSIDE'], ['t', 'END'], \n",
    "    ['a', 'BEGIN'], ['t', 'END'], \n",
    "    ['i', 'BEGIN'], ['o', 'INSIDE'], ['n', 'END'], \n",
    "    ['s', 'SINGLE'] ], \n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN = \"BEGIN\"\n",
    "INSIDE = \"INSIDE\"\n",
    "END = \"END\" \n",
    "SINGLE = \"SINGLE\"\n",
    "\n",
    "# your code here\n",
    "def get_bies_notation(data):\n",
    "    result = []\n",
    "\n",
    "    return result\n",
    "# your code here\n",
    "\n",
    "trainbies = get_bies_notation(traindata)\n",
    "devbies = get_bies_notation(devdata)\n",
    "testbies = get_bies_notation(testdata)\n",
    "\n",
    "# A few assertions to make sure that your code is working properly.\n",
    "assert(trainbies[0] == [['v', 'BEGIN'], ['o', 'INSIDE'], ['u', 'INSIDE'], ['c', 'INSIDE'], ['h', 'END'], ['s', 'BEGIN'], ['a', 'INSIDE'], ['f', 'END'], ['e', 'BEGIN'], ['d', 'END']])\n",
    "assert(devbies[0] == [['s', 'BEGIN'], ['a', 'INSIDE'], ['l', 'INSIDE'], ['e', 'END'], ['s', 'SINGLE']])\n",
    "assert(testbies[0] == [['b', 'BEGIN'], ['u', 'INSIDE'], ['n', 'INSIDE'], ['k', 'END'], ['h', 'BEGIN'], ['o', 'INSIDE'], ['u', 'INSIDE'], ['s', 'INSIDE'], ['e', 'END'], ['s', 'SINGLE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 1.3: From BIES notation to morphemes\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "For evaluation purposes, we will need to transform BIES notation produced by our segmentation model back into segmented word forms, i.e. take the following as input:\n",
    "\n",
    "```\n",
    "[['s', 'BEGIN'], ['a', 'INSIDE'], ['l', 'INSIDE'], ['e', 'END'], ['s', 'SINGLE']]\n",
    "```\n",
    "\n",
    "And generate the following as output:\n",
    "\n",
    "```\n",
    "[\"sale\", \"s\"]\n",
    "```\n",
    "\n",
    "Implement a function `unbies(data)` that takes a list of examples in BIES format as input and returns a list of pairs in the format:\n",
    "\n",
    "```\n",
    "[\"sales\", [\"sale\",\"s\"]]\n",
    "```\n",
    "\n",
    "The first element in the pair is the unsegmented word form and the second one is the segmented word form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`unbies()` converts data from **trainbies** to **traindata**:\n",
    "```\n",
    "print(unbies([[[\"d\",\"BEGIN\"],[\"o\",\"INSIDE\"],[\"g\",\"END\"],[\"s\",\"SINGLE\"]]]))    <-- trainbies\n",
    "[[\"dogs\", [\"dog\",\"s\"]]]                                                       <-- traindata\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def unbies(data):\n",
    "    result = []\n",
    "\n",
    "    return result\n",
    "#your code here\n",
    "\n",
    "# An assertion to make sure that your code is working properly.\n",
    "assert(unbies([[[\"d\",\"BEGIN\"],[\"o\",\"INSIDE\"],[\"g\",\"END\"],[\"s\",\"SINGLE\"]]]) == [[\"dogs\", [\"dog\",\"s\"]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: BPE baseline\n",
    "\n",
    "You are now going to use the Python library `bpe` to segment the data using byte-pair encoding. This will serve as a baseline for our supervised morpheme segmentation system.\n",
    "\n",
    "Start by installing `bpe` using `pip` (follow instructions [here](https://github.com/soaxelbrooke/python-bpe)). \n",
    "\n",
    "Study the example for using a `bpe.Encoder` model [here](https://github.com/soaxelbrooke/python-bpe). The `__init__` function for `Encoder` takes four arguments: \n",
    "\n",
    "1. the size of the BPE vocabulary, \n",
    "1. `pct_bpe`, a real valued parameter, which controls how `Encoder` handles frequent tokens (`pct_bpe==1` will apply BPE to all tokens in the training data and `pct_bpe==0` won't segment anything)   \n",
    "1. `ngram_max` which controls the maximal frequency of BPE tokens and\n",
    "1. `ngram_min` which contains the minimum frequency of BPE tokens. \n",
    "\n",
    "You should always set `bpe_pct` to `1` in order to apply BPE to all tokens. You should set `ngram_max` to a large number like `100000` and `ngram_min` to `1`. These settings will ensure that `bpe.Encoder` implements the basic BPE algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 2.1: Training a BPE model\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "You should initialize a `bpe.Encoder` model `encoder` with vocabulary size 64,000. You should then read training data for BPE from the file `en-ud-train.txt` in the sub-directory `segmentation_data` in the `.data` directory. Split the file into lines and call the `encoder.fit` giving the dataset as parameter.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/soaxelbrooke/python-bpe\n",
    "\n",
    "```\n",
    "\n",
    "from bpe import Encoder\n",
    "\n",
    "# Generated with http://pythonpsum.com\n",
    "test_corpus = '''\n",
    "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
    "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future. Python raspberrypi community pypy. Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
    "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
    "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future integration mercurial self script web. Return raspberrypi community test she stable.\n",
    "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
    "'''\n",
    "\n",
    "encoder = Encoder(200, pct_bpe=0.88)  # params chosen for demonstration purposes\n",
    "encoder.fit(test_corpus.split('\\n'))\n",
    "```\n",
    "\n",
    "`Encoder`:\n",
    "```\n",
    "Encoder(\n",
    "    vocab_size=8192,\n",
    "    pct_bpe=0.2,\n",
    "    word_tokenizer=None,\n",
    "    silent=True,\n",
    "    ngram_min=2,\n",
    "    ngram_max=2,\n",
    "    required_tokens=None,\n",
    "    strict=False,\n",
    "    EOW='__eow',\n",
    "    SOW='__sow',\n",
    "    UNK='__unk',\n",
    "    PAD='__pad',\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bpe.encoder import Encoder\n",
    "\n",
    "VOCAB_SIZE=64000\n",
    "PCTBPE=1\n",
    "NGRAM_MAX=1000000\n",
    "NGRAM_MIN=1\n",
    "\n",
    "# your code here\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 2.2: Segmenting development and test data using BPE\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "You should now segment the development and test data using your BPE model `encoder`. Perform segmentation on `devwords` and `testwords` which only contain unsegmented word forms. You can use `encoder.tokenize`. Store the result in two lists `bpe_tokenized_dev` and `bpe_tokenized_test`.\n",
    "\n",
    "`bpe.Encoder` uses a few special characters to indicate start and end of words (among other things). The special characters always start with two underscores. For example, `\"__sow\"`. Filter out all special characters from `bpe_segmented_dev` and `bpe_segmented_test` (you can assume that it is safe to filter out tokens starting with a double underscore `__`).  \n",
    "\n",
    "Examples in `bpe_segmented_dev` and `bpe_segmented_test` should have the following format:\n",
    "\n",
    "```\n",
    "['bunk', 'houses']\n",
    "```\n",
    "\n",
    "(note that this example is actually incorrectly segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`encoder.tokenize`**:\n",
    "```\n",
    "bpe_tokenized_dev:\n",
    "[   ['__sow', 'sales', '__eow'], ['__sow', 'oppress', 'or', '__eow'], \n",
    "    ['__sow', 'wi', 'pes', '__eow'], ['__sow', 'bash', 'fully', '__eow'], \n",
    "    ['__sow', 'feli', 'cit', 'ous', '__eow']    ]\n",
    "```\n",
    "\n",
    "**filter out tokens starting with a double underscore `__`**:\n",
    "```\n",
    "bpe_segmented_dev:\n",
    "[   ['sales'], ['oppress', 'or'], ['wi', 'pes'], ['bash', 'fully'], \n",
    "    ['feli', 'cit', 'ous']  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_tokenized_dev\t [['__sow', 'sales', '__eow'], ['__sow', 'oppress', 'or', '__eow']]\n",
      "bpe_segmented_dev\t [['sales'], ['oppress', 'or']]\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "\n",
    "#your code here\n",
    "\n",
    "print('bpe_tokenized_dev\\t', bpe_tokenized_dev[:2])\n",
    "print('bpe_segmented_dev\\t', bpe_segmented_dev[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3: Evaluation\n",
    "\n",
    "rubric={accuracy:10}\n",
    "\n",
    "We will evaluate segmentation algorithms using precision, recall and f-score on segment boundaries. As an example, let's say we evaluate against the following gold standard:\n",
    "\n",
    "```\n",
    "[[\"hot\", \"dog\",\"s\"],[\"king\",\"s\"]]\n",
    "```\n",
    "\n",
    "and our segmentation system returned the following segmentation:\n",
    "\n",
    "```\n",
    "[[\"hot\",\"dog\",\"s\"], [\"k\",\"ing\",\"s\"]]\n",
    "```\n",
    "\n",
    "The gold standard segment boundaries are:\n",
    "\n",
    "```\n",
    "[[3,6],[4]]\n",
    "```\n",
    "\n",
    "and our system gives the following segment boundaries:\n",
    "\n",
    "```\n",
    "[[3,6],[1,4]]\n",
    "```\n",
    "\n",
    "Now three of our segment boundaries are actually found in the gold standard (`3` and `6` for the first example and `4` for the second one). This gives us a precision of $P = 3/4 = 75\\%$ (three of the four morpheme boundaries were found are in the gold standard) and a recall of $R = 3/3 = 100\\%$ (we found two of the three token boundaries given by the gold standard). Finally, we get the f-score as $2 \\cdot P \\cdot R /(P+R) \\approx 87.5\\%$. \n",
    "\n",
    "Implement a function `evaluate` which takes two arguments:\n",
    "\n",
    "1. `sys_segmented_data` which is a list of segmented examples like `[\"k\",\"ing\",\"s\"]` produced by a segmentation system.\n",
    "1. `gold_segmented_data` which is a list of gold standard segmentation examples like `[\"kings\",[\"k\",\"ing\",\"s\"]]`.\n",
    "\n",
    "Your function should return the precision, recall and fscore for segment boundaries in `sys_segmented_data`.\n",
    "\n",
    "**Note!** You should get fscore above 45% for the BPE segmented test set. \n",
    "\n",
    "If you want, you can now tune the vocabulary size of `bpe.Encoder` (which we set to 64k) so that it you get maximal performance on the development set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P = \\frac{\\textrm{retrieved ones}~\\cap~\\textrm{relevant ones}}{\\textrm{retrieved ones}}$\n",
    "\n",
    "$R  = \\frac{\\textrm{retrieved ones}~\\cap~\\textrm{relevant ones}}{\\textrm{relevant ones}}$\n",
    "\n",
    "```\n",
    "GOLD:\n",
    "[[\"hot\", \"dog\",\"s\"],[\"king\",\"s\"]] => [[3,6],[4]]\n",
    "\n",
    "SYSTEM:\n",
    "[[\"hot\",\"dog\",\"s\"], [\"k\",\"ing\",\"s\"]] => [[3,6],[1,4]]\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "- $\\textrm{retrieved ones}~\\cap~\\textrm{relevant ones}$:  `tp` = **3** where `tp` = GOLD $\\cap$ SYSTEM\n",
    "- $\\textrm{retrieved ones}$ (SYSTEM): `tp + fp` = 3 + 1 =  **4** where `fp` = SYSTEM - GOLD\n",
    "- $\\textrm{relevant ones}$ (GOLD): `tp + fn` = 3 + 0 = **3** where  `fn` = GOLD - SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BPE segmentation:\n",
      "Test set precision: 46.19, recall: 48.02, f-score: 47.08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(sys_segmented_data,gold_segmented_data):\n",
    "    # your code here\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    return precision, recall, fscore\n",
    "    \n",
    "\n",
    "precision, recall, fscore = evaluate(bpe_segmented_test,testdata)\n",
    "print(\"Results for BPE segmentation:\")\n",
    "print(\"Test set precision: %.2f, recall: %.2f, f-score: %.2f\" % (precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4: Supervised morphological segmentation\n",
    "\n",
    "You will now use the Python toolkit `pycrfsuite` to train a supervised morphological segmentation system. You can install `pycrfsuite` using pip (see [Installation](https://python-crfsuite.readthedocs.io/en/latest/)).\n",
    "\n",
    "To get a better understanding of the `pycrfsuite` toolkit, you can browse through the following [tutorial](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb) for building a NER system using `pycrfsuite`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famous CRF implementations:\n",
    "- **crf++**  https://taku910.github.io/crfpp/\n",
    "- **wapiti** https://wapiti.limsi.fr\n",
    "- **crfsuite** https://www.chokkan.org/software/crfsuite/} $\\Rightarrow$ `pycrfsuite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4.1: Feature engineering\n",
    "\n",
    "rubric={accuracy:10}\n",
    "\n",
    "Our segmentation system is feature based structured CRF tagger and your task now is to implement a feature extraction function called `char2features`. It takes two arguments:\n",
    "\n",
    "1. an example in BIES format (for example, `[[\"d\",\"BEGIN\"],[\"o\",\"INSIDE\"],[\"g\",\"END\"],[\"s\",\"SINGLE\"]]`) and\n",
    "1. a position in the examples (for example, 2).\n",
    "\n",
    "As a very simple example consider:\n",
    "\n",
    "```\n",
    "def char2features(example, i):\n",
    "    char_at_i = example[i][0]\n",
    "    features = [\"CHARACTER_AT_i=%s\" % char_at_i]\n",
    "    return features\n",
    "```\n",
    "\n",
    "Given the arguments `example = [[\"d\",\"BEGIN\"],[\"o\",\"INSIDE\"],[\"g\",\"END\"],[\"s\",\"SINGLE\"]]` and `i = 2`, this function will return `[\"CHARACTER_AT_i=g\"]`. Although this is a possible feature function, it is probably too simplistic to ensure good segmentation accuracy. You should expand it to include additional features like:\n",
    "\n",
    "1. Surrounding characters around position i.\n",
    "1. Substrings around position i of varying lengths.\n",
    "1. Distance to the end of the example and beginning of the example\n",
    "1. Anything else you can come to think of. \n",
    "\n",
    "You are allowed to use external datasets for feature engineering. If you do, please make sure to hand in your datasets together with the assignment.\n",
    "\n",
    "**NOTE!** `char2features` is not allowed to look at the labels (for example, `\"INSIDE\"`). You may only extract features from the characters in `example`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "oppress or\n",
    "    *\n",
    "\n",
    "o BEGIN\n",
    "p INSIDE\n",
    "p INSIDE\n",
    "r INSIDE    \n",
    "e INSIDE *  <-- current position\n",
    "s INSIDE \n",
    "s END \n",
    "o BEGIN\n",
    "r END\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Feature engineering**:\n",
    "- current character at i: $w_i$ = *e*\n",
    "- surrounding characters around position i: $w_{i-1}$ = *r*, $w_{i-2}$ = *p*, $w_{i+1}$ = *s*, $w_{i+2}$ = *s*\n",
    "- substrings around position i of varying lengths: $w_{i-2,i-1,i}$ = *pre*, $w_{i,i+1,i+2}$ = *ess*, $w_{i-1,i,i+1}$ = *res*\n",
    "- distance to the end of the example and beginning of the example: $d = 4$ \n",
    "- anything else you can come to think of ....\n",
    "\n",
    "\n",
    "```\n",
    "[   [\n",
    "        ...     ], \n",
    "    [   ['CHAR=o', ...], \n",
    "        ['CHAR=p', ...], \n",
    "        ['CHAR=p', ...], \n",
    "        ['CHAR=r', ...], \n",
    "        ['CHAR=e', 'CHAR-1=r', 'CHAR-2=p', 'CHAR+1=s', 'CHAR+2=s', 'STR--=pre', 'STR++=ess',\n",
    "            'STR-+=res', 'DIST_FROM_START=4'], \n",
    "        ['CHAR=s', ...], \n",
    "        ...     ],\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUNDARY=\"<BD>\"\n",
    "\n",
    "def char2features(example, i):\n",
    "    # your code here\n",
    "\n",
    "    # your code here\n",
    "    return features\n",
    "\n",
    "def data2features(data):\n",
    "    \"\"\" Extract features for a data set in BIES format. \"\"\"\n",
    "    return [[char2features(example,i) for i in range(len(example))] for example in data]\n",
    "\n",
    "def data2labels(data):\n",
    "    \"\"\" Extract the tags from a data set in BIES format. \"\"\"\n",
    "    return [[tok[1] for tok in example] for example in data]\n",
    "\n",
    "# Initialize the training, development and test sets for pycrfsuite.\n",
    "X_train = data2features(trainbies)\n",
    "y_train = data2labels(trainbies)\n",
    "\n",
    "X_dev = data2features(devbies)\n",
    "y_dev = data2labels(devbies)\n",
    "\n",
    "X_test = data2features(testbies)\n",
    "y_test = data2labels(testbies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4.2: Training the segmentation system\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "Find out how to train a `pycrfsuite` model in the [tutorial](https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb) (for now, just copy the hyper-parameter values from the tutorial). You should train your model using `X_train` and `y_train` which we just created. Save your model in the file `segmentation.model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the segmentation system**  See https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n",
    "\n",
    "\n",
    "`trainer.train(’segmentation.model’)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 11041\n",
      "Seconds required: 0.033\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 1.000000\n",
      "c2: 0.001000\n",
      "num_memories: 6\n",
      "max_iterations: 50\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "***** Iteration #1 *****\n",
      "Loss: 11501.826562\n",
      "Feature norm: 1.000000\n",
      "Error norm: 3365.016869\n",
      "Active features: 5599\n",
      "Line search trials: 1\n",
      "Line search step: 0.000187\n",
      "Seconds required for this iteration: 0.008\n",
      "\n",
      "***** Iteration #2 *****\n",
      "Loss: 7643.037593\n",
      "Feature norm: 3.135730\n",
      "Error norm: 2641.393094\n",
      "Active features: 5285\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #3 *****\n",
      "Loss: 5919.984383\n",
      "Feature norm: 4.274837\n",
      "Error norm: 1174.072773\n",
      "Active features: 4904\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #4 *****\n",
      "Loss: 5057.965647\n",
      "Feature norm: 5.372313\n",
      "Error norm: 1083.340613\n",
      "Active features: 4684\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #5 *****\n",
      "Loss: 3834.483256\n",
      "Feature norm: 7.517508\n",
      "Error norm: 1236.910324\n",
      "Active features: 3905\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #6 *****\n",
      "Loss: 3430.353564\n",
      "Feature norm: 8.750770\n",
      "Error norm: 1176.713108\n",
      "Active features: 3826\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #7 *****\n",
      "Loss: 3189.047727\n",
      "Feature norm: 9.225426\n",
      "Error norm: 420.253692\n",
      "Active features: 3916\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #8 *****\n",
      "Loss: 2998.458869\n",
      "Feature norm: 9.960309\n",
      "Error norm: 774.570116\n",
      "Active features: 3761\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #9 *****\n",
      "Loss: 2770.452755\n",
      "Feature norm: 11.073526\n",
      "Error norm: 789.116418\n",
      "Active features: 3640\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #10 *****\n",
      "Loss: 2613.395519\n",
      "Feature norm: 12.154972\n",
      "Error norm: 831.072223\n",
      "Active features: 3571\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #11 *****\n",
      "Loss: 2476.654595\n",
      "Feature norm: 13.386300\n",
      "Error norm: 758.178938\n",
      "Active features: 3527\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #12 *****\n",
      "Loss: 2358.875450\n",
      "Feature norm: 13.813885\n",
      "Error norm: 358.471505\n",
      "Active features: 3515\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #13 *****\n",
      "Loss: 2325.605253\n",
      "Feature norm: 14.621075\n",
      "Error norm: 636.618333\n",
      "Active features: 3460\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #14 *****\n",
      "Loss: 2245.807498\n",
      "Feature norm: 15.097089\n",
      "Error norm: 324.734687\n",
      "Active features: 3431\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #15 *****\n",
      "Loss: 2201.147917\n",
      "Feature norm: 15.627877\n",
      "Error norm: 353.502521\n",
      "Active features: 3374\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #16 *****\n",
      "Loss: 2160.663054\n",
      "Feature norm: 16.101209\n",
      "Error norm: 359.396459\n",
      "Active features: 3334\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #17 *****\n",
      "Loss: 2120.314717\n",
      "Feature norm: 16.690157\n",
      "Error norm: 399.772520\n",
      "Active features: 3257\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #18 *****\n",
      "Loss: 2079.735387\n",
      "Feature norm: 16.983601\n",
      "Error norm: 284.384746\n",
      "Active features: 3188\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #19 *****\n",
      "Loss: 2051.899382\n",
      "Feature norm: 17.659279\n",
      "Error norm: 399.076347\n",
      "Active features: 3110\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #20 *****\n",
      "Loss: 2013.216791\n",
      "Feature norm: 18.046605\n",
      "Error norm: 321.555353\n",
      "Active features: 3020\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #21 *****\n",
      "Loss: 1980.995964\n",
      "Feature norm: 18.659157\n",
      "Error norm: 322.189064\n",
      "Active features: 2950\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #22 *****\n",
      "Loss: 1954.965927\n",
      "Feature norm: 19.013428\n",
      "Error norm: 327.080815\n",
      "Active features: 2878\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #23 *****\n",
      "Loss: 1930.131514\n",
      "Feature norm: 19.694242\n",
      "Error norm: 311.581487\n",
      "Active features: 2821\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #24 *****\n",
      "Loss: 1906.017435\n",
      "Feature norm: 20.020491\n",
      "Error norm: 251.861859\n",
      "Active features: 2761\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #25 *****\n",
      "Loss: 1885.725467\n",
      "Feature norm: 20.497101\n",
      "Error norm: 248.673211\n",
      "Active features: 2702\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #26 *****\n",
      "Loss: 1866.293025\n",
      "Feature norm: 20.923574\n",
      "Error norm: 270.029307\n",
      "Active features: 2621\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #27 *****\n",
      "Loss: 1848.093472\n",
      "Feature norm: 21.652176\n",
      "Error norm: 297.573079\n",
      "Active features: 2552\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #28 *****\n",
      "Loss: 1826.533921\n",
      "Feature norm: 22.091453\n",
      "Error norm: 229.650867\n",
      "Active features: 2506\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #29 *****\n",
      "Loss: 1811.202797\n",
      "Feature norm: 22.699654\n",
      "Error norm: 240.808747\n",
      "Active features: 2447\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #30 *****\n",
      "Loss: 1795.365258\n",
      "Feature norm: 23.078343\n",
      "Error norm: 216.001613\n",
      "Active features: 2422\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #31 *****\n",
      "Loss: 1782.206576\n",
      "Feature norm: 23.727008\n",
      "Error norm: 218.039448\n",
      "Active features: 2377\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #32 *****\n",
      "Loss: 1769.829926\n",
      "Feature norm: 24.022204\n",
      "Error norm: 173.261727\n",
      "Active features: 2333\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #33 *****\n",
      "Loss: 1759.856886\n",
      "Feature norm: 24.513239\n",
      "Error norm: 145.576273\n",
      "Active features: 2295\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #34 *****\n",
      "Loss: 1751.039183\n",
      "Feature norm: 24.762845\n",
      "Error norm: 110.941998\n",
      "Active features: 2262\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #35 *****\n",
      "Loss: 1742.836166\n",
      "Feature norm: 25.199635\n",
      "Error norm: 145.926988\n",
      "Active features: 2210\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #36 *****\n",
      "Loss: 1734.042112\n",
      "Feature norm: 25.603955\n",
      "Error norm: 179.329349\n",
      "Active features: 2150\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #37 *****\n",
      "Loss: 1722.220709\n",
      "Feature norm: 26.101183\n",
      "Error norm: 117.873894\n",
      "Active features: 2089\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.004\n",
      "\n",
      "***** Iteration #38 *****\n",
      "Loss: 1714.333865\n",
      "Feature norm: 26.301154\n",
      "Error norm: 124.855887\n",
      "Active features: 2052\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #39 *****\n",
      "Loss: 1705.005398\n",
      "Feature norm: 26.645137\n",
      "Error norm: 95.526957\n",
      "Active features: 1997\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #40 *****\n",
      "Loss: 1696.031395\n",
      "Feature norm: 26.795464\n",
      "Error norm: 110.487157\n",
      "Active features: 1923\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #41 *****\n",
      "Loss: 1687.155746\n",
      "Feature norm: 27.013347\n",
      "Error norm: 114.021749\n",
      "Active features: 1862\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #42 *****\n",
      "Loss: 1679.850605\n",
      "Feature norm: 27.047681\n",
      "Error norm: 126.298017\n",
      "Active features: 1803\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #43 *****\n",
      "Loss: 1671.854543\n",
      "Feature norm: 27.199986\n",
      "Error norm: 58.800971\n",
      "Active features: 1757\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #44 *****\n",
      "Loss: 1665.702881\n",
      "Feature norm: 27.180573\n",
      "Error norm: 109.702271\n",
      "Active features: 1690\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #45 *****\n",
      "Loss: 1658.380359\n",
      "Feature norm: 27.254796\n",
      "Error norm: 69.343952\n",
      "Active features: 1649\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #46 *****\n",
      "Loss: 1652.620664\n",
      "Feature norm: 27.135555\n",
      "Error norm: 84.370081\n",
      "Active features: 1583\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #47 *****\n",
      "Loss: 1646.786948\n",
      "Feature norm: 27.142508\n",
      "Error norm: 39.808944\n",
      "Active features: 1518\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #48 *****\n",
      "Loss: 1642.097870\n",
      "Feature norm: 27.092484\n",
      "Error norm: 47.728041\n",
      "Active features: 1447\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #49 *****\n",
      "Loss: 1636.729810\n",
      "Feature norm: 27.199248\n",
      "Error norm: 57.612566\n",
      "Active features: 1370\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "***** Iteration #50 *****\n",
      "Loss: 1632.716245\n",
      "Feature norm: 27.311487\n",
      "Error norm: 45.929854\n",
      "Active features: 1314\n",
      "Line search trials: 1\n",
      "Line search step: 1.000000\n",
      "Seconds required for this iteration: 0.005\n",
      "\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 0.232\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 1314 (11041)\n",
      "Number of active attributes: 846 (8400)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "trainer.train('segmentation.model')\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4.3: Fine-tuning and segmentation\n",
    "\n",
    "rubric={accuracy:5}\n",
    "\n",
    "You can now segment the development data `devbies` using the function `segment` below. You should get f-score above 84% on the development data. Please tune the L1 and L2 penalty for `pycrfsuite.Trainer` using segmentation f-score on the development data as criterion. Remember that you will need to retrain `tagger` after adjusting the hyperparameter values.\n",
    "\n",
    "When you are done with tuning, you can segment the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To prevent overfitting**: \n",
    "- L1 regularization technique is called Lasso Regression:  Least Absolute Shrinkage and Selection Operator (Lasso) adds “absolute value of magnitude” of coefficient as penalty term to the loss function\n",
    "- L2 is called Ridge Regression, which adds “squared magnitude” of coefficient as penalty term \n",
    "\n",
    "\n",
    "`pycrfsuite.Trainer`:\n",
    "```\n",
    "    'c1': float,  # coefficient for L1 penalty\n",
    "    'c2': float,  # coefficient for L2 penalty\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for supervised segmentation:\n",
      "Development set precision: 81.82, recall: 90.00, f-score: 85.71\n",
      "Results for supervised segmentation:\n",
      "Test set precision: 87.02, recall: 79.74, f-score: 83.22\n"
     ]
    }
   ],
   "source": [
    "def segment(data,tagger):\n",
    "    tagged_data = []\n",
    "    for i,example in enumerate(data2features(data)):\n",
    "        tags = tagger.tag(example)\n",
    "        tagged_example = [(char, tag) for (char, _), tag in zip(data[i],tags)]\n",
    "        tagged_data.append(tagged_example)\n",
    "    return [segmented for _,segmented in unbies(tagged_data)]\n",
    "    \n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('segmentation.model')\n",
    "\n",
    "supervised_tokenized_dev = segment(devbies,tagger)\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Development set precision: %.2f, recall: %.2f, f-score: %.2f\" % evaluate(supervised_tokenized_dev[:10],devdata[:10]))\n",
    "supervised_tokenized_dev[:10]\n",
    "\n",
    "# When you're done tuning the hyper-parameters, uncomment the lines below to segment the test data.\n",
    "\n",
    "supervised_tokenized_test = segment(testbies,tagger)\n",
    "print(\"Results for supervised segmentation:\")\n",
    "print(\"Test set precision: %.2f, recall: %.2f, f-score: %.2f\" % evaluate(supervised_tokenized_test,testdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that these f-scores are examplary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
