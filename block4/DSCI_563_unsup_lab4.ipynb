{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 563 Lab Assignment 4: Information Retrieval (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will\n",
    "\n",
    "- Set up a Whoosh index using newsgroup posts \n",
    "- Build some basic information retrieval systems and compare them to Whoosh output\n",
    "\n",
    "\n",
    "## What does this have to do with what we are doing in class?\n",
    "\n",
    "In this lab you will be using Whoosh, just like we saw in class. You'll then build an alternative search engine that does the stuff that Whoosh is doing for you.\n",
    "\n",
    "Some things to keep in mind:\n",
    "\n",
    "* By default, the Whoosh parsers and analyzers will combine all search terms with \"AND\".  If you want to have an OR or a NOT, you will need to include them (all uppercase) as well as appropriate parentheses, in the query.\n",
    "* Debug thoroughly!  Make sure that when you run a query through a parser, it's producing what you would expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: whoosh in /Users/jungyeul/Library/Python/3.9/lib/python/site-packages (2.7.4)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "import sys\n",
    "!{sys.executable} -m pip install whoosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to access relevant modules (you can add to this as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "import numpy as np\n",
    "from math import log\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from collections import defaultdict,Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from whoosh.qparser import *\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, NUMERIC\n",
    "from whoosh.analysis import RegexTokenizer, LowercaseFilter, StopFilter, StemFilter ### The analyzers tokenize and stems the data\n",
    "from whoosh import index ### We'll create an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)\n",
    "- This is a *paired* lab.  Please pair up with another student - I recommend working with someone you haven't submitted a lab with, yet.  Always working with the same partner means that you come to rely on their skills, and don't get exposed to any other skill sets.  Everyone in the cohort has similar goals, but different strengths and weaknesses - there are no \"weak\" partners.\n",
    "- Create a repo and grant access to all instructors.  You only need to submit a single lab for your team.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Whoosh index\n",
    "\n",
    "In this lab, we will be using a classic IR corpus, the \"20 newsgroups\" dataset which consists of over 11k posts from 20 newsgroups, [accessible through sklearn](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html). The provided code below extracts the information from the original plaintext format into a list of dictionaries `newsgroup_info_dicts`, where the dictionary for each post includes not just the `Text` body, but also the `Newsgroup`, the original poster (`From`), their organization (`Organization`) and the `Subject` of the post.  Before moving on to the later parts of the lab, take a look at what the post looks like - you'll be indexing specific information in the post, so it is important that you understand how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH:  11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'From': \"lerxst@wam.umd.edu (where's my thing)\",\n",
       " 'Subject': 'WHAT car is this!?',\n",
       " 'Nntp-Posting-Host': 'rac3.wam.umd.edu',\n",
       " 'Organization': 'University of Maryland, College Park',\n",
       " 'Lines': '15',\n",
       " 'Text': 'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.',\n",
       " 'Newsgroup': 'rec.autos'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provided code\n",
    "def get_post_info_dict(full_text):\n",
    "    '''given a newsgroup post in typical format, extracts the header as a \n",
    "    dict, including the body of the text in the \"text\" field'''\n",
    "    info_dict = {}\n",
    "    header_boundry = full_text.find(\"\\n\\n\")  ### Header is separated by 2 newlines\n",
    "    for line in full_text[:header_boundry].split(\"\\n\"): ### Split the header by lines\n",
    "        first_colon = line.find(\": \") ### Everything before the colon is a topic (ie, author, subject, etc.); everything after is more information\n",
    "        info_dict[line[:first_colon]] = line[first_colon +2:]\n",
    "    info_dict[\"Text\"] = full_text[header_boundry + 2:].strip()\n",
    "    return info_dict\n",
    "\n",
    "newsgroups = fetch_20newsgroups(remove=('footers', 'quotes'))\n",
    "newsgroup_info_dicts = []\n",
    "print(\"LENGTH: \", len(newsgroups.data))\n",
    "for i in range(len(newsgroups.data)):\n",
    "    info_dict =  get_post_info_dict(newsgroups.data[i])\n",
    "    info_dict[\"Newsgroup\"] = newsgroups.target_names[newsgroups.target[i]]\n",
    "    if 20 < len(info_dict[\"Text\"]) < 50000: # get rid of very big or very small texts\n",
    "        newsgroup_info_dicts.append(info_dict)\n",
    "newsgroup_info_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `text` for the main text, \n",
    "* `newsgroup` for the name of the newsgraoup,\n",
    "* `subject` for the title of the post, \n",
    "* `author` for the author, and \n",
    "* `organization` which contains tha name of the poster's organization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 \n",
    "rubric={accuracy:2}\n",
    "\n",
    "First, you're going to define two different analyzers:\n",
    "\n",
    "You should not tokenize the author or organization fields, so you can store them in the `Keyword` data type.\n",
    "1. An analyzer for the text and subject fields (`text_analyzer`) which tokenizes, lowercases, removes stopwords, and stems, in that order (if you put the stemmer first, it will block some stopword removal!  Think a bit about why this happens.).\n",
    "2. An analyzer for the newgroup fields (`group_analyzer`) which tokenizes on \".\". This will tokenize newsgroups such as \"comp.os.ms-windows.misc\" into \"comp\", \"os\", \"ms-windows\", \"misc\". No filters needed for this analyzer.  Hint: The regex tokenizer is looking for a pattern to match in each of its words - not the pattern to split on.  How will you write a regex that captures everything *except* the period?\n",
    "\n",
    "You will then create a schema that contains the analyzers for the text, subject, author, and organization fields.\n",
    "\n",
    "You will not be tokenizing the author or organization fields, so you can use the KEYWORD analyzer (This is very similar to how we stored the \"genre\" type in class - by specifying KEYWORD, you're telling the index not to do any special parsing).  \n",
    "\n",
    "I recommend that you create a `QueryParser` for the text, group, author, and organization, and test that they are parsing correctly. \n",
    "For example, `TextParser=QueryParser(\"text\", schema=schema)` where `schema` is `Schema(text=Text(analyzer= ...))`\n",
    "\n",
    "\"We are testing the parser\" in the text should return (text:test AND text:parser), while \"comp.os.ms-windows.misc\" in the group should return (group:comp AND group:os AND group:ms-windows AND group:misc).  The author and organization should only split by word.\n",
    "\n",
    "If you're curious, test queries that have \"OR\", and \"NOT\", as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analyzer = \n",
    "group_analyzer = \n",
    "schema = Schema(text=TEXT(analyzer=text_analyzer, stored=True), ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(text:test AND text:parser)\n",
      "(group:comp AND group:os AND group:ms-windows AND group:misc)\n",
      "(author:Charles AND author:Dickens)\n",
      "organization:Microsoft\n",
      "(subject:floppi AND subject:disk)\n"
     ]
    }
   ],
   "source": [
    "TextParser = \n",
    "GroupParser = \n",
    "AuthorParser = \n",
    "OrganizationParser = \n",
    "SubjectParser = \n",
    "\n",
    "print(TextParser.parse(\"We are testing the parser\"))\n",
    "print(GroupParser.parse(\"comp.os.ms-windows.misc\"))\n",
    "print(AuthorParser.parse(\"Charles Dickens\"))\n",
    "print(OrganizationParser.parse(\"Microsoft\"))\n",
    "print(SubjectParser.parse(\"FloPPY Disk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2 \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Now, create the index for your posts and iterate over the `newsgroup_info_dicts` to add each post to your index. You should give each of the posts an`id` equal to its index in `newsgroup_info_dicts`.  The index creates an index in the specified directory.  Make sure to call writer.commit() after adding the documents to the index!  If you don't, and you lock the index, you can usually reset things by restartng your kernel.  Keep in mind that every time you call create_in, it will overwrite your old index. The newsgroup_info_dicts from the last part has ~11K entries, so you might want to create a status-tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from whoosh.index import create_in\n",
    "\n",
    "\n",
    "# mkdir, then create_in; \n",
    "\n",
    "# open_dir\n",
    "\n",
    "# create a writer object to add documents to the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "for id, info_dict in enumerate(newsgroup_info_dicts):\n",
    "    if id%1000 == 0: \n",
    "        print(id)\n",
    "\n",
    "    # get from the diction for each \n",
    "        \n",
    "    # then, add_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, you should `commit`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3 \n",
    "rubric={accuracy:1}\n",
    "\n",
    "Test your index by printing out all the authors of posts in one of the \"comp\" newsgroups (there are more than one) whose subject contains the word *floppy*. There should be 10 of them, and one of them is named *Stig*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Top 10 Results for And([Term('group', 'comp'), Term('subject', 'floppi')]) runtime=0.00213558399991598>\n",
      "limagen@hpwala.wal.hp.com\n",
      "jdresser@altair.tymnet.com (Jay Dresser)\n",
      "towwang@statler.engin.umich.edu (Tow Wang Hui)\n",
      "venaas@flipper.pvv.unit.no (Stig Venaas)\n",
      "balog@eniac.seas.upenn.edu (Eric J Balog)\n",
      "tcking@uswnvg.com (Tim King)\n",
      "jtrascap@nyx.cs.du.edu (Jim Trascapoulos)\n",
      "bagels@gotham.East.Sun.COM (Alex Beigelman - NYC SE)\n",
      "flyboy@spf.trw.com (Jeff Wright)\n",
      "cctr132@csc.canterbury.ac.nz (Nick FitzGerald, PC Software Consultant, CSC, UoC, NZ)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "parser = QueryParser(\"text\",schema=schema)\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    # TextParse: \"comp\" newsgroups & the word *floppy*\n",
    "    \n",
    "    # search, where the results object acts like a list of the matched documents.\n",
    "    results = \n",
    "    print (results)\n",
    "    \n",
    "    for r in results:\n",
    "        # print author; \n",
    "\n",
    "# print length; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Boolean IR\n",
    "\n",
    "In this exercise, you will create your own boolean IR system and make sure it works the same way as Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "First, create an inverted index (`inverted_index`) for the 20 newsgroup corpus using a Python (default)dict. As you did above, use the index in newsgroup_info_dicts. In order to make the preprocessing consistent with Elasticsearch, you will need to use the `text_analyzser.simulate(text)` method of the text analyzer you built in 1.1, and deal with the somewhat messy output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# # your code here\n",
    "# def get_tokens(text, analyzer):\n",
    "#     '''applies the Whoosh analyzer on the Text field of the info_dict\n",
    "#     and returns a list of tokens'''\n",
    "#     ...\n",
    "#     tokens = []\n",
    "   \n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    " \n",
    "# iterate newsgroup_info_dicts\n",
    "#   iterate your tokens (`get_tokens`)\n",
    "#        then, add document to inverted_index[token]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3969, 1026, 9091, 6660, 10372, 904, 1802, 3339, 4492, 10122, 5906, 7955, 7572, 9086, 2838, 2456, 2201, 10779, 9628, 1053, 4773, 8613, 5674, 4526, 7471, 7726, 3889, 8758, 10039, 10166, 2877, 10814, 6719, 8255, 9284, 5319, 9543, 4429, 6093, 4815, 9294, 2513, 5973, 5975, 5848, 6615, 1626, 218, 7644, 94, 3835, 9823, 8673, 6754, 8546, 484, 101, 9952, 5738, 9580, 4079, 112, 4720, 6768, 758, 6518, 10486, 2811, 9084, 126}\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index[\"dog\"])\n",
    "print(len(inverted_index[\"dog\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert \"the\" not in inverted_index\n",
    "assert len(inverted_index[\"dog\"]) == 70 \n",
    "assert 94 in inverted_index[\"dog\"]\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "rubric={accuracy:3,efficiency:1}\n",
    "\n",
    "Now you're going to create a boolean IR engine. You should complete the recursive `get_docs` function which takes your inverted index and a search \"expression\" and returns a set of all document ids which satisfy the expression. An expression is either a string (a single word), or a 2-uple where the first element is a boolean operator (\"or\",\"and\",or \"not\") and the second is a either another a tuple of expressions (if the operator is \"or\" or \"and\") or a single expression if the operator is \"not\" (HINT: for \"not\", a set consisting of all the document ids will be useful, and is provided below). For example, the following call to `get_docs` for **_documents that contain the word *'hit'* and not any of the words *'base'*, *'ball'*, or *'run'*_**:\n",
    "\n",
    "`get_docs(inverted_index,(\"and\",(\"hit\",(\"not\", (\"or\", (\"base\",\"ball\", \"run\")))))))`\n",
    "\n",
    "Means that you want documents that contain the word *hit* and not any of the words *base*, *ball*, or *run*.  \n",
    "\n",
    "The base case which gets the relevant documents for a single word using your inverted index is provided for you. You'll want to use set operators extensively here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = set(range(len(newsgroup_info_dicts)))\n",
    "\n",
    "def get_docs(inverted_index,expression):\n",
    "    '''given an inverted index which provides a mapping from words to documents \n",
    "    in a collection, evaluates expression according to boolean logic and \n",
    "    returns a list of documents for which the expression holds\n",
    "    '''\n",
    "    if isinstance(expression, str):\n",
    "        return inverted_index.get(expression, set())\n",
    "    else:\n",
    "        operator_type,operands = expression\n",
    "        # your code here\n",
    "        if operator_type == \"not\":\n",
    "            # results = all_doc - get_doc's result\n",
    "            return all_docs - ...\n",
    "            \n",
    "        elif operator_type == \"and\":\n",
    "            # results = ...\n",
    "            return results\n",
    "\n",
    "        elif operator_type == \"or\":\n",
    "            # results = ...\n",
    "            return results\n",
    "        # your code here    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "rubric={accuracy:1,quality:1}\n",
    "\n",
    "Here, you will create tests which assert that the output of your boolean IR system is the same as Elasticsearch. You should have at least three tests:\n",
    "\n",
    "1. Documents that contain the word *hit*\n",
    "2. Documents that contain the words *hit*, *home*, and *run*\n",
    "3. Documents that contain the word *hit* and not any of the words *base*, *ball*, or *run*. \n",
    "\n",
    "You should write a function that takes a Whoosh search and returns a set of document ids, the same output as `get_docs` (so it is easy to compare). Remember that you'll need to increase the default number of returned hits for Whoosh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_ids(s):\n",
    "    '''get a set of ids returned from a ElasticSearch search'''\n",
    "    ids = set()\n",
    "\n",
    "    # ids is a set for your result file ids\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoolIR = get_docs(inverted_index, (\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{94,\n",
       " 101,\n",
       " 112,\n",
       " 126,\n",
       " 218,\n",
       " 484,\n",
       " 758,\n",
       " 904,\n",
       " 1026,\n",
       " 1053,\n",
       " 1626,\n",
       " 1802,\n",
       " 2201,\n",
       " 2456,\n",
       " 2513,\n",
       " 2811,\n",
       " 2838,\n",
       " 2877,\n",
       " 3339,\n",
       " 3835,\n",
       " 3889,\n",
       " 3969,\n",
       " 4079,\n",
       " 4429,\n",
       " 4492,\n",
       " 4526,\n",
       " 4720,\n",
       " 4773,\n",
       " 4815,\n",
       " 5319,\n",
       " 5674,\n",
       " 5738,\n",
       " 5848,\n",
       " 5906,\n",
       " 5973,\n",
       " 5975,\n",
       " 6093,\n",
       " 6518,\n",
       " 6615,\n",
       " 6660,\n",
       " 6719,\n",
       " 6754,\n",
       " 6768,\n",
       " 7471,\n",
       " 7572,\n",
       " 7644,\n",
       " 7726,\n",
       " 7955,\n",
       " 8255,\n",
       " 8546,\n",
       " 8613,\n",
       " 8673,\n",
       " 8758,\n",
       " 9084,\n",
       " 9086,\n",
       " 9091,\n",
       " 9284,\n",
       " 9294,\n",
       " 9543,\n",
       " 9580,\n",
       " 9628,\n",
       " 9823,\n",
       " 9952,\n",
       " 10039,\n",
       " 10122,\n",
       " 10166,\n",
       " 10372,\n",
       " 10486,\n",
       " 10779,\n",
       " 10814}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoolIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "WhooshIR = get_hit_ids(ix.searcher(), \"dog\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{94,\n",
       " 101,\n",
       " 112,\n",
       " 126,\n",
       " 218,\n",
       " 484,\n",
       " 758,\n",
       " 904,\n",
       " 1026,\n",
       " 1053,\n",
       " 1626,\n",
       " 1802,\n",
       " 2201,\n",
       " 2456,\n",
       " 2513,\n",
       " 2811,\n",
       " 2838,\n",
       " 2877,\n",
       " 3339,\n",
       " 3835,\n",
       " 3889,\n",
       " 3969,\n",
       " 4079,\n",
       " 4429,\n",
       " 4492,\n",
       " 4526,\n",
       " 4720,\n",
       " 4773,\n",
       " 4815,\n",
       " 5319,\n",
       " 5674,\n",
       " 5738,\n",
       " 5848,\n",
       " 5906,\n",
       " 5973,\n",
       " 5975,\n",
       " 6093,\n",
       " 6518,\n",
       " 6615,\n",
       " 6660,\n",
       " 6719,\n",
       " 6754,\n",
       " 6768,\n",
       " 7471,\n",
       " 7572,\n",
       " 7644,\n",
       " 7726,\n",
       " 7955,\n",
       " 8255,\n",
       " 8546,\n",
       " 8613,\n",
       " 8673,\n",
       " 8758,\n",
       " 9084,\n",
       " 9086,\n",
       " 9091,\n",
       " 9284,\n",
       " 9294,\n",
       " 9543,\n",
       " 9580,\n",
       " 9628,\n",
       " 9823,\n",
       " 9952,\n",
       " 10039,\n",
       " 10122,\n",
       " 10166,\n",
       " 10372,\n",
       " 10486,\n",
       " 10779,\n",
       " 10814}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhooshIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoolIR == WhooshIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "#tests here\n",
    "#your code here\n",
    "assert get_docs(inverted_index,(\"hit\")) ==  get_hit_ids(ix.searcher(), \"hit\", None)\n",
    "print(\"Success!\")\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Document relevance with Okapi BM25\n",
    "\n",
    "In this exercise, you are again going to mimic the output of Elasticsearch by implementing [Okapi BM25 document relevance](https://en.wikipedia.org/wiki/Okapi_BM25), which a special version of tf-idf (not the same used in sklearn!), as well as a simplified version of the vector-space model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1\n",
    "rubric={accuracy:2,quality:1}\n",
    "\n",
    "First you are going to calculate the *idf* part of the equation, as well as the average length of the texts (after preprocessing) which will be used in 3.2. First iterate over the corpus (`newsgroup_info_dicts`) and, after again using `simulate` for the Elasticsearch text analyzer, calculate an initial document frequency for each term, as well as the average length. Then, iterate over your df dict and create an corresponding idf dict. For Okapi BM25, idf is calculated as follows:\n",
    "\n",
    "$$\\text{IDF}(q_i) = \\ln (\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}+1)$$\n",
    "\n",
    "where $q_i$ is the term (word type), $n(q_i)$ its document frequency, and $N$ the total number of documents. \n",
    "For quality, it's a good idea to have a function which calculates this idf. \n",
    "**Therefore, you implement `calculate_idf(n, N)` where `N` is `N` (total number of documents), and `n` is `n(q_i)` (document frequency).** \n",
    "You can use the Elasticsearch `explain` method shown in lecture to get some test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1823215567939546\n"
     ]
    }
   ],
   "source": [
    "def calculate_idf(n, N):\n",
    "    '''calculates Okapi BM25 idf based on the df n and the total number of\n",
    "    documents N'''\n",
    "    # your code here\n",
    "    idf = log( ... )\n",
    "    return idf \n",
    "\n",
    "print(calculate_idf(2, 2))  ### Should be 0.1823215567939546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `total_length` = all tokens in `newsgroup_info_dicts` (note that you should use `text_analyzer` as before: `text_analyzser.simulate(text)`)\n",
    "- `df`, document frequency of the token (hint: use `Counter()`), which will be represented as `n`\n",
    "- `N` = length of `newsgroup_info_dicts`\n",
    "- `avg_length = total_length/N`\n",
    "- `idf_dict` by `calculate_idf(n, N)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "dfs = Counter()                         # by text_analyzer.simulate(info_dict[\"Text\"])[\"tokens\"]`\n",
    "total_length = 0                        # total_length of dfs\n",
    "N = len(newsgroup_info_dicts)\n",
    "\n",
    "# iterate newsgroup_info_dicts to get_tokens and update dfs (and total_length); \n",
    "\n",
    "avg_length = total_length/N\n",
    "\n",
    "idf_dict = {}\n",
    "\n",
    "# then, iterate dfs for idf using `calculate_idf(n, N)`;\n",
    "# N is given\n",
    "# n is an item in dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "rubric={accuracy:2,quality:1}\n",
    "\n",
    "Now for the *tf* part, which will need to be calculated for each document. Iterate through the corpus again, and, for each document, first build a tf dictionary (a count of terms in the document), and then use that and the document frequencies you calculated in 3.1 to assign a tf-idf value for each term in each document. You are not using the raw term frequency, but a special term frequency calculated as follows:\n",
    "\n",
    "$$\\text{BM25-tf}(D,Q) = \\frac{f(q_i, D)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}$$\n",
    "\n",
    "Where $q_i$ is the current term, f(q_i, D) is its frequency in the current document (the term frequency you have calculated), k_1 is the term saturation parameter with a (default) value of 1.2, b is the the length normalization parameter with a (default) value of 0.75 (you can just use these values directly, you don't need to make them parameters here), $|D|$ is the token length of the current document, and $avgdl$ is the average document length you calculated in 3.1. \n",
    "Again, you should have a separate function which calculates this special BM25-tf, again with some test cases which should ideally come from Elasticsearch `explain`. \n",
    "**Therefore, you implement `BM25_tf(tf, doc_length, avg_length)` where `tf` is $f(q_i, D)$, `doc_length` is $|D|$, and `avg_length` is avgdl**\n",
    "Then you should multiply it by the idf for that term you calculated in **3.1** to get a tf-idf score for that term in that document. When you have created a dictionary of all tf-idf scores in a document, append that dictionary to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k_1 = 1.2\n",
    "# b = 0.75\n",
    "\n",
    "# def BM25_tf(tf, doc_length, avg_length):\n",
    "#     # your code here\n",
    "#     tf_value = tf / (tf + ... )\n",
    "#     return tf_value\n",
    "\n",
    "BM25_tf(2,9,9) ### Should be 0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each item in `newsgroup_info_dicts`:\n",
    "- `tfidf_dict = BM25_tf * idf_dict` for each term (type)\n",
    "- use `text_analyzser.simulate(text)` to get all tokens\n",
    "- `doc_length` for the length of tokens\n",
    "- `tf_dict`  for all terms\n",
    "- finally, `tfidfed_corpus.append(tfidf_dict)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "tfidfed_corpus = []\n",
    "\n",
    "for info_dict in newsgroup_info_dicts:\n",
    "    tfidf_dict = {}\n",
    "    ...\n",
    "    doc_length = ...\n",
    "    for term,value in tf_dict.items():\n",
    "        tfidf_dict[term] = BM25_tf(value, doc_length, avg_length)*idf_dict[term]            # avg_length (from 3.1)\n",
    "    \n",
    "    tfidfed_corpus.append(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wa': 1.0479337681490617,\n",
       " 'wonder': 1.8735039967194669,\n",
       " 'anyone': 1.5475372870680149,\n",
       " 'out': 0.8636541627604866,\n",
       " 'there': 0.7142514227210075,\n",
       " 'could': 1.1068030003948344,\n",
       " 'enlighten': 3.484161348259486,\n",
       " 'me': 0.8433127347025031,\n",
       " 'car': 2.666514410203664,\n",
       " 'saw': 2.1538572662151334,\n",
       " 'other': 0.8976704330308405,\n",
       " 'dai': 1.4743743759915375,\n",
       " 'door': 3.1764122676129545,\n",
       " 'sport': 2.6546920772093427,\n",
       " 'look': 1.4531881704497596,\n",
       " 'late': 2.304552087215622,\n",
       " '60': 2.3159553446871413,\n",
       " 'earli': 2.1692814584620717,\n",
       " '70': 2.555079292665207,\n",
       " 'call': 1.3033830350270366,\n",
       " 'bricklin': 4.652778513628567,\n",
       " 'were': 1.0990566413761538,\n",
       " 'realli': 1.3659512811809624,\n",
       " 'small': 1.8748433735110683,\n",
       " 'addit': 2.128338364879106,\n",
       " 'front': 2.230475356162961,\n",
       " 'bumper': 3.529460747022825,\n",
       " 'separ': 2.3798240685565886,\n",
       " 'rest': 2.0380158654396667,\n",
       " 'bodi': 2.1262618477543906,\n",
       " 'all': 0.7306445111637885,\n",
       " 'know': 0.8876413177849213,\n",
       " 'tellm': 5.14189911389988,\n",
       " 'model': 2.0506048315224845,\n",
       " 'name': 1.59380062749238,\n",
       " 'engin': 1.9630484311713814,\n",
       " 'spec': 2.670436811485926,\n",
       " 'year': 1.1848751457435927,\n",
       " 'product': 2.01705364203593,\n",
       " 'where': 1.2728144807787336,\n",
       " 'made': 1.5182123972079447,\n",
       " 'histori': 2.1019093663832,\n",
       " 'whatev': 2.097948656930185,\n",
       " 'info': 1.9902379699776866,\n",
       " 'funki': 4.295425045193319,\n",
       " 'pleas': 1.3134358992308655,\n",
       " 'mail': 1.5648707593735556}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfed_corpus[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3\n",
    "rubric={accuracy:2,quality:1}\n",
    "\n",
    "Now you are going to built an relevance ranking search engine. Create a function `BM25IR` which takes a raw English query as input. You should again be using `simulate` to \n",
    "- convert the query to tokens compatible with your corpus. Then, \n",
    "- iterate over your the corpus (`tfidfed_corpus`) and, \n",
    "- **for each term in the query, sum the corresponding tf-idf values** from the document (if a term does not appear, its tf-idf defaults to 0) to get a relevance score for each document. \n",
    "Then, \n",
    "- rank all the documents in your corpus and **return an ordered list of the top 10 ids**, highest ranked first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BM25IR(query, analyzer, tfidfed_corpus):\n",
    "#     # your code here\n",
    "\n",
    "def BM25IR(query, analyzer, tfidfed_corpus):\n",
    "    rankings = []\n",
    "    query_terms = get_tokens(query, analyzer)\n",
    "    # iterate tfidfed_corpus .. \n",
    "\n",
    "    return # top 10 ids; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4\n",
    "rubric={accuracy:1,quality:1}\n",
    "\n",
    "Create at least 3 example queries that are relevant to topic matters in the corpus and have at least 3 interesting (not stop) words. Print out the top document for each query as determined by your system, and show that your output is *almost* the same as the results of Whoosh using your index from Exercise 1 by comparing the two using P@10 (i.e., consider documents in the top 10 ranked results of Whoosh to be the \"relevant\" documents for this calculation); if you have done the rest of this problem correctly, you should get 1 or 0.9 for nearly all queries.\n",
    "\n",
    "Be careful!  Your search engine should be using the text_analyzer you created above, but the Whoosh engine should take the result of the TextParser.  It's a fine distinction, but the Parser produces a generator, while the analyzer produces a list.\n",
    "\n",
    "FYI, the reason your results may not be exactly the same as Whoosh is because Whoosh stores only approximations for some values. That is, your implementation of Okapi BM25 is in fact more accurate! You will note that it is also quite fast (and could be made even faster with the inverted index from Exercise 2), the main efficiency issue here is that you're using quite a lot of memory with all those Python dicts. Generally, this approach would not scale well to millions of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT:  [8458, 3290, 9289, 10586, 5580, 6188, 655, 9948, 10566, 5217]\n",
      "-----------------------------\n",
      "Whoosh P@10:\n",
      "QUERY:  (text:home OR text:run OR text:extra OR text:inn)\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "def get_top10_hits(index, analyzer, query):\n",
    "    '''get a list of ids returned from a Whoosh search, in order of relevance'''\n",
    "\n",
    "    query = # how to make the query? \n",
    "    print(\"QUERY: \", TextParser.parse(query))\n",
    "    results = \n",
    "    ids = set()\n",
    "    # iterate results to add at ids; \n",
    "\n",
    "    return ids\n",
    "\n",
    "def check_query(query,analyzer,tfidfed_corpus):\n",
    "    '''print out the top document in the tfidifed_corpus given the query,\n",
    "    and calculate P@10 compared with Whoosh output'''\n",
    "    output = BM25IR(...)\n",
    "    print(\"OUTPUT: \", output)\n",
    "    \n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Whoosh P@10:\")\n",
    "    # print the ratio of (get_top10_hits10 INTERSECTION your `output`) / 10\n",
    "\n",
    "check_query(\"home runs and extra innings\", text_analyzer, tfidfed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Language models for IR (Optional)\n",
    "rubric={accuracy:1,efficiency:1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
