{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiPHNcdtrd6E"
   },
   "source": [
    "# COLX 561 Lab Assignment 2: Semantic Role Labelling\n",
    "## Assignment Objectives\n",
    "\n",
    "In this lab, you will be fine-turning BERT to predict Semantic Role Labels (SRLs). Components of this lab include:\n",
    "\n",
    "1. Building a dataset\n",
    "2. Fine tune BERT for SRLabeling\n",
    "3. Evaluating and running inference on your model\n",
    "\n",
    "If you do not have access to a GPU locally you may want to run this lab on Google CoLab with a GPU backend. Go to the 'Runtime' menu, choose 'Change Runtime Type...' and there is usually a T4 GPU available for free, for at least a few hours. Our dataset is not huge, nor is our model, so CPU training is doable (shouldn't take more than about 30 minutes), but training will be *much* faster on a GPU if you are trying different options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKuXX1pxrd6F"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "Run the code below to access relevant modules (you can add to this as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# !{sys.executable} -m pip install panda transformers accelerate tokenizers datasets \n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrbFsgi4rd6G"
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer,AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wUbaFeird6J"
   },
   "source": [
    "For this lab, we will be working with the OntoNotes v. 5.0 corpus, specifically the data tagged with Semantic Role Labels in the PropBank style using the standard format of CoNLL. Download [the data](https://github.ubc.ca/MDS-CL-2022-23/COLX_563_adv-semantics_instructors/tree/master/labs/Data/Lab2) from github, unzip it into a directory outside of your lab repo and change the path below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yw21luW4rd6O"
   },
   "source": [
    "## Tidy Submission\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this Jupyter notebook with your answers embedded\n",
    "- Be sure to follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEvu9N0Grd6P"
   },
   "source": [
    "## Initial Data Processing\n",
    "In the following cells, we'll be collecting our data and building our data set. **The code has been provided for you. You just need to run the cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fjsOhGerd6Q"
   },
   "source": [
    "First, we'll generate three lists (`train_files`, `dev_files`, and `test_files`) which consist of the paths to all SRL datafiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontonotes_path = '../Data/Lab2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrKFDcr5rd6Q"
   },
   "outputs": [],
   "source": [
    "train_files = [os.path.join('train',filename) for filename in os.listdir(os.path.join(ontonotes_path , 'train'))]\n",
    "dev_files = [os.path.join('dev', filename) for filename in os.listdir(os.path.join(ontonotes_path, 'dev'))]\n",
    "test_files = [os.path.join('test', filename) for filename in os.listdir(os.path.join(ontonotes_path,'test'))]\n",
    "\n",
    "train_files, dev_files, test_files = sorted(train_files), sorted(dev_files), sorted(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYPyglw-rd6T",
    "outputId": "7149732c-375f-4904-bf8b-0711fe9673e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check that we get the correct number of files\n",
    "assert len(train_files) == 262\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELWtVI9Srd6X"
   },
   "source": [
    "### Building our dataset\n",
    "\n",
    "Like with our NER set, we need to generate IOB (**I**nside-**O**utside-**B**eginning) SRL tags for our CoNLL formatted data. But this time, because a sentence can have multiple verbal predicates (with SRL arguments for each predicate), we will potentially create *multiple* IOB tag sequences for a single sentence. \n",
    "\n",
    "For your reference, here is an example of CoNLL formatted data. Columns 9 and 10 contain SRL arguments for the verbs *Shooting* and *turn*, respectively. Parentheses like `(ARG2* ... *)` are used to indicate spans for arguments. For example, `ARG2` for the verb *turn* contains the entire span *Into a Funeral in the Gaza Strip*. \n",
    "\n",
    "```\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    0  Celebration    NN  (TOP(S(NP*     -          -    (ARGM-LOC*)   (ARG0*\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    1     Shooting    NN           *) shoot  shoot.02           (V*)        *)\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    2        Turns   VBZ        (VP*   turn   turn.02             *       (V*)\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    3      Wedding    NN        (NP*)    -          -             *    (ARG1*)\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    4         Into    IN        (PP*     -          -             *    (ARG2*\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    5            a    DT        (NP*     -          -             *         *\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    6      Funeral    NN           *)    -          -             *         *\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    7           in    IN        (PP*     -          -             *         *\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    8     Southern    JJ        (NP*     -          -             *         *\n",
    "ontonotes/wb/a2e/00/a2e_0000   0    9         Gaza   NNP           *     -          -             *         *\n",
    "ontonotes/wb/a2e/00/a2e_0000   0   10        Strip   NNP      *))))))    -          -             *         *)\n",
    "\n",
    "```\n",
    "To distinguish between predicates, we'll add a binary indicator feature to **every token**. The value is **1** if the token is the targeted verb and **0**, otherwise. For example, the first SRL column for the sentence above would generate tokens and tags that would look like this (*shooting* is the predicate here as indicated by `('shooting', 1)`):\n",
    "```\n",
    "('celebration', 0) : B-ARGM\n",
    "('shooting', 1) : B-PRED\n",
    "('turns', 0) : O\n",
    "('wedding', 0) : O\n",
    "('into', 0) : O\n",
    "('a', 0) : O\n",
    "('funeral', 0) : O\n",
    "('in', 0) : O\n",
    "('southern', 0) : O\n",
    "('gaza', 0) : O\n",
    "('strip', 0) : O\n",
    "```\n",
    "The second SRL column would generate an output that looks like this. The predicate here is *turns*.\n",
    "```\n",
    "('celebration', 0) : B-ARG0\n",
    "('shooting', 0) : I-ARG0\n",
    "('turns', 1) : B-PRED\n",
    "('wedding', 0) : B-ARG1\n",
    "('into', 0) : B-ARG2\n",
    "('a', 0) : I-ARG2\n",
    "('funeral', 0) : I-ARG2\n",
    "('in', 0) : I-ARG2\n",
    "('southern', 0) : I-ARG2\n",
    "('gaza', 0) : I-ARG2\n",
    "('strip', 0) : I-ARG2\n",
    "```\n",
    "\n",
    "**Note:** The 'V' tag has been changed to 'PRED,' and all tags 'ARGM(-XYZ)' have been collapsed into a single 'ARGM' type.\n",
    "\n",
    "The code here reads each sentence as a pandas.DataFrame() and then generates sets of tokens and tags for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceW-fgGmrd6X"
   },
   "outputs": [],
   "source": [
    "def df2iob(df):\n",
    "    '''Generates tokens and tags from a dataframe corresponding to a sentence of the Ontonotes\n",
    "    corpus tagged for semantic roles. Returns a tuple consisting of a list of lists of \n",
    "    (token,is_target_pred) pairs, and lists of lists of IOB tags; the length of these lists will\n",
    "    correspond to number of predicates in the sentence (potentially zero)'''\n",
    "    word_tokens = df['Word'].values.tolist()\n",
    "    all_tags = []\n",
    "    all_tokens = []\n",
    "    for column in df:\n",
    "        if column.startswith(\"SRL\"):\n",
    "            sent_tags = []\n",
    "            srl = df[column].values.tolist()    \n",
    "            in_tag = 0\n",
    "            for tag in srl:\n",
    "                # Used to collapse tag classes\n",
    "                tag = tag.replace('C-', '')\n",
    "                tag = tag.replace('R-','')\n",
    "                tag = tag.replace('-DSP','')\n",
    "                if '(' in tag and ')' in tag:\n",
    "                    curr_tag = tag[1:-2]\n",
    "                    curr_tag = 'PRED' if curr_tag == 'V' else curr_tag\n",
    "                    curr_tag = 'ARGM' if 'ARGM' in curr_tag else curr_tag\n",
    "                    sent_tags.append('B-' + curr_tag)\n",
    "                elif '(' in tag:\n",
    "                    curr_tag = tag[1:-1]\n",
    "                    curr_tag = 'PRED' if curr_tag == 'V' else curr_tag\n",
    "                    curr_tag = 'ARGM' if 'ARGM' in curr_tag else curr_tag\n",
    "                    sent_tags.append('B-' + curr_tag)\n",
    "                    in_tag = 1\n",
    "                elif ')' in tag:\n",
    "                    sent_tags.append('I-' + curr_tag)\n",
    "                    in_tag = 0\n",
    "                elif in_tag:\n",
    "                    sent_tags.append('I-' + curr_tag)\n",
    "                else:\n",
    "                    sent_tags.append('O')\n",
    "            word_token_is_verb = [1 if 'PRED' in tag else 0 for tag in sent_tags]\n",
    "            # sent_token_pairs = [(w.lower(), j) for w,j in zip(word_tokens, word_token_is_verb)]\n",
    "            # all_tokens.append(sent_token_pairs)\n",
    "            sent_tokens = [w.lower() for w in word_tokens]\n",
    "            all_tags.append(sent_tags)\n",
    "            all_tokens.append(sent_tokens)\n",
    "    return all_tokens, all_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extracts sentences from a file and converts them into Pandas `DataFrame` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Blm0EwEdrd6a"
   },
   "outputs": [],
   "source": [
    "column_headers = ['Path', 'SentId', 'WordIdx', 'Word', 'POS', 'Parse', 'Verb', 'VerbSense']\n",
    "\n",
    "def get_dfs(file):\n",
    "    '''Gets all dataframes (which correspond to sentences) from a .gold_conll file'''\n",
    "    dfs = []\n",
    "    curr_sent = []\n",
    "    with open(file, encoding='utf=8') as inF:\n",
    "        for line in inF:\n",
    "            if line == '\\n' and curr_sent:\n",
    "                num_srl = len(curr_sent[0]) - len(column_headers)\n",
    "                local_column_headers = column_headers + ['SRL' + str(ii) for ii in range(num_srl)] \n",
    "                df = pd.DataFrame(curr_sent, columns=local_column_headers)\n",
    "                dfs.append(df)\n",
    "                #Reset for next sentence\n",
    "                curr_sent = []\n",
    "            else:\n",
    "                curr_sent.append(line.strip().split())\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4pFA8cyrd6d"
   },
   "source": [
    "Code to make sure things are working properly. Please have a look at `all_tokens` and `all_tags` returned by `df2iob()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzZ_Or-nrd6e",
    "outputId": "945918a2-3959-4715-8d3f-84854599c46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "check_file = os.path.join(ontonotes_path, dev_files[0])\n",
    "dfs = get_dfs(check_file)\n",
    "all_tokens, all_tags = df2iob(dfs[3])\n",
    "assert len(all_tokens) == 3\n",
    "assert len(all_tags) == 3\n",
    "# assert all_tokens[2] == [('the', 0), ('yugoslav', 0), ('election', 0), ('commission', 0), ('claims', 0), ('he', 0), ('did', 0), ('not', 0), ('win', 1), ('more', 0), ('than', 0), ('50', 0), ('%', 0), ('of', 0), ('the', 0), ('vote', 0), ('.', 0)]\n",
    "assert all_tags[2] == ['O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'B-ARGM', 'B-PRED', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O']\n",
    "# assert all_tokens[0] == [('the', 0), ('yugoslav', 0), ('election', 0), ('commission', 0), ('claims', 1), ('he', 0), ('did', 0), ('not', 0), ('win', 0), ('more', 0), ('than', 0), ('50', 0), ('%', 0), ('of', 0), ('the', 0), ('vote', 0), ('.', 0)]\n",
    "assert all_tags[0] == ['B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'B-PRED', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O']\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'yugoslav',\n",
       " 'election',\n",
       " 'commission',\n",
       " 'claims',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'win',\n",
       " 'more',\n",
       " 'than',\n",
       " '50',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vote',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ARG0',\n",
       " 'I-ARG0',\n",
       " 'I-ARG0',\n",
       " 'I-ARG0',\n",
       " 'B-PRED',\n",
       " 'B-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'I-ARG1',\n",
       " 'O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUZcP8oTrd6h"
   },
   "source": [
    "## Exercise 1: Building a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r69MKaierd6h"
   },
   "source": [
    "### Exercise 1.1\n",
    "rubric = {accuracy:1}\n",
    "\n",
    "Using the functions `get_dfs()` and `df2iob()` provided in **Initial Data Processing** above, write a `prepare_SRL_data` function that takes as input a list of .gold_conll files and creates a two python lists `input_words` and `output_tags`. Each list should contain an element for every sentense in the input files. \n",
    "\n",
    "`input_tokens` should be a list of lists of strings representing the words in the input as returned by `df2iob()`.\n",
    "\n",
    "`output_tags` should be a list of list of IOB tags, corresponding to the words in `input_words`\n",
    "\n",
    "**HINT:** Check how to use the functions `get_dfs()` and `df2iob()` in the assert cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKpyZuw0rd6h"
   },
   "outputs": [],
   "source": [
    "def prepare_SRL_data(srl_files):\n",
    "    '''create IOB-data for all the SRL-tagged Ontonotes files in srl_files. \n",
    "    \n",
    "       Input: list of file names\n",
    "       Output: (input_words, output_tags) \n",
    "       \n",
    "       input_words is a list of lists, one list for every sentence in the data. Here \n",
    "                    each sentence list consists of (token, is_target_pred) pairs. \n",
    "       output_tags  is a list of lists, one list for every sentence in the data. Here \n",
    "                    each sentence list consists of IOB tags.\n",
    "    '''\n",
    "    input_words, output_tags = [], []\n",
    "\n",
    "    # #your code here\n",
    "    \n",
    "    # 1. Iterating Over SRL Files\n",
    "    #     srl_files: A list of filenames containing SRL data.\n",
    "    #     The loop iterates through each filename (fn) in this list.\n",
    "    # 2. Joining File Path\n",
    "    #     ontonotes_path: The base directory path where the SRL files are stored.\n",
    "    #     os.path.join combines the base path (ontonotes_path) with the filename (fn) to create the full path to the SRL file (srl_file).\n",
    "    # 3. Parsing the SRL File\n",
    "    #     get_dfs(srl_file): A function that processes the SRL file and likely returns a list of DataFrames (dfs). \n",
    "    #     Each DataFrame may represent the semantic role labeling data for a sentence or a group of sentences.\n",
    "    # 4. Converting DataFrames to IOB Format\n",
    "    #     df2iob(df): A function that converts each DataFrame (df) into IOB format (Inside-Outside-Beginning tagging scheme).\n",
    "    #     annotated_sents: A list where each element is the result of converting a DataFrame to IOB format. Each element might be a tuple of (words, tags):\n",
    "    # 5. Updating Input Words and Output Tags\n",
    "    #     Loops through the list annotated_sents, where each element is (word, tags).\n",
    "    #     input_words: A list where all the tokenized words from all sentences are appended.\n",
    "    #     output_tags: A list where all the IOB tags corresponding to the words are appended.\n",
    "    #     The += operation extends the input_words and output_tags lists.    \n",
    "\n",
    "\n",
    "    return input_words, output_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5IUu3oEzrd6k",
    "outputId": "34440acf-dfa1-4427-efc2-cc2610fe5aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "input_words, output_tags = prepare_SRL_data(train_files)\n",
    "assert len(input_words) == 22437\n",
    "assert len(output_tags) == 22437\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset is huge, so you might want to run the rest of this notebook with just a smaller slice for now. Feel free to adjust the numbers in the cell below to your needs. The 'tokens' variable is also being renamed to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 50 #\n",
    "words = input_words[:limit]\n",
    "tags = output_tags[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tFWsbpcrd6o"
   },
   "source": [
    "## Exercise 1.2\n",
    "rubric={accuracy:3, quality:1}\n",
    "\n",
    "Next you need to create a dataset, consisting of inputs paired with outputs. Inputs are words, and the outputs are numbers representing semantic role labels, which the model must learn to predict. Before training, the inputs need to be tokenized. You can get a tokenizer that is appropriate for BERT through HuggingFace's `AutoTokenizer` object, by passing the name of a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "#This model was chosen because it is small. You can experiment with larger models if you like. \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the tokenizer, simply call the it and pass a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 7592, 102]], 'token_type_ids': [[0, 0, 0]], 'attention_mask': [[1, 1, 1]]}\n",
      "{'input_ids': [[101, 7592, 102], [101, 9119, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n",
      "{'input_ids': [[101, 2204, 2851, 2412, 23684, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[101, 2204, 2851, 7955, 102]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "#Experiment with the tokenizer and print out a few things before going any further, and be comfortable with the output format\n",
    "t = tokenizer(['Hello'])\n",
    "print(t)\n",
    "t2 = tokenizer(['Hello', 'Goodbye'])\n",
    "print(t2)\n",
    "t3 = tokenizer(['Good morning everbody'])\n",
    "print(t3)\n",
    "t4 = tokenizer([['Good', 'morning', 'everybody']], is_split_into_words=True)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output labels, you can use the following dictionaries to convert semantic role tags to numbers, or the other way around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tag_list = list(set([tag for sent_tags in output_tags for tag in sent_tags]))\n",
    "tag_to_id = {tag:i for (i,tag) in enumerate(full_tag_list)}\n",
    "id_to_tag = {i:tag for (i,tag) in enumerate(full_tag_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can put this all together with a Dataset object from HuggingFace for this task. To create a Dataset, you need to convert your data into a list of dictionaries, where each dictionary has two keys:\n",
    "\n",
    "- `words` is a list of strings representing input words (not yet tokenized)\n",
    "- `tags` is a list of integers representing semantic role tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an illustration of the format you're looking for, but of course you should generate it by using a for loop or list comprehension\n",
    "data_to_tokenize = [              \n",
    "{'words': ['opposition', 'leaders','claim', 'milosevic', 'rigged','last','week', \"'s\",'presidential','vote','.'],\n",
    " 'tags': [0, 0, 0, 5, 11, 1, 4, 4, 4, 4, 0]}, #first sentence\n",
    "{'words': [], \n",
    " 'tags': []}, #second sentence\n",
    "{'words': [], \n",
    " 'tags':[]} #third sentence, etc.\n",
    "]\n",
    "\n",
    "data_to_tokenize = [{'words': word, 'tags': [tag_to_id[t] for t in tag]} for (word,tag) in zip(words, tags)]\n",
    "dataset = Dataset.from_list(data_to_tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a `process_data` function that applies the tokenizer to the dataset. You will need to look up additional arguments that enable the tokenizer to (1) pad the input, (2) truncate the input, (3) allow a maximum of 32 tokens. \n",
    "\n",
    "The tokenizer may split a word into more than one token, so you will also need to write some code to 're-align' the output and map each of the sub-word tokens to the same output label. There are some hints in the code below. \n",
    "\n",
    "You can find more information in the Tokenizer documentation [here](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__), and you can also consult the BERT notebook from Lecture 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    tokenized_data = tokenizer(data['words'], is_split_into_words=True, padding=True, truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    all_labels = []\n",
    "    #your code here for aligning labels goes below\n",
    "    #HINTS:\n",
    "    #for each semantic role tag, you need to find all of the tokens that correspond to the input word which carried that tag\n",
    "    #you can find the tokens for tag N like this:\n",
    "    # word_ids = tokenized_data.word_ids(batch_index=N)\n",
    "    #you can pass word_ids to the `align_labels` function below to get back a list of labels for a given token, and append that to all_labels\n",
    "    for i, label in enumerate(data[\"tags\"]):\n",
    "        word_ids = tokenized_data.word_ids(batch_index=i)\n",
    "        aligned_labels = align_labels(label, word_ids)\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_data[\"labels\"] = all_labels\n",
    "    return tokenized_data\n",
    "\n",
    "def align_labels(labels, word_ids):\n",
    "    new_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            #This means there's nothing to label, e.g. it's a pad token\n",
    "            #(-100) is a special value that the model will ignore\n",
    "            new_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            new_labels.append(labels[word_idx])\n",
    "        else:\n",
    "            new_labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a dataset with your function, run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ddcf0206914473acff1b50f02c84f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(process_data, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['words', 'tags', 'token_type_ids']) #These are unnecessary for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  8936,\n",
       "  2078,\n",
       "  2343,\n",
       "  22889,\n",
       "  16429,\n",
       "  13390,\n",
       "  2078,\n",
       "  26038,\n",
       "  17726,\n",
       "  2003,\n",
       "  5432,\n",
       "  1997,\n",
       "  1996,\n",
       "  8465,\n",
       "  1997,\n",
       "  2019,\n",
       "  4559,\n",
       "  3377,\n",
       "  1999,\n",
       "  2019,\n",
       "  9046,\n",
       "  19550,\n",
       "  2602,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the output\n",
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tokenized_dataset)\n",
    "assert len(tokenized_dataset['input_ids']) == len(tokenized_dataset['labels']) == len(tokenized_dataset['attention_mask'])\n",
    "assert all(isinstance(label, int) for label in tokenized_dataset['labels'][0]), 'Labels were not converted to integers'\n",
    "assert max(len(tokenized_dataset[n]['input_ids']) <=32 for n in range(N)), 'Truncation failed'\n",
    "assert any((-100) in tokenized_dataset[n]['labels'] for n in range(N)), 'Label alignment failed'\n",
    "assert any(0 in tokenized_dataset[n]['input_ids'] for n in range(N)), 'Padding failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, split the data into train, test, and validate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = tokenized_dataset.train_test_split(test_size=0.1) # keep 10% of the data as test set\n",
    "test_valid = train_test[\"test\"].train_test_split(test_size=0.05) #keep 5% of the training data as validation \n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_test[\"train\"], #training data\n",
    "    \"eval\": test_valid[\"train\"], #for evaluation/validation during the training\n",
    "    \"test\": test_valid[\"test\"] #for testing after training is complete\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Training BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFC1udwIrd6x"
   },
   "source": [
    "# Exercise 2.1\n",
    "rubric = {accuracy:1}\n",
    "\n",
    "The next step is to set up BERT for training. Some code is already provided for you below, but must make the following changes:\n",
    "\n",
    "- Modify the TrainingArguments object so the model trains for 5 epochs\n",
    "- Create a Trainer object, by passing it (1) a model, (2) a tokenizer, (3) a train set, (4) an eval set, (5) training arguments. \n",
    "\n",
    "When you run the next cell, you will probably get a warning BERT needs to be trained. You can ignore it, you will be doing the training shortly.\n",
    "\n",
    "If you get an error running the next cell because of a failure to load the 'accelerate' library, follow the instructions to `pip install` the correct library, then restart your notebook kernal for this to take effect. \n",
    "\n",
    "You can find more details about the Trainer object in the [transformers documentation here](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/trainer#transformers.Trainer), and you can also consult the notebook from Lecture 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TySaTC7urd6y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_srl_tags = len(full_tag_list) #set this equal to the actual number of output labels in your dataset\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_srl_tags)\n",
    "\n",
    "#generic method to give average label accuracy during training, no changes required\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "#     # accuracy = (predictions == labels).float().mean()\n",
    "#     accuracy = (predictions == labels).type(torch.float32).mean()\n",
    "\n",
    "#     return {\"accuracy\": accuracy.item()}\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors (if not already)\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # Get predictions and ensure they are tensors\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == labels).float().mean()\n",
    "    return {\"accuracy\": accuracy.item()}\n",
    "\n",
    "\n",
    "#update this so that the model trains for 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", #you can change this to any directory you like\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    # no_cuda=True,  # Disable GPU usage, ensuring that no `accelerate` optimizations are applied\n",
    ")\n",
    "\n",
    "#fill in the appropriate arguments here\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset['train'],\n",
    "    eval_dataset=final_dataset['eval'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4E_e2hVrd61"
   },
   "outputs": [],
   "source": [
    "assert len(trainer.train_dataset) == len(final_dataset['train'])\n",
    "assert len(trainer.eval_dataset) == len(final_dataset['eval'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step actually trains the model. Depending on the size of the dataset you are using and the number of epochs you chose above, this can anywhere from minutes to hours. If you want to play more with the model parameters, a GPU is recommended. For the purpose of this lab, you can train the model on a few hundred examples for 5 epochs. The results will not be fantastic, but you can complete the lab on an ordinary laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2074c7bf4ca04a918cc5b633a6aaf3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c314daf714ff46c6a859d932d8f7afd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.607465982437134, 'eval_accuracy': 0.109375, 'eval_runtime': 0.0079, 'eval_samples_per_second': 505.566, 'eval_steps_per_second': 126.392, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13977ada3914cbaa25048467a39a2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.576277732849121, 'eval_accuracy': 0.15625, 'eval_runtime': 0.0069, 'eval_samples_per_second': 577.529, 'eval_steps_per_second': 144.382, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a043ee26f9e48f0aa0f73981eadafa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5534257888793945, 'eval_accuracy': 0.203125, 'eval_runtime': 0.0085, 'eval_samples_per_second': 473.251, 'eval_steps_per_second': 118.313, 'epoch': 3.0}\n",
      "{'loss': 2.5855, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87490e59771d4648872fc789041eec80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5392467975616455, 'eval_accuracy': 0.2265625, 'eval_runtime': 0.0071, 'eval_samples_per_second': 561.863, 'eval_steps_per_second': 140.466, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991a2212f7c14b75b54f79cd5f1a4a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.533625364303589, 'eval_accuracy': 0.234375, 'eval_runtime': 0.0074, 'eval_samples_per_second': 539.669, 'eval_steps_per_second': 134.917, 'epoch': 5.0}\n",
      "{'train_runtime': 0.5994, 'train_samples_per_second': 375.384, 'train_steps_per_second': 25.026, 'train_loss': 2.5679574330647785, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert history.metrics['epoch'] >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mW3JtXPMrd7G"
   },
   "source": [
    "### Exercise 3: Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuONBeTyrd7G"
   },
   "source": [
    "## Exercise 3.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Now you will test the model in two different ways. \n",
    "\n",
    "First, run a full evaluation over the test set created earlier in the notebook. To do this, look up how to use the Trainer object's `.evaluate()` method in the transformers documentation. Save the results in a variable called `test_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2ebd0dab1045e8b3f4f4c43cab0c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(final_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5009238719940186, 'eval_accuracy': 0.25, 'eval_runtime': 0.017, 'eval_samples_per_second': 58.84, 'eval_steps_per_second': 58.84, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_metrics['epoch'] == history.metrics['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well did the model do on the test set, compared to the training set? Write a simple if-else block that compares the training and testing loss values, and prints out which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is better\n"
     ]
    }
   ],
   "source": [
    "if history.training_loss > test_metrics['eval_loss']:\n",
    "    print('training is better')\n",
    "else:\n",
    "    print('testing is better')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqGYf_LSrd7N"
   },
   "source": [
    "### Exercise 3.2\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Lastly, you will run 'inference' on the model, by giving it specific inputs and checking its predictions. To get 1 point on this exercise, write a function that takes a list of strings as input, tokenizes them, and then returns the predicted labels.  To get 2 points on this exercise, the function should also re-align the predictions with full words from the input. Some of this code is already supplied for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZMrqUpp6rd7N",
    "outputId": "0ce79450-5d93-444b-8d7c-0e76d5b819cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'O'), ('like', 'O'), ('swimming', 'B-ARG3'), ('in', 'O'), ('the', 'O'), ('lake', 'B-ARG4'), ('sometimes', 'I-ARG2')]\n"
     ]
    }
   ],
   "source": [
    "def predict_labels(sentence):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize the input sentence, update this appropriately. You can re-use code from the tokenization in Exercise 1.2\n",
    "    tokenized_sentence = tokenizer(sentence, is_split_into_words=True, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "    # Make predictions, no changes needed\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_sentence)\n",
    "        logits = outputs.logits \n",
    "    predicted_label_ids = logits.argmax(dim=-1).squeeze() # Shape: (sequence_length,)\n",
    "    predicted_label_ids = [int(id) for id in predicted_label_ids]\n",
    "\n",
    "    #your code for formatting the return value goes here\n",
    "    #HINTS:\n",
    "    #Use the align_labels() function from earlier as a guide\n",
    "    #You will once again need to loop over token IDs using tokenized_sentence.word_ids(batch_index=0)\n",
    "    #the batch_index will always be 0 this time, because there's only 1 input sentence to this function\n",
    "\n",
    "    word_ids = tokenized_sentence.word_ids(batch_index=0)\n",
    "    predictions = []\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is not None: \n",
    "            label_id = predicted_label_ids[idx]\n",
    "            label = id_to_tag[label_id]\n",
    "            predictions.append((sentence[word_id], label))\n",
    "    return predictions\n",
    "\n",
    "sentence = [\"I\", \"like\", \"swimming\", \"in\", \"the\", \"lake\", \"sometimes\"]\n",
    "print(predict_labels(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_labels([\"I\", \"like\", \"swimming\"])\n",
    "assert not all(isinstance(id, int) for id in prediction), 'Failed to convert back to label names'\n",
    "assert isinstance(prediction[0], tuple)\n",
    "assert prediction[0][1][0] in ['B', 'I', 'O']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
