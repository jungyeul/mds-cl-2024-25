{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSCI 572 Lab 4: RNNs for sentiment analysis\n",
    "\n",
    "### Assignment Topics\n",
    "- Train a deep learning system for real-world task\n",
    "- Recurrent Neural Networks\n",
    "- RNN, LSTM, GRU \n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.7.1) \n",
    "- Jupyter (latest)  \n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: Sunday, February 5, 2023, 23:59:00 \n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Use random seeds appropriately\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Kaggle competition on sentiment analysis\n",
    "\n",
    "(This assignment is based **heavily** on the work of the 2022 DSIC572 teaching team. Thank you Chiyu, Peter and Muhammad!)\n",
    "\n",
    "In this exercise, you'll be compeating for the best performance in sentiment analysis of Yelp reviews. Remember, this is just a game. Don't put too much pressure on yourself, but do try to come up with a fully functional system for sentiment analysis using the approaches which we've explored in the course. The top-2 submissions will be awarded extra points. \n",
    "\n",
    "### Yelp sentiment analysis\n",
    "\n",
    "Millions of people share a great number of reviews about businesses on [Yelp.com](https://www.yelp.com/) and Yelp mobile app everyday. We used [Yelp APIs (application programming interface)](https://www.yelp.ca/developers) to collect over 35,000 reviews of 1,000 restaurants in New York City. We split this dataset into 90\\% TRAIN set (28,000 reviews), 10\\% DEV set (3,500 reviews), and 10\\% TEST set (3,500 reviews). \n",
    "\n",
    "The data-points in each dataset contain both an input review and a corresponding label ranging from 1 star (the worst reviews) to 5 stars (the best reviews). This table shows the class distribution of TRAIN and DEV sets.\n",
    "\n",
    "|    Rating  |   # of Train   reviews| # of Dev reviews    |  \n",
    "| ---------- | -----------------  |-----|  \n",
    "| 1star      | 5,619              | 683 |  \n",
    "| 2star      | 5,616              | 677 |  \n",
    "| 3star      | 5,583              | 713 |  \n",
    "| 4star      | 5,532              | 733 |  \n",
    "| 5star      | 5,650              | 694 |  \n",
    "\n",
    "\n",
    "In directory `data` (which you can find in your DSCI572 student repo `labs/Lab4/data`), we provide the `TRAIN` (`train.tsv`) and `DEV` (`val.tsv`) sets with the corresponding labels for your system development. \n",
    "Please use the TRAIN and DEV sets to develop a classification system for this task. You can use any model we've covered in the course (e.g., linear regression, feed-forward neural network, RNN, GRUs, LSTM) or some machine learning model which we didn't cover. However, your solution has to contain a non-trivial machine learning component. \n",
    "\n",
    "We also provide the inputs for the `TEST` split. You will use your best trained model to predict the labels of **`TEST` reviews** and submit your predictions to a Kaggle competition. \n",
    "\n",
    "**The performance of your submitted systems will be evaluated on predictions of rating labels for reviews in TEST set. Macro Averaged F-score will be the evaluation metric.** \n",
    "\n",
    "Your mark is based on the quality of your work:\n",
    "\n",
    "* Does your code run? \n",
    "* Is the code neat, commented and easy to follow?\n",
    "* Is the machine learning strategy appropriate for this task?\n",
    "* Are the hyper-parameters tuned? Are you exploring the space of hyper-parameters adequately?\n",
    "\n",
    "Moreover, particularly interesting and thorough work including thorough analysis of the results will be rewarded. As mentioned above, we will also award a few extra points to the top-2 systems submitted to the Kaggle competition.\n",
    "\n",
    "Gentle reminder of the rubrics that will be relevant:  \n",
    "\n",
    "[Accuracy](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_accuracy.md)    \n",
    "[Quality](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_quality.md)   \n",
    "[Reasoning](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_reasoning.md)  \n",
    "[Mechanics](https://github.com/UBC-MDS/public/blob/master/rubric/rubric_mech.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Yelp review rating prediction \n",
    "rubric={accuracy:10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data and preprocessing**\n",
    "\n",
    "The Yelp reviews are untokenized. You should first select a tokenizer for your system (e.g., SpaCy English model, whitespace tokenizer, NLTK word tokenizer, or other). The choice of tokenizer can influence your final performance, so it can be a good idea to try a few different ones and select the best system based on the macro F1 score on the development set. \n",
    "\n",
    "You should use pandas to read the train, development and test data.\n",
    "\n",
    "If you use pytorch for this assignment, you should also numericalize your data and organize it into mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your data code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model selection and hyper-parameter tuning. You need to select the architecture you want to use. You may need to search the optimal hyper-parameter set to improve your model performance.**\n",
    "\n",
    "You might want to explore a few different architectures for the sentiment analysis task. One suggestion is to use some sort of RNN sequence classifier, but many other architectures will also be possible and can give you nice performance. If you decide to implement a neural model, Miikka recommends that you establish a baseline first using a traditional feature-based model like logistic regression. \n",
    "\n",
    "There are many ways to improve model performance: \n",
    "\n",
    "* You might want to use pretrained embeddings for this task (e.g., [google news word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), [ELMo](https://allennlp.org/elmo)) instead of randomly initialized embeddings \n",
    "* You might explore various feature extraction approaches like CountVectorizer and TfidfVectorizer \n",
    "* You can adjust your vocabulary size (by dropping very frequent or infrequent items)\n",
    "* If you're using a neural model, you can adjust the number of layers and the size of hidden layers\n",
    "* You can try out different flavors of neural models (RNN, LSTM, GRU)\n",
    "* You can apply regularization, e.g. dropout\n",
    "* Changing training procedure, such as number of epochs, learning rate (with or without scheduling), adding regularization and momentum (or Adam).\n",
    "* You may find some novel ideas in the state-of-the-art NLP systems [here](http://nlpprogress.com/english/sentiment_analysis.html).\n",
    "\n",
    "You can evaluate the macro averaged F1 score (as compute by sklearn functions) to monitor the impact of various design choices and hyperparameters.\n",
    "\n",
    "Hint: Due to the potentially intensive computational resource requirements, we suggest that you run your experiments on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true). However, this is not required. \n",
    "\n",
    "Please read `Colab instructions` for more information on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. When you get your best model on DEV set, you will use this model to predict the labels of `TEST set` and submit your prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your prediction code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. For submission of predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline validation\n",
    "\n",
    "1. In directory `./data/yelp_review/`,  `EXAMPLE_GOLD.txt` and `EXAMPLE_PRED.txt` are examples of gold and prediction files which can be used with the ``Scorer.py`` provided (description below). Your submission should have excactly the same structure as **`EXAMPLE_PRED.txt`** (i.e., each line contains one predication label without header of the column.) This is important.\n",
    "\n",
    "2. `./data/yelp_review/Scorer.py`\n",
    "\n",
    "The scoring script (`Scorer.py`) is provided at the root directory of the released data. `Scorer.py` is a python script that will take in two text files containing true labels and predicted labels and will output accuracy, F1 score, precision and recall. (Note that the evaluation metric is F1 score).  The scoring script is used for evaluating your TEST prediction. This is an offline scorer to see how you do on the Dev set. Once you are satisfied with your performance you can upload your submission to the Kaggle.\n",
    "\n",
    "Please make sure to have `sklearn library` installed before running `Scorer.py`.\n",
    "```\n",
    "Usage of the scorer:\n",
    "\n",
    "    python3 Scorer.py  <gold-file> <pred-file>\n",
    "\n",
    "In the dataset directory, there are example\n",
    "gold and prediction files. If they are used with the scorer,\n",
    "they should produce the following results:\n",
    "\n",
    "python3 Scorer.py EXAMPLE_GOLD.txt EXAMPLE_PRED.txt\n",
    "\n",
    "OVERALL SCORES:\n",
    "MACRO AVERAGE PRECISION SCORE: 20.97 %\n",
    "MACRO AVERAGE RECALL SCORE: 20.97 %\n",
    "MACRO AVERAGE F1 SCORE: 20.97 %\n",
    "OVERALL ACCURACY: 20.97 %\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "1. Your submission must have the **same** structure as `Tom_Smith_PRED.txt` (the example prediction file in `blank_labs/Lab4/` in the student repo). \n",
    "\n",
    "\n",
    "2. The predication label must be in the **original label format** (`i.e., '1star', '2star', '3star', '4star', or '5star'`).\n",
    "\n",
    "We provide a funtion `out_prediction()` to generate the predictions in the correct format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_prediction(first_name, last_name, prediction_list):\n",
    "    \"\"\"\n",
    "    out_prediction takes three input varibles: first_name, last_name, prediction_list\n",
    "    <first_name>, string, your first name, e.g., Tom\n",
    "    <last_name>, string, your last name, e.g., Smith\n",
    "    <prediction_list>, list of string which includes all your predications of TEST samples\n",
    "                        e.g., ['1star','5star','3star']\n",
    "                        \n",
    "    Generate a file is named with <yourfirstname>_<yourlastname>_PRED.txt in current directory\n",
    "    \"\"\"\n",
    "    output_file = open(\"{}_{}_PRED.txt\".format(first_name,last_name),'w')\n",
    "    for item in prediction_list:\n",
    "        output_file.write(item+\"\\n\")\n",
    "    output_file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of using `out_prediction` funtion. You can find a file `Tom_Smith_PRED.txt` in your diretory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prediction(\"Tom\", \"Smith\", ['1star','5star','3star'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed instructions on uploading your predictions to Kaggle along with the link to the Kaggle competition will be given in the lab session on Thursday."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS-CL-2023-24 DSCI_572 Lab Assignment 4: https://www.kaggle.com/competitions/mds-cl-2023-24-dsci572-lab-assignment-4 \n",
    "\n",
    "Your prediction file for Kaggle: \n",
    "```\n",
    "ID,Predicted\n",
    "0,\"5\"\n",
    "1,\"5\"\n",
    "2,\"1\"\n",
    "...\n",
    "```\n",
    "\n",
    "DO NOT USE the \"5star\" prediction, just \"5\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) will allow you to train your model on a GPU. \n",
    "\n",
    "You can follow the steps to use Colab:\n",
    "\n",
    "1. We provide a new notebook `(lab4_colab.ipynb)` for your experiments on Colab. You should develop your system on `lab4_colab.ipynb` instead of current jupyter notebook. \n",
    "2. Go to [Google colab](https://colab.research.google.com).\n",
    "3. Create an account or login your account.\n",
    "4. Select \"UPLOAD\" and upload `lab4_colab.ipynb`, again please don't upload current notebook (Lab4.ipynb).\n",
    "5. Set the hardware: \n",
    "**Go to the navigation bar, click Runtime --> Change runtime type --> Hardware accelerator --> Select GPU.**\n",
    "6. You don't need to install any packages. Google prepared everything for you.\n",
    "7. You can find all your generations in `Files`. You can download your notebook and files.\n",
    "\n",
    "Suggestion: \n",
    "1. You can download the notebook from Colab and overwrite your local version of **`lab4_colab.ipynb`**. \n",
    "2. If you train your model on GPU, please make sure your model, input and loss is sent to GPU using XXX.to(device) where device is `cuda`. \n",
    "3. If you want to send the GPU varibles to CPU, please use XXX.cpu() to detach from GPU. You can find more related information [here](https://pytorch.org/docs/stable/notes/cuda.html). \n",
    "4. Google colab keeps disconnecting automatically after 30 mins without activity. You can find some solutions [here](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting). \n",
    "\n",
    "``Warning``: Running on Colab CPU will be slow (easily slower than running on your laptop). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spark Bonus (Optional)\n",
    "rubric={spark:5} \n",
    "\n",
    "We'll award 5 extra points to the top-2 systems and/or for particularly good work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Please clearly describe the system you submitted in 1.1 (i.e., your best model) in a maximum of 400 words.\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Hints: \n",
    "1. Describe your overall approach to sentiment analysis. You should also mention strategies which you attempted but which did not eventually end up being successful.\n",
    "1. Describe all the hyper-parameters of your submitted system and describe the hyper-parmeter tuning process.\n",
    "1. Format things to make them easy to understand.\n",
    "1. Provide some error analysis. Do some of the classes (1-5 stars) attain better performance than others. Why do you think that might be?\n",
    "1. Mention a few ideas which might help to improve your current system"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Please organize your code in `Lab4.ipynb` only keeping the code that you used to train your submitted system in 1.1. Submit `Lab4.ipynb` with the repo.\n",
    "rubric={quality:5,mechanics:5}\n",
    "\n",
    "Make sure that your code is commented and neatly organized. Also, make sure that you use descriptive variable names and split your code into functions where appropriate. \n",
    "\n",
    "Please submit your code in `Lab4.ipynb` even if you used Google Colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
