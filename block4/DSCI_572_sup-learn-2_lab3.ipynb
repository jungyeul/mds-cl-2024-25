{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5564ad55",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 3: CBOW model, minibatch training and (optionally) pretrained embeddings\n",
    "\n",
    "In this lab, we'll work on a familiar task, namely, sentiment analysis. We'll build a CBOW model using pytorch. We'll also incorporate pretrained embeddings which turn out to have a substantial impact on model performance. Finally, we'll investigate the impact of minibatch training and dropout on model accuracy.\n",
    "\n",
    "**Note!** This can be a good opportunity to rehearse running code on Google Colab, where you get access to a GPU. Check this [tutorial](https://www.marktechpost.com/2021/01/09/getting-started-with-pytorch-in-google-collab-with-free-gpu/). \n",
    "\n",
    "## Getting started\n",
    "\n",
    "Run the following code:\n",
    "\n",
    "## IMBD data:\n",
    "https://github.com/jungyeul/mds-cl-2023-24/blob/main/block4/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4962cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "\n",
    "# We'll use double values in our tensors\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Checks if GPU is available, otherwise use CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda02f96",
   "metadata": {},
   "source": [
    "We'll now read data for sentiment analysis. This code is given to you. \n",
    "\n",
    "We get 500 training examples, 1000 development examples and 8476 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc2ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 500\n",
      "Number of development examples: 1000\n",
      "Number of test examples: 8476\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/IMDB.train.tsv\", header=None, names=[\"text\", \"sentiment\"], sep=\"\\t\")[:500]\n",
    "dev = pd.read_csv(\"data/IMDB.dev.tsv\", header=None, names=[\"text\", \"sentiment\"], sep=\"\\t\")\n",
    "test = pd.read_csv(\"data/IMDB.test.tsv\", header=None, names=[\"text\", \"sentiment\"], sep=\"\\t\")\n",
    "\n",
    "print(f\"Number of training examples: {len(train)}\")\n",
    "print(f\"Number of development examples: {len(dev)}\")\n",
    "print(f\"Number of test examples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f57f5",
   "metadata": {},
   "source": [
    "We'll then encode sentiment labels (`positive` and `negative`) as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44979eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train.sentiment)\n",
    "\n",
    "train_y = label_encoder.transform(train.sentiment)\n",
    "dev_y = label_encoder.transform(dev.sentiment)\n",
    "test_y = label_encoder.transform(test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866de62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      positive\n",
       "1      positive\n",
       "2      negative\n",
       "3      positive\n",
       "4      positive\n",
       "         ...   \n",
       "495    negative\n",
       "496    negative\n",
       "497    negative\n",
       "498    positive\n",
       "499    negative\n",
       "Name: sentiment, Length: 500, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299429de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
      " 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
      " 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
      " 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0\n",
      " 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
      " 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04e626",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "We'll start by training baseline sklearn sentiment analysis systems using [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for feature extraction. \n",
    "\n",
    "### Assignment 1.1\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Start by fitting a `CountVectorizer` and `TfidfVectorizer` using the training set `train`. For now, you don't need to worry about setting any of the parameters for either vectorizer.\n",
    "\n",
    "You can then transform our datasets into two sets of matrices:\n",
    "\n",
    "* `train_count_X`, `dev_count_X` and `test_count_X` (using `CountVectorizer`)\n",
    "* `train_tfidf_X`, `dev_tfidf_X` and `test_tfidf_X` (using `TfidfVectorizer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bda5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "train_count_X, dev_count_X, test_count_X = ...\n",
    "train_tfidf_X, dev_tfidf_X, test_tfidf_X = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# This is useful for discrete probabilistic models that model binary events \n",
    "###########################################################################\n",
    "count_vectorizer = CountVectorizer(stop_words=en_stopwords, binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafd3da",
   "metadata": {},
   "source": [
    "## Assignment 1.2\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "You should now fit two [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) models:\n",
    "\n",
    "* `lr_count` using your count vectorizer features\n",
    "* `lr_tfidf` using your tfidf vectorizer features\n",
    "\n",
    "Evaluate your models on the **development** data using the sklearn function [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). `lr_count` should get f-score > 75% and `lr_tfidf` > 55%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147ce033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score (macro) for Count: 0.7814496603940944\n",
      "F-score (micro) for Count: 0.795\n",
      "F-score (macro) for TF-IDF: 0.575930278181811\n",
      "F-score (micro) for TF-IDF: 0.679\n"
     ]
    }
   ],
   "source": [
    "lr_count = LogisticRegression()\n",
    "# fit, predict, results \n",
    "lr_tfidf = LogisticRegression()\n",
    "# fit, predict, results \n",
    "\n",
    "\n",
    "# 'micro': f1 for parsing (lab2)\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "# 'macro': f1 for \"each\" label; \n",
    "# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66420c3",
   "metadata": {},
   "source": [
    "Why do you think CountVectorizer would achieve better performance than TfidfVectorizer on this task?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a95197",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43343ee1",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "We'll then convert our training data into pytorch tensors. We *will not* use the output of sklearn vectorizers for this assignment. Instead we will directly numericalize our `train`, `dev` and `test` datasets. \n",
    "\n",
    "### Assignment 2.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Start by creating a [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) `vocabulary` which gives the count for each word type in the `train` dataset.\n",
    "\n",
    "To tokenize the sentences in `train`, you can simply split at spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231b2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "vocabulary = Counter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fc396",
   "metadata": {},
   "source": [
    "Assertions to test your code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ecc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "assert vocabulary[\"the\"] == 6416\n",
    "assert vocabulary[\"dog\"] == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15f6fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12775"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a9c8a",
   "metadata": {},
   "source": [
    "Next, create a mapping `word2id` which translates every word type in `vocabulary` into a unique id number in the range `1 ... len(vocabulary)`. `word2id` should also map the symbol `PAD=\"<pad>\"` to the ID `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160d36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use this constant whenever you refer to the padding symbol\n",
    "PAD=\"<pad>\"\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e32d736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf1854",
   "metadata": {},
   "source": [
    "Assertions as a partial check of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848d2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "assert word2id[PAD] == 0\n",
    "assert len(word2id) == len(vocabulary) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa492a77",
   "metadata": {},
   "source": [
    "### Assignment 2.2\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Write a function `numericalize_ex` which takes the following arguments:\n",
    "\n",
    "1. `ex`, **a string representing a review**. E.g. `\"great movie !\"`\n",
    "1. `vocabulary`, the word type counter which we created above\n",
    "1. `word2id`, the mapping words -> ID numbers which we created above\n",
    "1. `min_count`, the minimum count of word type. Rarer words are filtered out.\n",
    "1. `max_count`, the maximum count of word type. More frequent words are filtered out.\n",
    "\n",
    "Your function should first split `ex` into individual tokens (you can split at spaces). You should then filter out all words whose frequency is < `min_count` or > `max_count`. \n",
    "\n",
    "Then, transform the example into a set and transform all the remaining words into ID numbers using `word2id`. Return a `torch.tensor` of shape `n`, where `n` is the count of ID numbers.\n",
    "\n",
    "When you initialize the tensor, use `dtype=torch.long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b864a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "max_count = 100\n",
    "\n",
    "def numericalize_ex(ex, vocabulary, word2id, min_count, max_count):\n",
    "# your code here\n",
    "# using ex, make a tensor (>= min_count and <= max_count) for each \"ex\" (a SINGLE review); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79a6f499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "assert numericalize_ex(train.text[0], vocabulary, word2id, 5, 100).size()[0] == 70\n",
    "numericalize_ex(train.text[0], vocabulary, word2id, 5, 100).size() # it contains 70 words... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c34742",
   "metadata": {},
   "source": [
    "### Assignment 2.3\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Write a function `numericalize()` which takes the following arguments:\n",
    "\n",
    "1. `data`, one of our datasets `train`, `dev` or `test`\n",
    "1. `data_y`, the list of numeric labels for the examples in `data` (0 or 1 corresponding to positive and negative sentiment, respectively)\n",
    "1. `vocabulary`, the word type counter which we created above\n",
    "1. `word2id`, the mapping words -> ID numbers which we created above\n",
    "1. `min_count`, the minimum count of word type in `train`. All rarer words are filtered out.\n",
    "1. `batch_size`, our minibatch size.\n",
    "\n",
    "You should first convert all examples in `data` into tensors using `numericalize_ex`. \n",
    "\n",
    "Then, pack the examples and their labels into minibatches containing `batch_size` examples each. Every minibatch should be a 3-tuple containing:\n",
    "\n",
    "1. A minibatch `b` of input examples of dimension `batch_size x k`, where `k` is the maximal length of an example vector in the minibatch: `mbatch = pad_sequence(sequences=mbatch, batch_first=True)` makes a tensor; \n",
    "1. A minibatch of sequence lengths of shape `batch_size`, where the elements are the lengths of the examples in `b` before padding is applied: lengths (of `ex` in `mbatch`) should be a tensor; \n",
    "1. A minibatch of labels of shape `batch_size`, where each label `i` corresponds to example `b[i]`: `mbatch_y` (labels of `mbatch`) should also be a tensor;\n",
    "\n",
    "\n",
    "You will need to pad all examples in `b` to the same length using the padding symbol `word2id[PAD]`. Use the pytorch function [`pad_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html?highlight=pad_sequence) to convert a list of examples `x` of shapes `len_x` into a padded minibatch of length `batch_size x max_len_x`. You will need to call the function with the argument `batch_first=True` because we want the batch size to be the first dimension.\n",
    "\n",
    "If `batch_size` does not evenly divide `len(data)`, you may need to create one smaller minibatch to account for all training examples. This is okay.\n",
    "\n",
    "**Note:** We're returning a list from this function. It is, however, often better to create a [data loader](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). This can save memory when we're dealing with very large training sets. You'll learn more about this later.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5160d6a1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30b25580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "def numericalize(data, \n",
    "                 data_y,\n",
    "                 vocabulary, \n",
    "                 word2id,\n",
    "                 min_count, \n",
    "                 max_count, \n",
    "                 batch_size):\n",
    "# your code here\n",
    "    \n",
    "    # !!!change your data into list of tensor (note: your ex is a tensor); !!!\n",
    "    \n",
    "    # a result list of tuples\n",
    "    res = []             \n",
    "    \n",
    "    for i in range(0, len(data), batch_size):           # get data with `batch_size`\n",
    "        # you may want to get your mbatch using the current start and the end positions\n",
    "        #   mbatch = data[start:end]\n",
    "        # mbatch should be padded!  `batch_first=True`\n",
    "        # get the length, and make it a tensor\n",
    "        # get the data_y, and make it a tensor\n",
    "    \n",
    "        res.append((mbatch, length, mbatch_y))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9556c",
   "metadata": {},
   "source": [
    "Some tests to check that the number of batches which you generate looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fa803f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT of the first batch:\n",
      "tensor([[ 132,   28,  116,  ...,    0,    0,    0],\n",
      "        [ 205,  285,  292,  ...,    0,    0,    0],\n",
      "        [ 132,  205,  351,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [1235, 1171,  292,  ...,    0,    0,    0],\n",
      "        [1290, 1117, 1318,  ...,    0,    0,    0],\n",
      "        [1399, 1409, 1390,  ...,    0,    0,    0]]) 131\n",
      "tensor([ 70, 100,  40,  68, 131,  14, 111,  62,  37,  35,  33,  77, 116,  75,\n",
      "         40])\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "batches = numericalize(train, train_y, vocabulary, word2id, 5, 100, 15)\n",
    "assert 500//15 + 1 == len(batches)\n",
    "assert batches[-1][0].size()[0] == 500 % 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad53fe",
   "metadata": {},
   "source": [
    "Let's then numericalize the training, development and test data using `min_count` 5, `max_count` 100 and `batch_size` 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "939364bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_train = numericalize(train, train_y, vocabulary, word2id, 5, 100, 10)\n",
    "torch_dev = numericalize(dev, dev_y, vocabulary, word2id, 5, 100, 10)\n",
    "torch_test = numericalize(test, test_y, vocabulary, word2id, 5, 100, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ac9cabf",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "\n",
    "We'll now build a CBOW model for sentiment classification. \n",
    "\n",
    "\n",
    "**!!!!! Using the context (surrounding words), predict a middle word**\n",
    "\n",
    "### Assignment 3.1\n",
    "rubric={accuracy:5}\n",
    "\n",
    "We'll now write a baseline torch model `CBOW` for classification of CBOW inputs. This model does not yet implement dropout or pretrained embeddings.\n",
    "\n",
    "#### The `__init__` function\n",
    "\n",
    "Your `__init__` function should take the following parameters:\n",
    "\n",
    "1. `num_words`, the number of unique word type features + 1 for the symbol `PAD` (i.e. `len(word2id)`) \n",
    "1. `num_classes`, the number of output classes . In ocur case, this will always be 2 because we have exactly two classes: positive and negative.\n",
    "1. `dropout_prob`, the dropout probability\n",
    "\n",
    "Your model should contain the following layers in order:\n",
    "\n",
    "1. `self.embedding`, an embedding of dimension `EMB_SIZE` which can embed all word types recognized by `word2id`: (`nn.Embedding`)\n",
    "1. `self.linear1`, a linear layer which maps `EMB_SIZE`-dimensional inputs to `HIDDEN_SIZE`-dimensional outputs: (`nn.Linear`)\n",
    "1. `self.dropout`, dropout with probability defined `dropout_prob`: (`nn.Dropout`)\n",
    "1. `self.relu` which applies relu to the output of `self.linear1`: (`nn.ReLU`)\n",
    "1. `self.linear2` which maps `HIDDEN_SIZE`-dimensional inputs to `num_classes`-dimensional outputs\n",
    "1. `self.log_softmax` which applies log-softmax to the output of `self.linear2`: (`nn.LogSoftmax`)\n",
    "\n",
    "**Note**, when you initalize `self.embedding`, make sure to define `word2id[PAD]` as the padding symbol as explained [in the documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). The effect is that `PAD` will always be embedded as the zero vector.\n",
    "\n",
    "#### The `forward` function\n",
    "\n",
    "Your forward function takes two arguments: \n",
    "\n",
    "1. a minibatch of examples `x` having shape `batch_size x k` as input.\n",
    "1. A tensor `lengths` which indiactes the length of each example in `x`. \n",
    "\n",
    "Your forward function should:\n",
    "\n",
    "1. Apply `self.embedding` to x. This results in a `batch_size x k x EMB_SIZE` tensors.\n",
    "1. You should then compute the sum of the embeddings for each example in the batch using [`torch.tensor.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html?highlight=sum#torch.sum) resulting in a `batch_size x EMB_SIZE` tensor.\n",
    "1. Normalize each embedded example by dividing with the lengths in `lengths`. You can first use [`unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html?highlight=unsqueeze#torch.unsqueeze) and [`expand`](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html?highlight=expand) `lengths` to a `batch_size x EMB_SIZE` tensor and then use [`torch.div`](https://pytorch.org/docs/stable/generated/torch.div.html)    \n",
    "1. Pass the averaged embeddings through `self.linear1`, `self.dropout` `self.relu`, `self.linear2` and finally `self.log_softmax`. This results in a `batch_size x num_classes` tensor.\n",
    "1. Return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d838b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "EMB_SIZE = 100\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, num_words, num_classes, dropout_prob):\n",
    "        super().__init__()\n",
    "#your code here\n",
    "        # number of input words -> embedding size\n",
    "        # for nn.Embedding;\n",
    "\n",
    "        # embedding size -> hidden size\n",
    "        # for nn.Linear\n",
    "        \n",
    "        # dropout\n",
    "        # nn.Dropout\n",
    "\n",
    "        # relu\n",
    "        # nn.ReLU\n",
    "\n",
    "        # hidden size -> number of output class\n",
    "        # for nn.Linear \n",
    "\n",
    "        # logsoftmax\n",
    "        # for LogSoftmax\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "#your code here\n",
    "# x -> embedding -> linear -> dropout -> relu -> linear -> return log_softmax; \n",
    "\n",
    "# for you x:\n",
    "    # 1. You should then compute the sum of the embeddings for each example in the batch using \n",
    "    #       [`torch.tensor.sum`] resulting in a `batch_size x EMB_SIZE` tensor.\n",
    "    # 1. Normalize each embedded example by dividing with the lengths in `lengths`. \n",
    "    # You can first use [`unsqueeze`] and [`expand`] `lengths` to a `batch_size x EMB_SIZE` tensor and \n",
    "    # then use [`torch.div`]\n",
    "\n",
    "        # TO DIV your x;\n",
    "\n",
    "        # print(\"x after embbeding: \", x.size(), \": batch_size x k x EMB_SIZE\")\n",
    "        # print(\"x after computing the sum of the embeddings for each example: \", x.size(), \": batch_size x EMB_SIZE\")\n",
    "\n",
    "        # print(\"length before unsqueeze:\", lengths.size())\n",
    "        # print(\"length after unsqueeze:\", lengths.size())\n",
    "        # print(\"length after expand:\", lengths.size()) \n",
    "        # print(\"then, you can use `div`, and `linear`, ....\")\n",
    "\n",
    "\n",
    "### RESULT:\n",
    "# x after embbeding:  torch.Size([10, 131, 100]) : batch_size x k x EMB_SIZE\n",
    "# x after computing the sum of the embeddings for each example:  torch.Size([10, 100]) : batch_size x EMB_SIZE\n",
    "\n",
    "# length before unsqueeze: torch.Size([10])\n",
    "# length after unsqueeze: torch.Size([10, 1])\n",
    "# then, get batch_size to expand; \n",
    "# length after expand: torch.Size([10, 100])\n",
    "\n",
    "# then, you can use `div`, and `linear`, ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29088ca",
   "metadata": {},
   "source": [
    "Assertions to check your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e98520b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7598, -0.6307], grad_fn=<SelectBackward0>)\n",
      "tensor([0.4678, 0.5322], grad_fn=<ExpBackward0>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "\n",
    "model = CBOW(len(word2id), 2, 0)    # initialize with `num_words, num_classes, dropout_prob`\n",
    "model.train(False)                  # disable dropout\n",
    "x = torch_train[0]                  # your x is tuple (mbatch, lengths, mbatch_y);\n",
    "res = model(x[0], x[1])             # foward using mbatch and length, which gives a result of log_softmax: values in the range [-inf, 0)\n",
    "assert res.size()[0] == x[0].size()[0]\n",
    "assert res[0].exp().sum() - 1 < 0.001\n",
    "print(res[0])                       # log_softmax restuls; \n",
    "print(res[0].exp())                 # return a new tensor with the exponential ;\n",
    "print(res[0].exp().sum())           # sum = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7afe0",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "### Assignment 4.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Write a function `eval_model` which takes two arguments:\n",
    "\n",
    "1. `data`, a torch data set containing examples `(input_minibatch, lengths, output_minibatch)` \n",
    "1. `model` a CBOW model\n",
    "\n",
    "The function applies `model` to each input minibatch in `data` and returns the macro F-score computed by the sklearn function `f1_score`. \n",
    "\n",
    "Before running inference, make sure to call `model.train(False)` to disable dropout.\n",
    "\n",
    "**Remember** to use `with torch.no_grad()` in order to avoid !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c91b527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate data: input_minibatch, lengths, output_minibatch\n",
    "# forward(input_minibatch, lengths) \n",
    "# save ressult using argmax     (sys)\n",
    "# save output_minibatch         (gold)\n",
    "# then, f1_score(gold, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b69f1c",
   "metadata": {},
   "source": [
    "You can now evaluate an untrained model on the development set. The performance is unlikely to be particularly good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6d71a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37347980766160427"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CBOW(len(word2id), 2, 0)\n",
    "eval_model(torch_dev, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca52376",
   "metadata": {},
   "source": [
    "### Assignment 4.2\n",
    "rubric={accuracy:3}\n",
    "\n",
    "You should now write a training function `train_model`. The function takes the following parameters:\n",
    "\n",
    "1. `model`, a CBOW model\n",
    "1. `train_data`, a dataset of torch training examples\n",
    "1. `dev_data`, a dataset of torch development examples\n",
    "1. `max_epochs`, the maximum number of epochs for training\n",
    "\n",
    "You should first:\n",
    "\n",
    "1. Initialize a `CBOW` model `model` with `len(word2id)` word types, 2 output classes and dropout probability `dropout_prob`\n",
    "1. Initialize an `Adam` optimizer for `model` (you can use the deafaults for the `lr` and `betas`)\n",
    "1. Initialize an [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss function.\n",
    "\n",
    "Run training for `max_epochs`. Each epoch iterates over the training examples `(x, lengths, y)` in `train` and:\n",
    "\n",
    "1. Calls `model.train(True)` to enable dropout\n",
    "1. Calls `zero_grad` to erase old gradients                 \n",
    "    - it requires for each data iteration!\n",
    "1. Applys the model to `x`\n",
    "1. Compute the loss w.r.t. `y`.\n",
    "1. Runs one step of backprop.\n",
    "\n",
    "You should keep track of the average loss per training example over the epoch. As a general rule, the average loss should decrease through training. \n",
    "\n",
    "Once every epoch, you need to evaluate your model the development data. Prinf the average loss and the `f1_score` on the development set. \n",
    "\n",
    "Keep track of the best development accuracy and store the model which attains the best development accuracy. You can use `deepcopy` to save the model so that its parameters won't be affected by subsequent updates.\n",
    "\n",
    "Finally, return the best model you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552347e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def train_model(model, train_data, dev_data, max_epochs):\n",
    "\n",
    "    # torch.optim.Adam(...)\n",
    "    # nn.NLLLoss()\n",
    "    ...\n",
    "\n",
    "    for epoch ...\n",
    "        ...\n",
    "        model.train(True)\n",
    "        for ... using `torch_train`\n",
    "            # 1. Calls `zero_grad` to erase old gradients                 \n",
    "            #     - it requires for each data iteration!\n",
    "            # 1. Applys the model to `x`\n",
    "            # 1. Compute the loss w.r.t. `y`.\n",
    "            # 1. Runs one step of backprop.      \n",
    "\n",
    "            # keep track of the average loss per training      \n",
    "            ...\n",
    "        ...\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Average loss = {total_loss/len(train_data)}, Dev F1 = {f1}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4146e",
   "metadata": {},
   "source": [
    "Now, train a model for 50 `max_epochs` with dropout probability 30%. You will probably get within 5%-points from CountVectorizer but without pretrained embeddings, it is hard to do better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e93425ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average loss = 0.8060874354314297, Dev F1 = 0.3714644877435575\n",
      "Epoch 2: Average loss = 0.7578593166999064, Dev F1 = 0.3714644877435575\n",
      "Epoch 3: Average loss = 0.7017631801750285, Dev F1 = 0.3714644877435575\n",
      "Epoch 4: Average loss = 0.7046927934780495, Dev F1 = 0.3714644877435575\n",
      "Epoch 5: Average loss = 0.679618664286193, Dev F1 = 0.3714644877435575\n",
      "Epoch 6: Average loss = 0.6864398440660732, Dev F1 = 0.3714644877435575\n",
      "Epoch 7: Average loss = 0.6642061903010327, Dev F1 = 0.3714644877435575\n",
      "Epoch 8: Average loss = 0.6666451351601858, Dev F1 = 0.3714644877435575\n",
      "Epoch 9: Average loss = 0.6604696580776849, Dev F1 = 0.3714644877435575\n",
      "Epoch 10: Average loss = 0.6538994665610914, Dev F1 = 0.3963202566089417\n",
      "Epoch 11: Average loss = 0.6331285712388628, Dev F1 = 0.4202847373036473\n",
      "Epoch 12: Average loss = 0.6037663973416716, Dev F1 = 0.45232760766741353\n",
      "Epoch 13: Average loss = 0.5653785109101361, Dev F1 = 0.6370346432870142\n",
      "Epoch 14: Average loss = 0.4918996149264689, Dev F1 = 0.6915529574196202\n",
      "Epoch 15: Average loss = 0.49765888544735604, Dev F1 = 0.7276798426134451\n",
      "Epoch 16: Average loss = 0.4576774934399254, Dev F1 = 0.6415613295880149\n",
      "Epoch 17: Average loss = 0.47802337210413987, Dev F1 = 0.7266971016774237\n",
      "Epoch 18: Average loss = 0.6006719765661195, Dev F1 = 0.5308058180078568\n",
      "Epoch 19: Average loss = 0.4013406005686491, Dev F1 = 0.6698186109901227\n",
      "Epoch 20: Average loss = 0.3583838790787249, Dev F1 = 0.6030378836676303\n",
      "Epoch 21: Average loss = 0.5200254325424595, Dev F1 = 0.5335020590253946\n",
      "Epoch 22: Average loss = 0.3394453555701677, Dev F1 = 0.7481271607129383\n",
      "Epoch 23: Average loss = 0.2378916037317792, Dev F1 = 0.6003498823671352\n",
      "Epoch 24: Average loss = 0.32343740868135384, Dev F1 = 0.5452103468635472\n",
      "Epoch 25: Average loss = 0.3343543281638675, Dev F1 = 0.5792360000051157\n",
      "Epoch 26: Average loss = 0.30939806764877925, Dev F1 = 0.582788427801346\n",
      "Epoch 27: Average loss = 0.19907373400733747, Dev F1 = 0.7439236111111112\n",
      "Epoch 28: Average loss = 0.0980731742874426, Dev F1 = 0.7273816121131949\n",
      "Epoch 29: Average loss = 0.135776650364733, Dev F1 = 0.6163963854359736\n",
      "Epoch 30: Average loss = 0.2141596980572896, Dev F1 = 0.6534350561196199\n",
      "Epoch 31: Average loss = 0.12564581075806147, Dev F1 = 0.7050363331214473\n",
      "Epoch 32: Average loss = 0.1253279322986179, Dev F1 = 0.7257736778694862\n",
      "Epoch 33: Average loss = 0.13636102875954853, Dev F1 = 0.6942689856075683\n",
      "Epoch 34: Average loss = 0.1352304639953313, Dev F1 = 0.6465921384781824\n",
      "Epoch 35: Average loss = 0.14170546501079642, Dev F1 = 0.6765375269016849\n",
      "Epoch 36: Average loss = 0.07371657989205815, Dev F1 = 0.7381541356744767\n",
      "Epoch 37: Average loss = 0.0992240795193333, Dev F1 = 0.7141768292682926\n",
      "Epoch 38: Average loss = 0.07340588513996547, Dev F1 = 0.7376942124023836\n",
      "Epoch 39: Average loss = 0.045271462312910966, Dev F1 = 0.747319860554114\n",
      "Epoch 40: Average loss = 0.03116606051672899, Dev F1 = 0.7352686700156255\n",
      "Epoch 41: Average loss = 0.025168303880140037, Dev F1 = 0.740742804005845\n",
      "Epoch 42: Average loss = 0.02162194681060934, Dev F1 = 0.7349640585406216\n",
      "Epoch 43: Average loss = 0.020400137487462933, Dev F1 = 0.7296866137266024\n",
      "Epoch 44: Average loss = 0.07091854423239434, Dev F1 = 0.7276297899636579\n",
      "Epoch 45: Average loss = 0.03344877108436145, Dev F1 = 0.7470297160699637\n",
      "Epoch 46: Average loss = 0.10093520030394805, Dev F1 = 0.6424103737604883\n",
      "Epoch 47: Average loss = 0.07221926757478114, Dev F1 = 0.7326917936380647\n",
      "Epoch 48: Average loss = 0.021676015904386392, Dev F1 = 0.7312492839560105\n",
      "Epoch 49: Average loss = 0.02592107039149948, Dev F1 = 0.7350993377483444\n",
      "Epoch 50: Average loss = 0.026914908913074326, Dev F1 = 0.7382141156210477\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(len(word2id), 2, 0.3)\n",
    "model = train_model(model, torch_train, torch_dev, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfe46d",
   "metadata": {},
   "source": [
    "Print the F-score of your model on the test data. Compare this against our CountVectorizer and TfidfVectorizer models.\n",
    "\n",
    "CBOW will probably land somewhere between CountVectorizer and TfidfVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dcf80e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score CBOW: 0.7413886839126629\n",
      "F-score CountVectorizer: 0.7772952581265258\n",
      "F-score TfidfVectorizer: 0.5661413372993042\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "## macro restuls: \n",
    "print(f\"F-score CBOW: {cbow_f1}\")\n",
    "print(f\"F-score CountVectorizer: {count_f1}\")\n",
    "print(f\"F-score TfidfVectorizer: {tfidf_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8788b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
