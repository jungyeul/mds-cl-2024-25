{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 535 Lab Assignment 3: CFG parsing algorithms (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will:\n",
    "- Implement the CYK parsing algorithm\n",
    "- Use probabilities to pick the best parse\n",
    "- Practice chart parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.earleychart import EarleyChartParser\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk import CFG,PCFG\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy submission\n",
    "\n",
    "rubric={mechanics:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: CYK parsing\n",
    "\n",
    "In the next two exercises you will building a parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Your CYK parser will be a class with an initialization function taking a grammar as argument. We will first write the initialization function for the class. There are three tasks you need to complete:\n",
    "\n",
    "1. Check whether the grammar you've been given is in Chomsky Normal Form. If it's not, you should convert it to CNF using NLTK library functions (see Monday's practical lecture). Store the grammar as `self.grammar`\n",
    "\n",
    "2. Build a Python dictionary `self.lexical_rules` which allows you to look up LHS of lexical rules based on their RHS (which will be a word form). For example, if your grammar contains the rules:\n",
    "```\n",
    "NP -> DT N\n",
    "VP -> V NP\n",
    "N -> 'look'\n",
    "V -> 'look'\n",
    "N -> 'cat'\n",
    "DT -> 'a'\n",
    "```\n",
    "You would have a dictionary like this:\n",
    "```\n",
    "{\n",
    " 'look': [V, N],\n",
    " 'house': [N],\n",
    " 'a' : [DT]\n",
    "}\n",
    "```\n",
    "Note that the non-terminals e.g. `V` and `DT` are not strings. They are `nltk.grammar.Nonterminal` objects.\n",
    "\n",
    "3. Build a Python dictionary `self.grammatical_rules` which allows you to look up LHS of grammatical rules based on their RHS (which will be a word form). For our example grammar, the dictionary would look like:\n",
    "```\n",
    "{\n",
    "(DT,N):[NP],\n",
    "(V,NP):[VP]\n",
    "}\n",
    "```\n",
    "\n",
    "**Why are we doing this?** When working with realistic grammars, you can't afford to be scanning through all the rules to find one that applies, you need the fast look-up of a hash map like Python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class CYKParser(object):\n",
    "    \n",
    "    def __init__(self, grammar):\n",
    "        # your code here\n",
    "\n",
    "        # your code here\n",
    "            \n",
    "    def parse(self,sentence):\n",
    "        \"\"\" No need to do anything about parse() yet. \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to test your initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_grammar (CFG):\n",
      "Grammar with 4 productions (start state = S)\n",
      "    S -> S2\n",
      "    S2 -> NP VP PP\n",
      "    NP -> 'dog'\n",
      "    VP -> 'dog'\n",
      "------------------------\n",
      "\n",
      "dummy_grammar (CNF):\n",
      "Grammar with 5 productions (start state = S)\n",
      "    NP -> 'dog'\n",
      "    S2 -> NP S2@$@NP\n",
      "    S2@$@NP -> VP PP\n",
      "    S -> NP S2@$@NP\n",
      "    VP -> 'dog'\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "dummy_grammar = CFG.fromstring('''\n",
    "S -> S2\n",
    "S2 -> NP VP PP\n",
    "NP -> \"dog\"\n",
    "VP -> \"dog\"\n",
    "''')\n",
    "\n",
    "dummy_parser = CYKParser(dummy_grammar)\n",
    "\n",
    "assert dummy_parser.grammar.is_chomsky_normal_form()\n",
    "assert (Nonterminal('VP'),Nonterminal('PP')) in dummy_parser.grammatical_rules\n",
    "assert 'dog' in dummy_parser.lexical_rules\n",
    "assert set(dummy_parser.lexical_rules[\"dog\"]) == set([Nonterminal(\"NP\"), Nonterminal(\"VP\")])\n",
    "assert dummy_parser.grammatical_rules[(Nonterminal('VP'), Nonterminal('PP'))] == [Nonterminal('S2@$@NP')]\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Write a function `create_table` which builds the table data structure. It should be a Python dictionary where the keys are ranges `(i,j)`, where \n",
    "```\n",
    "0 <= i < j <= sent_len\n",
    "```\n",
    "For each key `(i,j)`, you should set the value of `table[(i,j)]` to `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(sent_len):\n",
    "    '''Create a table appropriate for CYK parsing. Returns a dictionary of empty lists'''\n",
    "    table = {}\n",
    "\n",
    "    #your code here\n",
    "    \n",
    "    #your code here\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to test your `create_table` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert create_table(3) == {(0,1):[], (0,2):[], (0,3):[], (1,2):[], (1,3):[], (2,3):[]}\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3\n",
    "rubric={accuracy:4,efficiency:2}\n",
    "\n",
    "In this step, you're going to write the main parsing logic for `CYKParser`. \n",
    "\n",
    "Start by copying your initialization code from assignment 1.1 and pasting it into the `__init__` function of `CYKParser`.\n",
    "\n",
    "Your first actual task is to implement the **function `CYKParser.check_sentence()`**. It takes a sentence as argument and **returns `True` if we can parse the sentence**, meaning that all the words in the sentence are found in the `lexical_rules` dictionary. Otherwise, it returns `False`.\n",
    "\n",
    "We will now work on **`CYKParser.parse()`**. You should implement code **to fill in the cells `(i,j)` of the parse table `table`**. The existing code iterates through the columns of `table` from left to right and iterates through every column from the bottom to the top. It then checks every split of the **range `(i,j)` into two children `(i,k)` and `(k,j)`**. It is your task to find all combinations of non-terminals `Y` and `Z` from `table[(i,k)]` and `table[(k,j)]` which can be combined into a new non-terminal `X` using a grammatical rule\n",
    "```\n",
    "X -> Y Z\n",
    "```\n",
    "You should then add the non-terminal `X` to cell `(i,j)` in the parse table. Note that you might add several non-terminals to the parse table in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class CYKParser(object):\n",
    "    def __init__(self, grammar):\n",
    "        # Copy-paste your __init__ function code from 1.1.\n",
    "\n",
    "        # your code here\n",
    "        \n",
    "                \n",
    "    def check_sentence(self, sentence):\n",
    "        # Check that all of the words in our sentence are in the grammar. \n",
    "        # If they aren't return False.\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        if not self.check_sentence(sentence):\n",
    "            return False\n",
    "            \n",
    "        # Create parse table\n",
    "        table = create_table(len(sentence))\n",
    "        \n",
    "        # Iterate through the parse table from left to right\n",
    "        for j in range(1,len(sentence)+1):\n",
    "            # Add the word form at position j-1 to the parse table\n",
    "            table[(j-1, j)] += self.lexical_rules[sentence[j-1]]\n",
    "            \n",
    "            # Iterate through colum j of the parse table \n",
    "            for i in range(j-2, -1, -1):\n",
    "                \n",
    "                # Iterate through all fence-posts in the range (i,j)\n",
    "                for k in range(i+1, j):\n",
    "                    # your code here\n",
    "                    \n",
    "                    # your code here\n",
    "        \n",
    "        # Check that we actually managed to generate a sentence node in \n",
    "        # the upper right corner of the parse table. This means that the\n",
    "        # grammar accepts our sentence\n",
    "        if self.grammar.start() in table[(0,len(sentence))]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check that your parser works as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 6 productions (start state = S)\n",
      "    VB -> 'waits'\n",
      "    NN -> 'person'\n",
      "    DT -> 'a'\n",
      "    NP -> DT NN\n",
      "    VP -> 'waits'\n",
      "    S -> NP VP\n",
      "( 0 , 2 ) <- NP\n",
      "( 0 , 3 ) <- S\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "grammar = CFG.fromstring('''\n",
    "S -> NP VP\n",
    "NP -> DT NN\n",
    "VP -> VB\n",
    "DT -> 'a'\n",
    "NN -> 'person'\n",
    "VB -> 'waits'\n",
    "''')\n",
    "\n",
    "sent1 = ['a', 'person', 'waits']\n",
    "sent2 = ['a', 'person', 'wait']\n",
    "sent3 = ['waits', 'person', 'a']\n",
    "\n",
    "parser = CYKParser(grammar)\n",
    "assert parser.parse(sent1)\n",
    "assert not parser.parse(sent2)\n",
    "assert not parser.parse(sent3)\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1.3](1.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Start by copying your initialization code from assignment 1.1 and pasting it into the `__init__` function of `CYKParser`. Then copy your code from 1.3 and paste it into the `parse()` function.\n",
    "\n",
    "In this assignment, we're working with **a second table object `parse_pointers`**. For each non-terminal that you add to `table` for range `(i,j)` and fence-post `k`, you should **add a tuple `(fence-post, left_child, right_child)` to parse-pointers**. This indicates the phrase boundary and which rule was used to derive the non-terminal in cell `(i,j)`. The `parse()` function will then call `generate_trees` which will generate the set of parse trees for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_trees(parse_pointers,table,i,j,non_term,sentence):\n",
    "    if i+1 == j:\n",
    "        return [Tree(pos,[sentence[i]]) for pos in table[(i,j)]]\n",
    "    else:\n",
    "        trees = []\n",
    "        # Iterate over sets here to avoid duplicate trees in the list.\n",
    "        for mother, (k, left_label, right_label) in set(zip(table[(i,j)], parse_pointers[(i,j)])):\n",
    "            if mother != non_term:\n",
    "                continue\n",
    "            left_children = _generate_trees(parse_pointers, table, i, k, left_label, sentence)\n",
    "            right_children = _generate_trees(parse_pointers, table, k, j, right_label, sentence)\n",
    "            for child1 in left_children:\n",
    "                for child2 in right_children:\n",
    "                    if child1.label() == left_label and child2.label() == right_label:\n",
    "                        trees.append(Tree(mother, [child1, child2]))\n",
    "        return trees\n",
    "\n",
    "def generate_trees(parse_pointers,table,sentence):\n",
    "    \"\"\" This is a helper function which generates parse trees based on a parse_pointer table\n",
    "        and a parse table. \"\"\"\n",
    "    return _generate_trees(parse_pointers, table, 0, len(sentence), Nonterminal(\"S\"), sentence)\n",
    "    \n",
    "class CYKParser(object):\n",
    "    def __init__(self, grammar):\n",
    "        # Copy-paste your __init__ function code from 1.1.\n",
    "        # your code here\n",
    "                \n",
    "    def check_sentence(self, sentence):\n",
    "        # Copy-paste your code from 1.3\n",
    "        # your code here\n",
    "       \n",
    "    def parse(self, sentence):\n",
    "        if not self.check_sentence(sentence):\n",
    "            return []\n",
    "            \n",
    "        # Create parse table and pointer table\n",
    "        table = create_table(len(sentence))\n",
    "        parse_pointers = create_table(len(sentence))\n",
    "        \n",
    "        # Iterate through the parse table from left to right\n",
    "        for j in range(1,len(sentence)+1):\n",
    "            # Add the word form at position j-1 to the parse table\n",
    "            table[(j-1, j)] += self.lexical_rules[sentence[j-1]]\n",
    "            \n",
    "            # Iterate through colum j of the parse table \n",
    "            for i in range(j-2, -1, -1):\n",
    "                \n",
    "                # Iterate through all fence-posts in the range (i,j)\n",
    "                for k in range(i+1, j):\n",
    "                    # Start by copy-pasting your code from 1.3, then add code \n",
    "                    # for updating the parse_pointers\n",
    "                    # your code here\n",
    "\n",
    "\n",
    "        if self.grammar.start() in table[(0,len(sentence))]:\n",
    "            return generate_trees(parse_pointers,table,sentence)\n",
    "        else:\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check that your `pointer_table` is working as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0 , 2 ) <- NP | 1 DT NN\n",
      "( 3 , 5 ) <- NP | 4 DT NN\n",
      "( 2 , 5 ) <- VP | 3 VBZ NP\n",
      "( 0 , 5 ) <- S | 2 NP VP\n"
     ]
    }
   ],
   "source": [
    "grammar = CFG.fromstring('''\n",
    "S -> NP VP\n",
    "NP -> DT NN\n",
    "VP -> VBZ NP\n",
    "DT -> 'a'\n",
    "NN -> 'person' | 'dog'\n",
    "VBZ -> 'sees'\n",
    "''')\n",
    "\n",
    "sent1 = ['a', 'person', 'sees', 'a', 'dog']\n",
    "sent2 = ['a', 'person', 'sees']\n",
    "sent3 = ['dog', 'person', 'sees', 'a', 'person']\n",
    "\n",
    "parser = CYKParser(grammar)\n",
    "parses = parser.parse(sent1)\n",
    "assert len(parses) == 1\n",
    "str(Tree.fromstring('(S (NP (DT a) (NN person)) (VP (VBZ sees) (NP (DT a) (NN dog))))'))\n",
    "assert(str(parses[0]) == str(Tree.fromstring('(S (NP (DT a) (NN person)) (VP (VBZ sees) (NP (DT a) (NN dog))))')))\n",
    "assert parser.parse(sent2) == []\n",
    "assert parser.parse(sent3) == []\n",
    "\n",
    "pp_grammar = CFG.fromstring('''\n",
    "S -> NP VP\n",
    "NP -> NP PP\n",
    "NP -> DT NN\n",
    "VP -> VP PP\n",
    "VP -> VBZ NP\n",
    "PP -> IN NP\n",
    "DT -> 'a'\n",
    "NN -> 'person' | 'dog'\n",
    "VBZ -> 'sees'\n",
    "IN -> 'with'\n",
    "''')\n",
    "sent1 = ['a', 'person', 'sees', 'a', 'dog', 'with', 'a', 'person']\n",
    "parser = CYKParser(pp_grammar)\n",
    "parses = parser.parse(sent1)\n",
    "assert len(parses) == 2\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1.4](1.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: PCFG parsing\n",
    "\n",
    "Now, rather than just outputing either one random parse or all possibile parses, let's see how we could pick the best parse. Let's convert the CFG parser into a PCFG parser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1\n",
    "rubric= {accuracy:2}\n",
    "\n",
    "\n",
    "For the ambiguous sentence `S` provided below (**HINT**: *flies* can be both a noun and a verb, *like* can be both a verb and a preposition), create a corresponding PCFG grammar called `pcfg_grammar` which reflects at least one possible ambiguity. It doesn't matter what the probabilities are as long as all the probabilities for a given LHS symbol add up to one. Make sure it is already in Chomsky normal form, since the conversion we've been using doesn't work for PCFGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = ['time', 'flies', 'like', 'an', 'arrow']\n",
    "\n",
    "# your code here\n",
    "from nltk import PCFG\n",
    "\n",
    "pcfg_grammar = PCFG.fromstring('''S -> NP VP [0.1]\n",
    "S -> NP VBN [0.2]\n",
    "S -> NNS VP [0.2]\n",
    "S -> NNS VBN [0.2]\n",
    "S -> VP PP [0.2]\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert pcfg_grammar.is_chomsky_normal_form()\n",
    "parser = CYKParser(pcfg_grammar)\n",
    "assert len(parser.parse(S)) > 1\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Optional\n",
    "rubric= {accuracy:4,efficiency:1,quality:1}\n",
    "\n",
    "Write a `best_parse` function which takes to arguments: `trees` which is a list of NLTK syntax trees and `grammar` which is a `PCFG`. The function iterates over trees, and picks the one which is assigned the highest probability by `grammar`. \n",
    "\n",
    "To get a good efficiency score, you need to do this in a way that will scale well to thousands of parses from a grammar with thousands of productions. One way to do this is to store your grammar productions in a python dictionary like we did in assignment 1. \n",
    "\n",
    "**HINT:** Remember that the probability of a tree is the product of the probabilities of all productions, e.g. `S -> NP VP [0.7]` and `NNS -> 'dogs' [0.2]`, which are used when generating the tree. It can be a good idea to use recursion for computing the probability of a parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should write your own test cases for `best_parse` (at least two) consisting of lists of two or more parse trees which should be outputs of the PCFG you wrote in assignment 2.1. Make sure that `best_parse` returns the tree which gets the highest probability from your grammar.\n",
    "\n",
    "Your test cases should involve at trees of at least height 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Chart parsing\n",
    "#### 3.1 \n",
    "rubric={accuracy:1}\n",
    "\n",
    "Use NLTK to carry out an Earley chart parse of the sentence below, in `trace=1` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|. the  . cat  .slept .  on  . the  .porch .|\n",
      "|[------]      .      .      .      .      .| [0:1] 'the'\n",
      "|.      [------]      .      .      .      .| [1:2] 'cat'\n",
      "|.      .      [------]      .      .      .| [2:3] 'slept'\n",
      "|.      .      .      [------]      .      .| [3:4] 'on'\n",
      "|.      .      .      .      [------]      .| [4:5] 'the'\n",
      "|.      .      .      .      .      [------]| [5:6] 'porch'\n",
      "|>      .      .      .      .      .      .| [0:0] S  -> * NP VP\n",
      "|>      .      .      .      .      .      .| [0:0] NP -> * DT NN\n",
      "|>      .      .      .      .      .      .| [0:0] DT -> * 'the'\n",
      "|[------]      .      .      .      .      .| [0:1] DT -> 'the' *\n",
      "|[------>      .      .      .      .      .| [0:1] NP -> DT * NN\n",
      "|.      >      .      .      .      .      .| [1:1] NN -> * 'cat'\n",
      "|.      [------]      .      .      .      .| [1:2] NN -> 'cat' *\n",
      "|[-------------]      .      .      .      .| [0:2] NP -> DT NN *\n",
      "|[------------->      .      .      .      .| [0:2] S  -> NP * VP\n",
      "|.      .      >      .      .      .      .| [2:2] VP -> * VBD\n",
      "|.      .      >      .      .      .      .| [2:2] VP -> * VP PP\n",
      "|.      .      >      .      .      .      .| [2:2] VBD -> * 'slept'\n",
      "|.      .      [------]      .      .      .| [2:3] VBD -> 'slept' *\n",
      "|.      .      [------]      .      .      .| [2:3] VP -> VBD *\n",
      "|[--------------------]      .      .      .| [0:3] S  -> NP VP *\n",
      "|.      .      [------>      .      .      .| [2:3] VP -> VP * PP\n",
      "|.      .      .      >      .      .      .| [3:3] PP -> * IN NP\n",
      "|.      .      .      >      .      .      .| [3:3] IN -> * 'on'\n",
      "|.      .      .      [------]      .      .| [3:4] IN -> 'on' *\n",
      "|.      .      .      [------>      .      .| [3:4] PP -> IN * NP\n",
      "|.      .      .      .      >      .      .| [4:4] NP -> * DT NN\n",
      "|.      .      .      .      >      .      .| [4:4] DT -> * 'the'\n",
      "|.      .      .      .      [------]      .| [4:5] DT -> 'the' *\n",
      "|.      .      .      .      [------>      .| [4:5] NP -> DT * NN\n",
      "|.      .      .      .      .      >      .| [5:5] NN -> * 'porch'\n",
      "|.      .      .      .      .      [------]| [5:6] NN -> 'porch' *\n",
      "|.      .      .      .      [-------------]| [4:6] NP -> DT NN *\n",
      "|.      .      .      [--------------------]| [3:6] PP -> IN NP *\n",
      "|.      .      [---------------------------]| [2:6] VP -> VP PP *\n",
      "|[=========================================]| [0:6] S  -> NP VP *\n",
      "|.      .      [--------------------------->| [2:6] VP -> VP * PP\n",
      "|.      .      .      .      .      .      >| [6:6] PP -> * IN NP\n",
      "(S\n",
      "  (NP (DT the) (NN cat))\n",
      "  (VP (VP (VBD slept)) (PP (IN on) (NP (DT the) (NN porch)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the cat slept on the porch\".split()\n",
    "\n",
    "grammar = CFG.fromstring('''\n",
    "S -> NP VP\n",
    "NP -> DT NN\n",
    "VP -> VBD\n",
    "VP -> VP PP\n",
    "PP -> IN NP\n",
    "DT -> 'the'\n",
    "NN -> 'cat'\n",
    "NN -> 'porch'\n",
    "VBD -> 'slept'\n",
    "IN -> 'on'\n",
    "''')\n",
    "\n",
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "rubric={raw:3}\n",
    "\n",
    "For each line in the trace, remove the visual span part of the line and indicate which part of the algorithm produced it and which other line or lines in the trace triggered this line. For instance, a completer line should point to both the version of rule before the dot moved as well as the completed/scanned rule, whereas the predictor should point to the completer line that caused the prediction to be made. You should number the lines for this purposes.  For example,\n",
    "\n",
    "```\n",
    "4. PP -> * P NP  Predictor 3\n",
    "5. P -> 'in' *  Scanner\n",
    "6. PP -> P * NP Completer 4 5\n",
    "```\n",
    "\n",
    "You can remove the lines corresponding to the words in the sentence and unscanned lexical rules such as `P -> * 'in'`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  |>      .      .      .      .      .      .| [0:0] S  -> * NP VP     START\n",
    "2.  |>      .      .      .      .      .      .| [0:0] NP -> * DT NN     Predictor 1\n",
    "3.  |[------]      .      .      .      .      .| [0:1] DT -> 'the' *     Scanner\n",
    "4.  |[------>      .      .      .      .      .| [0:1] NP -> DT * NN     Completer 2 3 \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Teamwork report\n",
    "\n",
    "#### 4.1\n",
    "\n",
    "rubric={raw:1, reasoning:1}\n",
    "\n",
    "Briefly discuss how each person contributed to the project. Though it is not necessary that every group member has a equal contribution in terms of code, every group member should have a significant contribution.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
