{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 535 Lab Assignment 2: Working with CFG Parsers (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of constituent parsers:\n",
    "\n",
    "\n",
    "- Magerman parser {magerman:1995:ACL}\n",
    "- Charniak parser {charniak:1996,charniak-goldwater-johnson:1998}\n",
    "- Johnson parser {johnson:1998:CL}\n",
    "- Collins parser {collins:1996:ACL,collins:1999} $\\rightarrow$ Bikel parser {bikel:2004,bikel:2004:CL}}\n",
    "- Stanford parser {klein-manning:2003:ACL,klein-manning:2003:HLT-NAACL,klein-manning:2003:NIPS} $\\leftarrow$ **Lab2**\n",
    "- (statistical) Berkeley parser {petrov-EtAl:2006:COLACL,petrov-klein:2007:main}\n",
    "- (neural) Berkeley parser {kitaev-klein:2018:ACL,kitaev-cao-klein:2019:ACL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will \n",
    "- Use the Stanford CoreNLP parser to parse new text into constituency trees\n",
    "- Create a parsing gold standard and use it to evaluate parsers\n",
    "- Build a context-free grammar from existing parses (optional assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment requires that you have set up the Stanford parser. First, make sure you have the more recent version of [Java](https://www.java.com/en/download/), then get the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) package. Make sure that you get the recent 4.5.1 version (or at least >=4.2.0). \n",
    "\n",
    "To load Stanford CoreNLP in Python, change the `coreNLP_dir` variable in the code below to where you unzipped Stanford coreNLP. You can follow this [tutorial](https://bbengfort.github.io/snippets/2018/06/22/corenlp-nltk-parses.html). Once the coreNLP server is running, you will be able to access it through NLTK.\n",
    "\n",
    "It may take a few seconds or up to a minute to start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.parse.corenlp import CoreNLPServer\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "\n",
    "# coreNLP_dir = \"/Users/jungyeul/Downloads/mds-cl-2022-23-block4/stanford-corenlp-4.5.1/\" # Change this to your coreNLP directory\n",
    "\n",
    "# server = CoreNLPServer(\n",
    "#    os.path.join(coreNLP_dir, \"stanford-corenlp-4.5.1.jar\"),\n",
    "#    os.path.join(coreNLP_dir, \"stanford-corenlp-4.5.1-models.jar\")    \n",
    "# )\n",
    "# server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running above command, you need to run the following command in the terminal to run CoreNLPServer:\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to parse a sentence to make sure that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT the) (NN book))\n",
      "      (PP (IN in) (NP (DT the) (NN box)))\n",
      "      (PP (IN on) (NP (DT the) (NN table))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "parse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ROOT                              \n",
      "                          |                                 \n",
      "                          S                                \n",
      "  ________________________|______________________________   \n",
      " |                        VP                             | \n",
      " |    ____________________|________________              |  \n",
      " |   |       |            PP               PP            | \n",
      " |   |       |         ___|____         ___|___          |  \n",
      " NP  |       NP       |        NP      |       NP        | \n",
      " |   |    ___|___     |    ____|___    |    ___|____     |  \n",
      "PRP VBD  DT      NN   IN  DT       NN  IN  DT       NN   . \n",
      " |   |   |       |    |   |        |   |   |        |    |  \n",
      " I  put the     book  in the      box  on the     table  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parser.raw_parse('I put the book in the box on the table.'))[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP They))\n",
      "    (VP\n",
      "      (VBD gave)\n",
      "      (NP (PRP me))\n",
      "      (NP (ADJP (JJ yellow) (CC and) (JJ blue)) (NNS pants)))))\n"
     ]
    }
   ],
   "source": [
    "parse = next(parser.raw_parse(\"They gave me yellow and blue pants\"))\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S                                 \n",
      "  ____|_________                         \n",
      " |              VP                      \n",
      " |     _________|__________              \n",
      " |    |    |               NP           \n",
      " |    |    |           ____|_________    \n",
      " NP   |    NP        ADJP            |  \n",
      " |    |    |     _____|________      |   \n",
      "PRP  VBD  PRP   JJ    CC       JJ   NNS \n",
      " |    |    |    |     |        |     |   \n",
      "They gave  me yellow and      blue pants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(next(parser.raw_parse(\"They gave me yellow and blue pants\")))[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ROOT                               \n",
      "      |                                  \n",
      "      S                                 \n",
      "  ____|_________                         \n",
      " |              VP                      \n",
      " |     _________|__________              \n",
      " |    |    |               NP           \n",
      " |    |    |           ____|_________    \n",
      " NP   |    NP        ADJP            |  \n",
      " |    |    |     _____|________      |   \n",
      "PRP  VBD  PRP   JJ    CC       JJ   NNS \n",
      " |    |    |    |     |        |     |   \n",
      "They gave  me yellow and      blue pants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parser.raw_parse(\"They gave me yellow and blue pants\"))[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if you want to shut down the coreNLP server after you've finished with it. It's a good idea to shut down the parser after finishing work with it because it may remain running in the background and you may not be able to start another parser instance without restarting your computer or manually killing the parser process. \n",
    "\n",
    "If you forget to stop the server, next time when you try to launch it, you'll get an error. In this case, you may first need to kill the old server manually. To do this, you can run `ps -ax | grep stanford` on the commandline (at least on OSX and Linux) which should give you the process ID of the server, e.g. 11111. You can then use `kill -9 11111` to kill the parser, after which you should be able to start the server again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    server.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things you'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from nltk.grammar import CFG,Nonterminal,Production,FeatureGrammar\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the marks for tidy submission:\n",
    "\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Test the parser\n",
    "\n",
    "#### 1.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Get the Stanford parser working, then parse the first 20 sentences of the NLTK `movie_reviews` corpus, and report the average depth (height) of the parse trees. If you find the parser is failing to parse something, you can skip over it.\n",
    "\n",
    "You should retain the tokenization in the `move_reviews` corpus. You can use `parser.parse()` to parse the tokenized input sentences. Note that `parser.parse()` returns an iterator over possible parses. There may be several if the sentence is ambiguous. You can compute statistics on the first sentence which is returned by `parser.parse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a gold standard\n",
    "\n",
    "One typical way to build treebanks is, rather than having humans build a tree from scratch, instead use an automatic parser to give an initial parse, and then have humans do a second pass to fix any errors. That's what you're going to do in this exercise. \n",
    "\n",
    "We will use the following three sentences from the NLTK `movie_reviews` corpus to build a mini-treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [['oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.'],\n",
    "          ['little', 'do', 'they', 'know', 'the', 'power', 'within', '.'],\n",
    "          ['so', ',', 'if', 'robots', 'and', 'body', 'parts', 'really', 'turn', 'you', 'on', ',', 'here', \"'\", 's', 'your', 'movie', '.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the three sections below you will see a parse tree produced by CorNLPParser. Each of the trees contains at least one parse error. You'll see a brief informal description of the error and it is your task to fix the tree.  \n",
    "\n",
    "Create an NLTK Tree corresponding to the correct parse, which should be appended to the `gold_standard_parses` list below. You can do this manually by printing the tree, creating a triple-quoted string, modifying it, and converting it back into a `Tree` using the function `Tree.fromstring` following the example below. If you are unsure exactly how to correct something, read through the lecture slides. Many common parse errors are explained there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNS Dogs)) (VP (VBN bark)) (. .))\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "\n",
    "# The second phrase should be an VP, not an NP\n",
    "err_tree_str = '''(S\n",
    "(NP (NNS Dogs)) \n",
    "(NP (VBN bark))\n",
    "(. .))\n",
    "'''\n",
    "\n",
    "corr_tree_str = '''(S\n",
    "(NP (NNS Dogs)) \n",
    "(VP (VBN bark))\n",
    "(. .))\n",
    "'''\n",
    "\n",
    "corr_tree = Tree.fromstring(corr_tree_str)\n",
    "print(corr_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S      \n",
      "  ____|____   \n",
      " NP   VP   | \n",
      " |    |    |  \n",
      "NNS  VBN   . \n",
      " |    |    |  \n",
      "Dogs bark  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corr_tree_root = Tree.fromstring(\"(ROOT \" + corr_tree_str + \")\")\n",
    "\n",
    "list(corr_tree_root)[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store your corrected nltk.Tree objects in this list\n",
    "gold_standard_parses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 1\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_tree_str:\n",
      "                                       S                                                   \n",
      "  _____________________________________|_________________________________________________   \n",
      " |    |   |       |           |   |                     VP                               | \n",
      " |    |   |       |           |   |     ________________|_____                           |  \n",
      " |    |   |       |           |   |    |   |                  NP                         | \n",
      " |    |   |       |           |   |    |   |        __________|______________            |  \n",
      " |    |   |       PP          |   |    |   |       |          |              NP          | \n",
      " |    |   |    ___|___        |   |    |   |       |          |         _____|______     |  \n",
      "INTJ  |   |   |       NP      |   NP   |   |       NP         |       NML           |    | \n",
      " |    |   |   |    ___|___    |   |    |   |    ___|____      |    ____|_____       |    |  \n",
      " UH   ,   CC  IN  DT      NN  ,   DT  VBZ  RB  DT       NN    CC  NN         NN     NN   . \n",
      " |    |   |   |   |       |   |   |    |   |   |        |     |   |          |      |    |  \n",
      " oh   ,  and  by the     way  ,  this  is not  a      horror  or teen     slasher flick  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The word \"flick\" should be modified by the entire noun phrase \n",
    "# \"a horror or teen slasher\" instead of just \"teen slasher\". A noun \n",
    "# phrase which modifies a noun is labeled as NML.\n",
    "err_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (INTJ (UH oh))\n",
    "    (, ,)\n",
    "    (CC and)\n",
    "    (PP (IN by) (NP (DT the) (NN way)))\n",
    "    (, ,)\n",
    "    (NP (DT this))\n",
    "    (VP\n",
    "      (VBZ is)\n",
    "      (RB not)\n",
    "      (NP\n",
    "        (NP (DT a) (NN horror))\n",
    "        (CC or)\n",
    "        (NP (NML (NN teen) (NN slasher)) (NN flick))))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"err_tree_str:\")\n",
    "list(Tree.fromstring(err_tree_str))[0].pretty_print()\n",
    "gold_standard_parses.append(Tree.fromstring(corr_tree_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 2\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_tree_str:\n",
      "                 S                           \n",
      "   ______________|_________________________   \n",
      "  |         VP                             | \n",
      "  |      ___|____                          |  \n",
      "  |     |       SBAR                       | \n",
      "  |     |        |                         |  \n",
      "  |     |        S                         | \n",
      "  |     |    ____|____                     |  \n",
      "  |     |   |         VP                   | \n",
      "  |     |   |     ____|______________      |  \n",
      "  NP    |   NP   |        NP         PP    | \n",
      "  |     |   |    |     ___|____      |     |  \n",
      "  RB   VBP PRP  VBP   DT       NN    IN    . \n",
      "  |     |   |    |    |        |     |     |  \n",
      "little  do they know the     power within  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The PP \"within\" should attach to the NP \"the power\", not to the VP \"know the power\". \n",
    "err_tree_str = '''(ROOT\n",
    "  (S\n",
    "    (NP (RB little))\n",
    "    (VP\n",
    "      (VBP do)\n",
    "      (SBAR\n",
    "        (S\n",
    "          (NP (PRP they))\n",
    "          (VP (VBP know) (NP (DT the) (NN power)) (PP (IN within))))))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"err_tree_str:\")\n",
    "list(Tree.fromstring(err_tree_str))[0].pretty_print()\n",
    "gold_standard_parses.append(Tree.fromstring(corr_tree_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence 3\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_tree_str:\n",
      "                                              SINV                                            \n",
      "  _____________________________________________|____________________________________________   \n",
      " |    |              |                   |                  VP                    |         | \n",
      " |    |              |                   |      ____________|____________         |         |  \n",
      " |    |              PP                  |     |            NP           |        |         | \n",
      " |    |    __________|___                |     |     _______|________    |        |         |  \n",
      " |    |   |              NP              |     |    |       PP       |   S        |         | \n",
      " |    |   |           ___|_________      |     |    |    ___|___     |   |        |         |  \n",
      "ADVP  |   |         NML            |    ADVP   |    NP  |   |   NP   |   NP       NP        | \n",
      " |    |   |     _____|_______      |     |     |    |   |   |   |    |   |    ____|____     |  \n",
      " RB   ,   IN  NNS    CC      NN   NNS    RB   VBP  PRP  IN  ,   RB   '' POS PRP$       NN   . \n",
      " |    |   |    |     |       |     |     |     |    |   |   |   |    |   |   |         |    |  \n",
      " so   ,   if robots and     body parts really turn you  on  ,  here  '   s  your     movie  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are several errors.\"body\" and \"parts\" should form an NP. \n",
    "# Moreover, \"here's your movie\" should form a clause and \"s\" is in \n",
    "# fact a verb.  \n",
    "err_tree_str = '''(ROOT\n",
    "  (SINV\n",
    "    (ADVP (RB so))\n",
    "    (, ,)\n",
    "    (PP\n",
    "      (IN if)\n",
    "      (NP (NML (NNS robots) (CC and) (NN body)) (NNS parts)))\n",
    "    (ADVP (RB really))\n",
    "    (VP\n",
    "      (VBP turn)\n",
    "      (NP (NP (PRP you)) (PP (IN on) (, ,) (NP (RB here))) ('' '))\n",
    "      (S (NP (POS s))))\n",
    "    (NP (PRP$ your) (NN movie))\n",
    "    (. .)))'''\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"err_tree_str:\")\n",
    "list(Tree.fromstring(err_tree_str))[0].pretty_print()\n",
    "gold_standard_parses.append(Tree.fromstring(corr_tree_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluating parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gold standard, we can use it to evaluate parser output. \n",
    "\n",
    "#### 3.1\n",
    "rubric={accuracy:3,quality:1}\n",
    "\n",
    "Start by writing a function, get_constituents, which takes a parse tree and returns a set of tuples, where each tuple is (*label*, *start*, *end*) where *start* and *end* correspond to the indicies of a corresponding constituent (phrase) in the sentence and *label* is the label of that constituent. \n",
    "\n",
    "Do **not** include simple POS constituents `(POS word)` like `(VBD ate)`. We want to evaluate the parser only on actual phrases.\n",
    "\n",
    "**HINT:** You may want to use recusrion to solve this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituents(tree,start_index=0):\n",
    "    constituents = set()\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    return constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "tree = Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )''')\n",
    "assert get_constituents(tree) == {(\"S\",0,7), (\"NP\",0,2), (\"VP\",2,7),(\"NP\",3,7),(\"NP\",3,5)}\n",
    "tree = Tree.fromstring('''(S (NP (DET the) (NP (NN cat) (CC and) (NN dog))) (VP (VBD fought)))''')\n",
    "assert get_constituents(tree) == {(\"S\",0,5), (\"NP\",0,4), (\"NP\",1,4),(\"VP\",4,5)}\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "rubric={accuracy:2,efficiency:1}\n",
    "\n",
    "Write a function `parse_f1` which uses get_constituents to implement the constituent F-score measure discussed in the lecture and reading. It should be given two lists, a lists of proposed parses and a corresponding list of gold standard parses, and return an F-score reflecting how close the proposed parses match. For full points, you should keep a running count of the relevant numbers over the entire set, and not average f-score across the individual sentences. \n",
    "\n",
    "**Hint:** to get the efficiency point, you should take advantage of Python's fast set operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{precision}  = \\displaystyle\\frac{\\text{relevant constituents} \\cap \\text{retrieved constituents}}{\\text{retrieved constituents}} = \\displaystyle\\frac{\\text{tp}}{\\text{tp + fp}}$\n",
    "\n",
    "$\\text{recall}  = \\displaystyle\\frac{\\text{relevant constituents} \\cap \\text{retrieved constituents}}{\\text{relevant constituents}} = \\displaystyle\\frac{\\text{tp}}{\\text{tp + fn}}$\n",
    "\n",
    "$F_1  = 2 \\cdot \\displaystyle\\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision + recall}}$\n",
    "\n",
    "\n",
    "\n",
    "- `EVALB`: https://nlp.cs.nyu.edu/evalb/\n",
    "- `EVALB_SPMRL`: https://github.com/nikitakit/self-attentive-parser/tree/master/EVALB_SPMRL (from Berkeley Neural Parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_f1(proposed_parses,gold_parses):\n",
    "    f1score = 0\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold\n",
      "      S            \n",
      "  ____|____         \n",
      " |         VP      \n",
      " |     ____|____    \n",
      " NP   |         NP \n",
      " |    |         |   \n",
      "NNS  VBD       NNS \n",
      " |    |         |   \n",
      "mice love     ducks\n",
      "\n",
      "proposed\n",
      "          S        \n",
      "       ___|_____    \n",
      "      NP        VP \n",
      "  ____|___      |   \n",
      "NNS       NN   VBZ \n",
      " |        |     |   \n",
      "mice     love ducks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"gold\")\n",
    "tree = Tree.fromstring('''(ROOT (S (NP (NNS mice)) (VP (VBD love) (NP (NNS ducks)))) )''')\n",
    "list(tree)[0].pretty_print()\n",
    "\n",
    "print(\"proposed\")\n",
    "tree = Tree.fromstring('''(ROOT (S (NP (NNS mice) (NN love)) (VP (VBZ ducks))) )''')\n",
    "list(tree)[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sys:  {('NP', 0, 2), ('VP', 2, 7), ('NP', 3, 7), ('S', 0, 7), ('NP', 3, 5)}\n",
      "gold:  {('NP', 0, 2), ('VP', 2, 7), ('NP', 3, 7), ('S', 0, 7), ('NP', 3, 5)}\n",
      " sys:  {('NP', 0, 2), ('VP', 2, 3), ('S', 0, 3)}\n",
      "gold:  {('NP', 0, 1), ('NP', 2, 3), ('VP', 1, 3), ('S', 0, 3)}\n",
      "tp =  6\n",
      "fp =  2\n",
      "fn =  3\n",
      "precision =  0.75\n",
      "recall =  0.6666666666666666\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "gold_parses = [Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )'''), Tree.fromstring('''(S (NP (NNS mice)) (VP (VBD love) (NP (NNS ducks))))''')]\n",
    "proposed_parses = [Tree.fromstring('''(S (NP (DT the) (DT mouse)) (VP (VBD ate) (NP (NP (DT the) (DT mouse)) (POS 's) (NN cheese))) )'''), Tree.fromstring('''(S (NP (NNS mice) (NN love)) (VP (VBZ ducks)))''')]\n",
    "assert 0.71> parse_f1(proposed_parses,gold_parses) > 0.7\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{precision}  = \\displaystyle\\frac{\\text{relevant constituents} \\cap \\text{retrieved constituents}}{\\text{retrieved constituents}} = \\displaystyle\\frac{6}{6+2}$ where # of retrieved constituents (`len(sys's items)`) = 8\n",
    "\n",
    "$\\text{recall}  = \\displaystyle\\frac{\\text{relevant constituents} \\cap \\text{retrieved constituents}}{\\text{relevant constituents}} = \\displaystyle\\frac{6}{6+3}$ where # of relevant constituents (`len(gold's items)`) = 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3\n",
    "\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Parse your three example sentences from assignment 2 using CoreNLPParser (you can find the sentences in the list `corpus`), extract the constituents and evaluate the result against `gold_standard_parses`. \n",
    "\n",
    "**Hint:** Your F1-score should be > 0.6 (the actual score may depend a bit on your version of the CoreNLP parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser f1-score: 0.68\n"
     ]
    }
   ],
   "source": [
    "parsed_corpus = ...\n",
    "print(\"Parser f1-score: %.2f\" % parse_f1(parsed_corpus, gold_standard_parses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4\n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Given the way we build our gold standard, do you think this is a valid indication of the quality of parsers? Why or why not? What about if we tested the parser on the Penn Treebank corpus instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Generate a grammar\n",
    "\n",
    "#### 4.1\n",
    "rubric= {accuracy:1}\n",
    "\n",
    "Parse trees implicitly contain the production rules for a CFG defined by the productions which are present in the parse tree. You can access these productions using the member function `nltk.Tree.productions()`. \n",
    "\n",
    "Produce a grammar corresponding to the CFG productions in your three sentences from exercise 2, and print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree:\n",
      "      S      \n",
      "  ____|____   \n",
      " NP   VP   | \n",
      " |    |    |  \n",
      "NNS  VBN   . \n",
      " |    |    |  \n",
      "Dogs bark  . \n",
      "\n",
      "rules:\n",
      "S -> NP VP .\n",
      "NP -> NNS\n",
      "VP -> VBN\n",
      ". -> '.'\n",
      "NNS -> 'Dogs'\n",
      "VBN -> 'bark'\n"
     ]
    }
   ],
   "source": [
    "corr_tree_str = '''(S\n",
    "(NP (NNS Dogs)) \n",
    "(VP (VBN bark))\n",
    "(. .))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = set()\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "for rule in rules:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2\n",
    "rubric= {accuracy:1}\n",
    "\n",
    "Show the rules in 4.1 are indeed sufficient to parse the sentences in the list `corpus`. Using an NLTK EarleyChartParser parser for this. Print out the number of parses for each sentence. Don't print out the parses themselves, as there might be a lot of them and you could crash your notebook (this depends a bit on how you fixed the parse trees in exercise 2). You should also set the `trace` keyword argument of the parser to 0 for the same reason. \n",
    "\n",
    "If you have individual sentences which are taking longer than 2 minutes to parse, you can skip over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import EarleyChartParser\n",
    "from nltk.grammar import CFG\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3  Optional\n",
    "rubric= {accuracy:2}\n",
    "\n",
    "Convert your CFG grammar into a feature grammar and implement noun-verb agreement (you should use the feature values `3SG` and `NON3SG`). Make sure that all S, NP and VP rules use agreement features. \n",
    "\n",
    "```\n",
    "S -> NP[NUM=SG] VP[NUM=SG] \".\"\n",
    "NP[NUM=SG] -> DT[NUM=SG] NN\n",
    "DT[NUM=SG] -> 'a'\n",
    "NN -> 'dog'\n",
    "VP[NUM=SG] -> VBP[NUM=SG] \n",
    "VBZ[NUM=SG] -> 'barks'\n",
    "```\n",
    "\n",
    "Give an example sentence, which displays noun-verb agreement. Show that your feature grammar can parse this sentence. Then create a version of the same sentence without proper agreement, and show that the number of parses for this setence is lower (possibly zero). \n",
    "\n",
    "Your grammar shouldn't contain any rules where the LHS contains special characters like `.` or `$`. Otherwise `FeatureGrammar.fromstring` might give an error. This means that you might need to rename some of your non-terminals. \n",
    "\n",
    "```\n",
    "S -> NP VP \".\"          <- NP VP . \n",
    "PRPS -> 'your'          <- PRP$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import FeatureGrammar\n",
    "from nltk.parse import FeatureEarleyChartParser\n",
    "\n",
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
