{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 563 Lab Assignment 1: Named Entity Recognition (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives\n",
    "\n",
    "In this lab, you will be training two models to perform Named Entity Recognition (NER). The primary focus of this lab will be on feature generation. Components of this lab include:\n",
    "\n",
    "1. Read in data and convert data to IOB-labeling\n",
    "2. Define basic features and train a baseline classifier on your data\n",
    "3. Train a Conditional Random Field classifier\n",
    "4. Have your model compete in a class Kaggle competition\n",
    "\n",
    "Note that parts of this lab are based on the [sklearn_crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/) docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to access relevant modules (you can add to this as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn-crfsuite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('gazetteers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import names,gazetteers \n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be working with the Ontonotes V 5.0 dataset, a commonly used dataset for the task. The data is available from the UBC library, but the subset you will be working with is available in the class Github repo in the [data directory](https://github.ubc.ca/mds-cl-2021-22/COLX_563_adv-semantics_students/tree/master/labs/Data/Lab1). You can pull this data from github and change the path below to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "ontonotes_path = \"/Users/MDS2021-2022/COLX_563_adv-sem_labs/Data/Lab1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the code below to get a list of files for the training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ['train/' + filename for filename in os.listdir(ontonotes_path + 'train')]\n",
    "dev_data = ['dev/' + filename for filename in os.listdir(ontonotes_path + 'dev')]\n",
    "\n",
    "print(f\"Read {len(train_data)} training files\")\n",
    "print(f\"Read {len(dev_data)} development files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this Jupyter notebook with your answers embedded\n",
    "- Be sure to follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Initial Data Processing\n",
    "rubric={accuracy:4,quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, you need to convert your data from the .name files to standard IOB/BIO (**I**nside-**O**utside-**B**eginning) tags for NER. Each line of the data file contains a sentence with XML tags indicating the named entities. For example, if the sentence contains a *GPE* tag such as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BIO](BIO.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<ENAMEX TYPE=\"GPE\"> Hong Kong </ENAMEX>\n",
    "\n",
    "-> \n",
    "\n",
    "Hong, Kong\n",
    "B-GPE, I-GPE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag for 'Hong' is *B-GPE* and 'Kong' is *I-GPE* (GPE stands for Geopolitical Entity).  Write a *sentence2iob* function that reads in a sentence from the dataset and converts it to a list of tokens with corresponding IOB-tags.\n",
    "\n",
    "Note that there are a few ways to approach this, and for each of them you will encounter challenges. You will likely end up missing some cases in your first pass, and will need to look for the specific cases which are causing your asserts to fail. \n",
    "\n",
    "**Notes:** \n",
    "\n",
    "* Spaces have been inserted between words and punctuation and you can just use use `split`, no need to tokenize.\n",
    "* ENAMEX tags sometimes contain attributes like `S_OFF=\"1\"` and `E_OFF=\"4\"`. You should ignore these attributes.\n",
    "* There are a number of nested elements in the dataset. You should ignore these.\n",
    "* When the sentence contains no tokens (i.e. it consists entirely of whitespace) or the sentence contains a start of document tag \"<DOC ...\" or end of document tag \"<\\/DOC>\", you should return an empty token and tag list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<DOC DOCNO=\"wb/a2e/00/a2e_0016@0016@a2e@wb@en@on\">          <--- ignore this line\n",
    "<ENAMEX TYPE=\"ORG\">The National Syrian Party</ENAMEX> in <ENAMEX TYPE=\"GPE\">Lebanon</ENAMEX> , and Very Dangerous Facts\n",
    "<ENAMEX TYPE=\"PERSON\">Al Shaheen2005</ENAMEX>\n",
    "At a time when the <ENAMEX TYPE=\"NORP\">Lebanese</ENAMEX> judiciary continues its investigation into ...\n",
    "...\n",
    "That 's not <ENAMEX TYPE=\"ORG\">fun . At <ENAMEX TYPE=\"PERSON\">all</ENAMEX></ENAMEX>     <--- nested;;; \n",
    "                                                                                             extract ORG, ignore PERSON\n",
    "... pursued <ENAMEX TYPE=\"NORP\" S_OFF=\"5\">anti-Serbian</ENAMEX> policies .   <--- remove  ' S_OFF=\"[0-9]*\"' (there will be ' E_OFF=\"[0-9]*\"')\n",
    "...\n",
    "</DOC>                                                      <--- ignore this line\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search, sub\n",
    "    \n",
    "def sentence2iob(sentence):\n",
    "    '''Input sentence is a string from the Ontonotes corpus, with xml tags indicating named entities\n",
    "    output is a list of tokens and a list of NER IOB-tags corresponding to those tokens'''\n",
    "    \n",
    "    tokens=[]\n",
    "    tags=[]\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    # ignore <DOC and </DOC>\n",
    "    # `sub`  S_OFF/E_OFF with ''  (see re.sub)\n",
    "    # make \"ENAMEX TYPE\" as a single token ->  \"ENAMEX_TYPE\",\n",
    "        # so you can have \"<ENAMEX_TYPE=\"GPE\">Moscow</ENAMEX>\" or \"<ENAMEX_TYPE=\"QUANTITY\">2\" as a single token\n",
    "\n",
    "    return tokens, tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ENAMEX_TYPE=\"GPE\">Moscow</ENAMEX>\n",
      ",\n",
      "overcast\n",
      "changing\n",
      "to\n",
      "moderate\n",
      "snow\n",
      ",\n",
      "<ENAMEX_TYPE=\"QUANTITY\">2\n",
      "degrees\n",
      "below\n",
      "zero</ENAMEX>\n",
      "to\n",
      "<ENAMEX_TYPE=\"QUANTITY\">1\n",
      "degree</ENAMEX>\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "check_sentence = '<ENAMEX TYPE=\"GPE\">Moscow</ENAMEX> , overcast changing to moderate snow , <ENAMEX TYPE=\"QUANTITY\">2 degrees below zero</ENAMEX> to <ENAMEX TYPE=\"QUANTITY\">1 degree</ENAMEX> .'\n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "check_sentence = 'While <ENAMEX TYPE=\"PERSON\">Galloway</ENAMEX> \\'s <ENAMEX TYPE=\"ORG\" S_OFF=\"4\">pro-Wal-Mart</ENAMEX> film introduces us to grateful employees /-'\n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)\n",
    "assert curr_tags == ['O', 'B-PERSON', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "check_sentence = '<ENAMEX TYPE=\"GPE\">Moscow</ENAMEX> , overcast changing to moderate snow , <ENAMEX TYPE=\"QUANTITY\">2 degrees below zero</ENAMEX> to <ENAMEX TYPE=\"QUANTITY\">1 degree</ENAMEX> .'\n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)\n",
    "assert curr_tags == ['B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'O', 'B-QUANTITY', 'I-QUANTITY', 'O']\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to build list containing tokenized sentences and tagged sentences for training and development. We will use these lists later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = []\n",
    "dev_sents = []\n",
    "\n",
    "for filenames, sents in [(train_data, train_sents), (dev_data, dev_sents)]: \n",
    "    for filename in filenames:\n",
    "        with open(ontonotes_path + filename, encoding=\"utf-8\") as f:\n",
    "            for sentence in f:\n",
    "                curr_tokens, curr_tags = sentence2iob(sentence)\n",
    "                assert \"\" not in curr_tokens # if you have empty strings, you've done something wrong\n",
    "                sents.append((curr_tokens, curr_tags))\n",
    "\n",
    "train_token_count = sum([len(tokens) for tokens, tags in train_sents])\n",
    "assert train_token_count == 1096878\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ENAMEX TYPE=\"GPE\">Moscow</ENAMEX> , overcast changing to moderate snow , <ENAMEX TYPE=\"QUANTITY\">2 degrees below zero</ENAMEX> to <ENAMEX TYPE=\"QUANTITY\">1 degree</ENAMEX> .\n",
      "['Moscow', ',', 'overcast', 'changing', 'to', 'moderate', 'snow', ',', '2', 'degrees', 'below', 'zero', 'to', '1', 'degree', '.']\n",
      "['B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'O', 'B-QUANTITY', 'I-QUANTITY', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Cheat sheet\n",
    "print(check_sentence) \n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)\n",
    "print(curr_tokens)\n",
    "print(curr_tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Naive Bayes Classification\n",
    "\n",
    "Now we are going to train a simple Naive bayes classifer to perform NER. \n",
    "\n",
    "### Exercise 2.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "The quality of the model depends on utilizing informative features for our task. Modify the *word2features* function to generate features for a specific word in the sentence that will be useful for Named Entity Recognition. You should include at least:\n",
    "\n",
    "- A feature which looks at neighbouring words (Note that this should be one feature in your feature dict, but it will correspond to multiple features in your sparse matrix output from your vectorizer) \n",
    "- A feature which looks at word morphology, for example the last few letters of the word. \n",
    "- A feature which considers the \"shape\" of word (i.e. which letters are upper or lower case). You may want to consider the length or location of the word in the sentence to derive a high performing feature here (what's special about the first word in an English sentence?). \n",
    "- A gazetteer feature (use `names` and/or `gazetteers` from `nltk.corpus`; note that `gazetteers` has multiword expressions like *United States* which won't correspond to individual tokens)\n",
    "\n",
    "\n",
    "You will use this same function in your CRF, and you may need to come back here later and improve your set of features to increase your performance in the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my code here\n",
    "import nltk\n",
    "\n",
    "names_gazetteer = set(names.words())\n",
    "location_gazetteer = set()\n",
    "for location in gazetteers.words():\n",
    "    words = location.split()\n",
    "    for word in words:\n",
    "        if word[0].isupper() and len(word) > 3:\n",
    "            location_gazetteer.add(word)\n",
    "# my code here\n",
    "\n",
    "def word2features(sentence, idx):\n",
    "    word_features = {}\n",
    "    word_features['word_lowercase'] = sentence[idx].lower()\n",
    "\n",
    "    # your code here\n",
    "    # word_features['my_features'] = ...\n",
    "    # word_features['prev_word'] = \n",
    "    # word_features['next_word'] = \n",
    "    # word_features['1-suffix'] = \n",
    "    # word_features['2-suffix'] = \n",
    "    # word_features['is_names_gazetteer'] = \n",
    "    # word_features['is_location_gazetteer'] = \n",
    "\n",
    "    return word_features\n",
    "    \n",
    "def sentence2features(sentence):\n",
    "    return [word2features(sentence, idx) for idx in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Cheat sheet\n",
    "print(\"Canada\" in location_gazetteer)\n",
    "print(\"canada\" in location_gazetteer)\n",
    "print(\"United States\" in location_gazetteer)\n",
    "print(\"United\" in location_gazetteer)\n",
    "print(\"States\" in location_gazetteer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Write a function `prepare_ner_feature_dicts` which takes `train_sents` or `dev_sents` which we prepared above and runs `sentence2features` on the tokenized sentences. You should return two lists. One containing the feature dictionaries for every sentence in the dataset and another one containing all tags. Note that these should should be plain lists of dictionaries and tags (not lists of lists). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_feature_dicts(sents):\n",
    "    '''ner_files is a list of Ontonotes files with NER annotations. Returns feature dictionaries and \n",
    "    IOB tags for each token in the entire dataset'''\n",
    "    all_dicts = []\n",
    "    all_tags = []\n",
    "    # your code here\n",
    "\n",
    "    return all_dicts, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "train_dicts, train_tags = prepare_ner_feature_dicts(train_sents)\n",
    "dev_dicts, dev_tags = prepare_ner_feature_dicts(dev_sents)\n",
    "\n",
    "assert(len(train_dicts)) == 1096878\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing purpose: ['G.', 'William', 'Ryan', ',', 'president', 'of', 'Post-Newsweek', 'Stations', ',']\n",
      "[{'word_lowercase': 'g.', 'all_caps': True, 'title_case_not_first_word': False, 'prev_word': '*None*', 'next_word': 'William', 'last_2_letters': 'G.', 'location_gazetteer': False, 'name_gazetteer': False, 'has_number': False}, {'word_lowercase': 'william', 'all_caps': False, 'title_case_not_first_word': True, 'prev_word': 'G.', 'next_word': 'Ryan', 'last_2_letters': 'Wi', 'location_gazetteer': False, 'name_gazetteer': True, 'has_number': False}]\n",
      "['B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Cheat sheet \n",
    "print(\"printing purpose:\", train_toks[:9]) # NOT EXISTS...\n",
    "print(train_dicts[:2])\n",
    "print(train_tags[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "rubric={accuracy:2,reasoning:1}\n",
    "\n",
    "Now use your features to train a Multinomial Naive Bayes classifer on `train_dicts` and `train_tags`, with default settings. You will need to vectorize `train_tokens` first using `DictVectorizer` from sklearn. Evaluate the model on the `dev_dicts` comparing system generated tags to `dev_tags`. \n",
    "\n",
    "Using [sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), print out a macroaveraged f-score, a microaveraged f-score, and a classification report, so you can see how you're doing with each class. Note you will get very divergent scores for microaverage and macroaveraged f-score. Briefly explain why, with reference to the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MicroF1: 0.9324081364058332\n",
      "MacroF1: 0.34002264404986104\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "print(\"MicroF1:\",f1_score(dev_tags, y_pred,average=\"micro\"))\n",
    "print(\"MacroF1:\",f1_score(dev_tags, y_pred,average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jungyeul/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jungyeul/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-CARDINAL       0.62      0.55      0.58      1216\n",
      "       B-DATE       0.74      0.61      0.67      2230\n",
      "      B-EVENT       1.00      0.02      0.03       130\n",
      "        B-FAC       0.00      0.00      0.00       149\n",
      "        B-GPE       0.73      0.94      0.82      2738\n",
      "   B-LANGUAGE       0.00      0.00      0.00       114\n",
      "        B-LAW       0.00      0.00      0.00        47\n",
      "        B-LOC       1.00      0.01      0.03       231\n",
      "      B-MONEY       0.71      0.52      0.60       712\n",
      "       B-NORP       0.82      0.81      0.81       928\n",
      "    B-ORDINAL       0.79      0.14      0.24       222\n",
      "        B-ORG       0.71      0.55      0.62      3024\n",
      "    B-PERCENT       0.75      0.78      0.76       574\n",
      "     B-PERSON       0.74      0.90      0.81      2082\n",
      "    B-PRODUCT       1.00      0.01      0.02       101\n",
      "   B-QUANTITY       0.00      0.00      0.00       125\n",
      "       B-TIME       1.00      0.01      0.03       203\n",
      "B-WORK_OF_ART       0.00      0.00      0.00        67\n",
      "   I-CARDINAL       0.67      0.01      0.03       428\n",
      "       I-DATE       0.73      0.69      0.71      2615\n",
      "      I-EVENT       0.78      0.02      0.04       307\n",
      "        I-FAC       0.00      0.00      0.00       254\n",
      "        I-GPE       0.83      0.55      0.66       689\n",
      "   I-LANGUAGE       0.00      0.00      0.00         9\n",
      "        I-LAW       0.00      0.00      0.00       157\n",
      "        I-LOC       1.00      0.09      0.16       222\n",
      "      I-MONEY       0.84      0.87      0.85      1464\n",
      "       I-NORP       0.00      0.00      0.00        53\n",
      "        I-ORG       0.67      0.81      0.73      4146\n",
      "    I-PERCENT       0.99      0.76      0.86       801\n",
      "     I-PERSON       0.83      0.67      0.74      1430\n",
      "    I-PRODUCT       1.00      0.03      0.06        91\n",
      "   I-QUANTITY       1.00      0.12      0.22       216\n",
      "       I-TIME       0.96      0.08      0.15       263\n",
      "I-WORK_OF_ART       1.00      0.01      0.02       165\n",
      "            O       0.97      0.99      0.98    141995\n",
      "\n",
      "     accuracy                           0.93    170198\n",
      "    macro avg       0.63      0.32      0.34    170198\n",
      " weighted avg       0.93      0.93      0.92    170198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jungyeul/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_tags, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "rubric={accuracy:1}\n",
    "\n",
    "One problem with using a regular (non-sequential) classifier for `IOB-based NER` is that it may create ill-formed named entities, i.e. `I-` tags with no corresponding `B-` or `I-` tags before it. Check how often this is happening in the dev set with your classifier (the answer is \"a lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3493 : the number can vary...\n"
     ]
    }
   ],
   "source": [
    "# Cheat sheet\n",
    "for iterate `y_pred``:\n",
    "    if compare the current tag and previous tag, then if not matched:\n",
    "        print(y_pred[i-1], y_pred[i])\n",
    "\n",
    "\n",
    "# the number can vary (depending on your features)\n",
    "# There are 163,282 I-tags in total\n",
    "# There are 6915 broken I-tags\n",
    "# 4.24% of all I-tags are broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Training a CRF\n",
    "\n",
    "Next, you're going to train a CRF model using the [`sklearn_crfsuite`](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "First, do the appropriate modification to `prepare_ner_feature_dicts` to put the data in the right format. The only difference is that you are now building lists of lists of feature dicts, with the \"extra\" lists corresponding to sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_feature_dicts(sents):\n",
    "    '''ner_files is a list of Ontonotes files with NER annotations. Returns feature dictionaries and \n",
    "    IOB tags for each token in the entire dataset'''\n",
    "    all_dicts = []\n",
    "    all_tags = []\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    return all_dicts, all_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2:\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Now train and evalute your model in the same way as you did with the Naive Bayes model. Note that this will take a lot longer to train than the naive Bayes, you might want to set the `max_iterations` parameter low to start (but you will need to set it fairly high to get good results). If you want to see the progress of training, use verbose=True, it will help your sanity and give you a sense of how many iterations you need.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "1. [`sklearn_crfsuite`](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html) does not require a DictVectorizer, you can input features and tags directly into the model.\n",
    "1. You may get an error `AttributeError: 'CRF' object has no attribute 'keep_tempfiles'` when training the CRF model because of an incompatible sklearn version. This is not fatal. Just use a `try ... except` clause which catches the error and pass it (note that this only works because the error is raised after the model parameters have been set):\n",
    "\n",
    "```\n",
    "    try:\n",
    "        call_produces_an_error()\n",
    "    except:\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████████████████████████████████████████████████████████████████████████| 57447/57447 [00:08<00:00, 6559.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 186119\n",
      "Seconds required: 2.704\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=10.94 loss=1204633.74 active=186119 feature_norm=5.00\n",
      "Iter 2   time=3.69  loss=1017711.04 active=186119 feature_norm=4.11\n",
      "Iter 3   time=3.63  loss=991387.84 active=186119 feature_norm=4.03\n",
      "Iter 4   time=3.83  loss=871534.63 active=186119 feature_norm=4.80\n",
      "Iter 5   time=3.64  loss=729903.23 active=186119 feature_norm=7.10\n",
      "Iter 6   time=3.64  loss=627632.90 active=186119 feature_norm=10.10\n",
      "Iter 7   time=3.71  loss=518951.38 active=186119 feature_norm=14.17\n",
      "Iter 8   time=7.21  loss=464748.65 active=186119 feature_norm=16.87\n",
      "Iter 9   time=3.93  loss=419471.75 active=186119 feature_norm=19.21\n",
      "Iter 10  time=3.59  loss=367431.42 active=186119 feature_norm=23.81\n",
      "Iter 11  time=3.60  loss=343781.30 active=186119 feature_norm=24.50\n",
      "Iter 12  time=4.00  loss=336111.96 active=186119 feature_norm=24.79\n",
      "Iter 13  time=3.59  loss=319224.48 active=186119 feature_norm=25.79\n",
      "Iter 14  time=3.74  loss=304725.45 active=186119 feature_norm=27.02\n",
      "Iter 15  time=3.63  loss=280007.02 active=186119 feature_norm=29.64\n",
      "Iter 16  time=7.06  loss=265731.22 active=186119 feature_norm=33.31\n",
      "Iter 17  time=3.50  loss=247500.82 active=186119 feature_norm=37.63\n",
      "Iter 18  time=3.63  loss=236680.95 active=186119 feature_norm=38.04\n",
      "Iter 19  time=3.99  loss=229735.04 active=186119 feature_norm=41.58\n",
      "Iter 20  time=3.58  loss=223745.97 active=186119 feature_norm=43.04\n",
      "Iter 21  time=3.51  loss=217812.64 active=186119 feature_norm=44.42\n",
      "Iter 22  time=3.53  loss=204308.75 active=186119 feature_norm=48.90\n",
      "Iter 23  time=3.51  loss=190510.94 active=186119 feature_norm=53.42\n",
      "Iter 24  time=3.51  loss=177914.63 active=186119 feature_norm=61.59\n",
      "Iter 25  time=3.51  loss=169757.20 active=186119 feature_norm=62.39\n",
      "Iter 26  time=3.59  loss=163318.84 active=186119 feature_norm=62.39\n",
      "Iter 27  time=7.32  loss=161406.58 active=186119 feature_norm=63.12\n",
      "Iter 28  time=3.51  loss=158862.22 active=186119 feature_norm=64.26\n",
      "Iter 29  time=3.51  loss=155475.88 active=186119 feature_norm=65.03\n",
      "Iter 30  time=3.50  loss=151056.73 active=186119 feature_norm=68.52\n",
      "Iter 31  time=3.52  loss=146812.76 active=186119 feature_norm=70.48\n",
      "Iter 32  time=3.52  loss=143803.56 active=186119 feature_norm=72.30\n",
      "Iter 33  time=3.52  loss=138917.17 active=186119 feature_norm=76.03\n",
      "Iter 34  time=7.02  loss=137021.19 active=186119 feature_norm=76.98\n",
      "Iter 35  time=3.49  loss=134644.31 active=186119 feature_norm=78.65\n",
      "Iter 36  time=3.50  loss=133305.04 active=186119 feature_norm=79.42\n",
      "Iter 37  time=3.51  loss=130403.77 active=186119 feature_norm=81.28\n",
      "Iter 38  time=3.51  loss=126917.76 active=186119 feature_norm=82.66\n",
      "Iter 39  time=3.71  loss=125270.05 active=186119 feature_norm=84.99\n",
      "Iter 40  time=3.67  loss=122678.94 active=186119 feature_norm=84.32\n",
      "Iter 41  time=4.08  loss=121436.59 active=186119 feature_norm=84.54\n",
      "Iter 42  time=3.94  loss=119887.30 active=186119 feature_norm=85.36\n",
      "Iter 43  time=7.15  loss=118747.37 active=186119 feature_norm=86.23\n",
      "Iter 44  time=3.55  loss=117763.37 active=186119 feature_norm=87.10\n",
      "Iter 45  time=3.55  loss=116459.42 active=186119 feature_norm=88.25\n",
      "Iter 46  time=3.57  loss=114850.00 active=186119 feature_norm=89.70\n",
      "Iter 47  time=3.58  loss=112537.30 active=186119 feature_norm=92.03\n",
      "Iter 48  time=7.26  loss=111410.86 active=186119 feature_norm=93.58\n",
      "Iter 49  time=3.59  loss=110504.55 active=186119 feature_norm=93.67\n",
      "Iter 50  time=3.58  loss=109496.82 active=186119 feature_norm=93.84\n",
      "Iter 51  time=3.56  loss=108657.22 active=186119 feature_norm=95.90\n",
      "Iter 52  time=3.56  loss=107028.66 active=186119 feature_norm=96.56\n",
      "Iter 53  time=3.56  loss=106465.03 active=186119 feature_norm=96.99\n",
      "Iter 54  time=3.61  loss=105758.59 active=186119 feature_norm=97.94\n",
      "Iter 55  time=3.75  loss=104656.75 active=186119 feature_norm=99.18\n",
      "Iter 56  time=3.71  loss=103069.52 active=186119 feature_norm=102.34\n",
      "Iter 57  time=3.70  loss=102582.27 active=186119 feature_norm=104.33\n",
      "Iter 58  time=3.68  loss=100892.16 active=186119 feature_norm=104.63\n",
      "Iter 59  time=3.79  loss=100296.24 active=186119 feature_norm=104.38\n",
      "Iter 60  time=3.68  loss=99902.98 active=186119 feature_norm=104.55\n",
      "Iter 61  time=3.73  loss=99495.17 active=186119 feature_norm=105.02\n",
      "Iter 62  time=3.73  loss=97925.90 active=186119 feature_norm=107.12\n",
      "Iter 63  time=3.71  loss=96936.45 active=186119 feature_norm=109.64\n",
      "Iter 64  time=3.71  loss=96355.83 active=186119 feature_norm=110.23\n",
      "Iter 65  time=3.73  loss=95473.92 active=186119 feature_norm=111.82\n",
      "Iter 66  time=7.34  loss=95244.26 active=186119 feature_norm=112.47\n",
      "Iter 67  time=3.78  loss=94839.36 active=186119 feature_norm=113.59\n",
      "Iter 68  time=3.67  loss=94424.62 active=186119 feature_norm=114.60\n",
      "Iter 69  time=3.69  loss=93782.22 active=186119 feature_norm=115.98\n",
      "Iter 70  time=3.67  loss=92985.01 active=186119 feature_norm=118.47\n",
      "Iter 71  time=3.66  loss=92191.60 active=186119 feature_norm=120.73\n",
      "Iter 72  time=3.68  loss=91787.61 active=186119 feature_norm=120.28\n",
      "Iter 73  time=3.66  loss=91337.00 active=186119 feature_norm=120.80\n",
      "Iter 74  time=3.82  loss=90966.81 active=186119 feature_norm=121.34\n",
      "Iter 75  time=3.86  loss=90687.50 active=186119 feature_norm=122.14\n",
      "Iter 76  time=3.62  loss=89890.00 active=186119 feature_norm=124.62\n",
      "Iter 77  time=3.58  loss=89261.88 active=186119 feature_norm=125.57\n",
      "Iter 78  time=7.22  loss=88922.89 active=186119 feature_norm=125.98\n",
      "Iter 79  time=3.73  loss=88518.80 active=186119 feature_norm=125.71\n",
      "Iter 80  time=3.66  loss=88281.73 active=186119 feature_norm=125.19\n",
      "Iter 81  time=3.94  loss=87961.37 active=186119 feature_norm=125.55\n",
      "Iter 82  time=3.72  loss=87512.18 active=186119 feature_norm=126.56\n",
      "Iter 83  time=3.65  loss=87108.36 active=186119 feature_norm=128.33\n",
      "Iter 84  time=3.65  loss=86624.79 active=186119 feature_norm=130.45\n",
      "Iter 85  time=3.71  loss=86346.29 active=186119 feature_norm=131.76\n",
      "Iter 86  time=7.50  loss=86142.24 active=186119 feature_norm=132.47\n",
      "Iter 87  time=3.66  loss=85844.28 active=186119 feature_norm=133.00\n",
      "Iter 88  time=3.67  loss=85514.75 active=186119 feature_norm=133.16\n",
      "Iter 89  time=3.68  loss=85287.93 active=186119 feature_norm=133.05\n",
      "Iter 90  time=4.41  loss=85159.60 active=186119 feature_norm=134.51\n",
      "Iter 91  time=3.69  loss=84803.27 active=186119 feature_norm=134.44\n",
      "Iter 92  time=3.70  loss=84728.70 active=186119 feature_norm=134.60\n",
      "Iter 93  time=3.66  loss=84616.39 active=186119 feature_norm=135.02\n",
      "Iter 94  time=3.65  loss=84441.35 active=186119 feature_norm=135.52\n",
      "Iter 95  time=4.28  loss=83964.87 active=186119 feature_norm=136.80\n",
      "Iter 96  time=8.31  loss=83761.86 active=186119 feature_norm=137.93\n",
      "Iter 97  time=4.32  loss=83499.94 active=186119 feature_norm=138.40\n",
      "Iter 98  time=4.18  loss=83276.76 active=186119 feature_norm=138.73\n",
      "Iter 99  time=4.27  loss=83136.77 active=186119 feature_norm=138.90\n",
      "Iter 100 time=8.31  loss=83031.69 active=186119 feature_norm=139.20\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 417.752\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 186119 (186119)\n",
      "Number of active attributes: 125189 (125189)\n",
      "Number of active labels: 37 (37)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microaveraged fscore 0.9637951092257253\n",
      "Macroaveraged fscore 0.6816350011552526\n"
     ]
    }
   ],
   "source": [
    "print('Microaveraged fscore', flat_f1_score(dev_tags, dev_pred, average='micro'))\n",
    "print('Macroaveraged fscore', flat_f1_score(dev_tags, dev_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "y_pred = crf.predict(dev_dicts)\n",
    "print(metrics.flat_f1_score(dev_tags, y_pred, average='weighted', labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheat sheet\n",
    "# for some reason `flat_classification_report()` works only with the previous version of sklearn...\n",
    "from sklearn import metrics\n",
    "from sklearn_crfsuite.utils import flatten\n",
    "\n",
    "print(metrics.classification_report(flatten(dev_tags), flatten(y_pred)))\n",
    "\n",
    "# import itertools\n",
    "# print(classification_report(list(itertools.chain(*dev_tags)), list(itertools.chain(*dev_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOMMEND\n",
    "# !python3 -m pip install conlleval\n",
    "\n",
    "from conlleval import evaluate # https://github.com/sighsmile/conlleval\n",
    "\n",
    "\n",
    "evaluate(flatten(dev_tags), flatten(y_pred), verbose=True) \n",
    "\n",
    "# processed 170198 tokens with 14893 phrases; found: 14463 phrases; correct: 12367.\n",
    "# accuracy:  85.43%; (non-O)\n",
    "# accuracy:  96.77%; precision:  85.51%; recall:  83.04%; FB1:  84.26\n",
    "#          CARDINAL: precision:  82.59%; recall:  83.88%; FB1:  83.23  1235\n",
    "#              DATE: precision:  85.23%; recall:  85.11%; FB1:  85.17  2227\n",
    "#             EVENT: precision:  57.33%; recall:  33.08%; FB1:  41.95  75\n",
    "#               FAC: precision:  49.43%; recall:  28.86%; FB1:  36.44  87\n",
    "#               GPE: precision:  91.45%; recall:  92.99%; FB1:  92.21  2784\n",
    "#          LANGUAGE: precision:  87.69%; recall:  50.00%; FB1:  63.69  65\n",
    "#               LAW: precision:  51.85%; recall:  29.79%; FB1:  37.84  27\n",
    "#               LOC: precision:  69.15%; recall:  60.17%; FB1:  64.35  201\n",
    "#             MONEY: precision:  92.40%; recall:  90.45%; FB1:  91.41  697\n",
    "#              NORP: precision:  87.71%; recall:  91.49%; FB1:  89.56  968\n",
    "#           ORDINAL: precision:  78.39%; recall:  83.33%; FB1:  80.79  236\n",
    "#               ORG: precision:  82.47%; recall:  78.24%; FB1:  80.30  2869\n",
    "#           PERCENT: precision:  90.37%; recall:  89.90%; FB1:  90.13  571\n",
    "#            PERSON: precision:  88.45%; recall:  87.94%; FB1:  88.20  2070\n",
    "#           PRODUCT: precision:  50.00%; recall:  27.72%; FB1:  35.67  56\n",
    "#          QUANTITY: precision:  82.61%; recall:  60.80%; FB1:  70.05  92\n",
    "#              TIME: precision:  63.45%; recall:  45.32%; FB1:  52.87  145\n",
    "#       WORK_OF_ART: precision:  34.48%; recall:  29.85%; FB1:  32.00  58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.3:\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "Look at the top and bottom 10 transitions in terms of weight (**Hint**: [`sklearn_crfsuite` tutorial](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned)). Do they make sense?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10: [(('O', 'B-CARDINAL'), 4.573875), (('I-MONEY', 'I-MONEY'), 4.620261), (('I-PERSON', 'I-PERSON'), 4.666901), (('B-PERCENT', 'I-PERCENT'), 4.716632), (('B-ORG', 'I-ORG'), 4.990618), (('I-ORG', 'I-ORG'), 5.134131), (('I-DATE', 'I-DATE'), 5.385755), (('B-PERSON', 'I-PERSON'), 5.629678), (('O', 'O'), 6.695223), (('B-DATE', 'I-DATE'), 6.768138)]\n",
      "Bottom 10: [(('B-PERCENT', 'O'), -1.28613), (('B-PERSON', 'B-PERSON'), -1.177372), (('I-ORG', 'B-PERSON'), -1.174839), (('B-PERSON', 'B-ORG'), -0.984528), (('I-ORG', 'B-GPE'), -0.984249), (('I-ORG', 'B-ORG'), -0.967117), (('I-ORDINAL', 'O'), -0.963708), (('I-LANGUAGE', 'O'), -0.96031), (('B-GPE', 'B-GPE'), -0.956907), (('B-GPE', 'B-PERSON'), -0.937251)]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# put `crf.transition_features_`  in `Counter` and find `most_common`\n",
    "# `crf.transition_features_` has (label_from, label_to), weight:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
