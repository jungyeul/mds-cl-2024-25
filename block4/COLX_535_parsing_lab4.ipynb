{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 535 Lab Assignment 4: Dependency Parsing (Cheat sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment objectives\n",
    "\n",
    "In this assignment you will:\n",
    "- Work with dependency parses\n",
    "- Solidify your understanding of shift-reduce parsing algorithm\n",
    "- Create a statistical scoring function for shift-reduce parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing algorithms:\n",
    "\n",
    "- transition-based\n",
    "    - Arc-eager by {nivre:2003}\n",
    "    - **Arc-standard by {yamada-matsumoto:2003}**    **<-- What we do!**\n",
    "        * MaltParser {nivre-hall-nilsson:2006:LREC}\n",
    "        * UDPipe {straka-hajic-strakova:2016:LREC,straka-strakova:2017:CoNLL}\n",
    "        * a \"first\" neural dependency parser {chen-manning:2014:EMNLP} $\\rightarrow$ google's dependency parser {andor-EtAl:2016:ACL}\n",
    "\n",
    "- graph-based based on the Edmond algorithm {edmonds:1967,tarjan:1972}\n",
    "    - MST parser {mcdonald-crammer-pereira:2005:ACL}\n",
    "    - Biaffine parser {dozat-manning:2017:ICLR} $\\rightarrow$ stanza: stanford CoreNLP in python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cat](cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"dependency_treebank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import dependency_treebank\n",
    "from nltk.parse import DependencyGraph\n",
    "from collections import defaultdict\n",
    "from nltk.tree import Tree\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(will\n",
      "  (Vinken Pierre , (old (years 61)) ,)\n",
      "  (join (board the) (as (director a nonexecutive)) (Nov. 29))\n",
      "  .)\n"
     ]
    }
   ],
   "source": [
    "# nltk dependency_treebank (older version of dependency structure)\n",
    "print(dependency_treebank.parsed_sents()[0].tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(will\n",
      "  (Vinken Pierre , (old (years 61)) ,)\n",
      "  (join (board the) (as (director a nonexecutive)) (Nov. 29))\n",
      "  .)\n"
     ]
    }
   ],
   "source": [
    "# current dependency structure (since 2013)\n",
    "'''\n",
    "McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., \n",
    "Das, D., Ganchev, K., Hall, K., Petrov, S., Zhang, H., Täckström, O., \n",
    "Bedini, C., Bertomeu Castelló, N., & Lee, J. (2013). \n",
    "Universal Dependency Annotation for Multilingual Parsing. \n",
    "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), \n",
    "p.92–97. http://www.aclweb.org/anthology/P13-2017\n",
    "'''\n",
    "print(dependency_treebank.parsed_sents()[0].tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UD = Universal Dependencies: http://universaldependencies.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Manual shift-reduce dependency parsing\n",
    "\n",
    "#### 1.1\n",
    "\n",
    "rubric={raw:2}\n",
    "\n",
    "Create a dependency parse for the following sentence, in UD format:\n",
    "\n",
    "***I invited the queen of England for tea***\n",
    "\n",
    "You can find a description of the UD dependency parse format [here](https://universaldependencies.org/format.html), a description of available UD POS tags [here](https://universaldependencies.org/u/pos/index.html) and a description of UD dependency relations [here](https://universaldependencies.org/u/dep/index.html).\n",
    "\n",
    "Your tree should contain four columns: (1) word form, (2) UD POS tag, (3) head ID and (4) UD dependency relation.\n",
    "\n",
    "Please format your tree neatly so that it's readable and store it in a string `tree`.\n",
    "\n",
    "**Note! Both the UD POS tag and dependency relation need to be written using uppercase letters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "tree='''\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Read the parse in as an `nltk.parse.DependencyGraph` (https://www.nltk.org/api/nltk.parse.dependencygraph.html) and print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(invited I (queen the (England of)) (tea for))\n"
     ]
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3\n",
    "rubric={raw:3}\n",
    "\n",
    "Provide the steps for a shift-reduce parse of the sentence, including the *stack*, the *buffer*, and the *action* taken at each step."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "BUFFER                                      STACK                                    ACTION\n",
    "-------------------------------------------------------------------------------------------\n",
    "I invited the queen of England for tea $    $ ROOT                                   SHIFT\n",
    "invited the queen of England for tea $      $ ROOT I                                 SHIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Arc standard](arc-standard.png)\n",
    "\n",
    "\n",
    "LEFT-ARC:\n",
    "- before Stack **(ROOT I invited)**: I$_i$ $\\leftarrow$ invited$_j$ **(POP $i$)**\n",
    "- after  Stack **(ROOT invited)**: \n",
    "\n",
    "\n",
    "RIGHT-ARC:\n",
    "- before Stack **(ROOT invited queen England)**: queen$_i$ $\\rightarrow$ English$_j$ **(POP $j$)**\n",
    "- after  Stack **(ROOT invited queen)**: \n",
    "\n",
    "\n",
    "![Queen](queen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: From parse to shift-reduce steps\n",
    "\n",
    "When we train parsers on gold standard manually parsed sentences, we actually don't train the parser directly on the dependency graphs. Instead, we first convert the graphs into shift-reduce steps which consist of three objects: a stack configuration, a buffer configuration and a parse action. \n",
    "\n",
    "<!-- Here are a few examples of shift-reduce steps:\n",
    "```\n",
    "[[(\"cheese\", 3)], [(\"ROOT\", 0), (\"eat\", 1), (\"the\", 2)], \"SHIFT\"] [[], [(\"ROOT\", 0), (\"eat\", 1), (\"the\", 2), (\"cheese\", 3)], \"LEFTARC\"] [[], [(\"ROOT\", 0), (\"eat\", 1), (\"cheese\", 3)], \"RIGHTARC\"]\n",
    "```\n",
    " -->\n",
    "The first list is the current *buffer*, the second is the current *stack*, and the third element of the list is the *action* that was taken when in this configuration. \n",
    "\n",
    "#### 2.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Start by writing a function `get_dependency_lookup`, which converts an NLTK dependency graphs into a more convenient lookup table. For each token in the sentence, the table contains the set of its dependents. For example, for the graph:\n",
    "```\n",
    "I     PRON   2   NSUBJ\n",
    "saw   VERB   0   ROOT\n",
    "the   DET    4   DET\n",
    "cat   NOUN   2   OBJ\n",
    "```\n",
    "we want to return a dictionary:\n",
    "```\n",
    "{(\"ROOT\", 0): {(\"saw\", 2)},                                     *ROOT -> saw\n",
    " (\"I\", 1): {},         \n",
    " (\"saw\", 2): {(\"I\", 1), (\"cat\", 4)},                            I <-- *saw, *saw -> cat\n",
    " (\"the\", 3): {},\n",
    " (\"cat\", 4): {(\"the\", 3)}}                                      the <-- *cat\n",
    "```\n",
    "where each pair consists of a token and it's line number in the dependency graph. \n",
    "\n",
    "**Note**, we include the `\"ROOT\"` token in the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_lookup(dependency_graph):\n",
    "    dependency_lookup = {}\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    return dependency_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph='''I   PRON   2   NSUBJ\n",
    "saw   VERB   0   ROOT\n",
    "the   DET    4   DET\n",
    "cat   NOUN   2   OBJ'''\n",
    "\n",
    "dep_graph = DependencyGraph(graph)\n",
    "assert get_dependency_lookup(dep_graph) == {(\"ROOT\", 0): {(\"saw\", 2)}, (\"I\", 1): set(), (\"saw\", 2): {(\"I\", 1), (\"cat\", 4)}, (\"the\", 3): set(), (\"cat\", 4): {(\"the\", 3)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "rubric={accuracy:1}\n",
    "\n",
    "You should now write a function `get_buffer` which converts a dependency graph into a sentence buffer. Given the graph:\n",
    "```\n",
    "I     PRON   2   NSUBJ\n",
    "saw   VERB   0   ROOT\n",
    "the   DET    4   DET\n",
    "cat   NOUN   2   OBJ\n",
    "```\n",
    "the `get_buffer` should return:\n",
    "```\n",
    "[(\"I\", 1), (\"saw\", 2), (\"the\", 3), (\"cat\", 4)]\n",
    "```\n",
    "**Note**, we don't include `\"ROOT\"` in the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given... \n",
    "def get_buffer(dependency_graph):\n",
    "     return [(dependency_graph.nodes[i][\"word\"], i) for i in range(1, len(dependency_graph.nodes))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph='''I   PRON   2   NSUBJ\n",
    "saw   VERB   0   ROOT\n",
    "the   DET    4   DET\n",
    "cat   NOUN   2   OBJ'''\n",
    "\n",
    "dep_graph = DependencyGraph(graph)\n",
    "assert get_buffer(dep_graph) == [(\"I\", 1), (\"saw\", 2), (\"the\", 3), (\"cat\", 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "rubric={accuracy:3}\n",
    "\n",
    "We will now start implementing the algorithm which converts a dependency graph into a sequence of shift-reduce actions. Your first task is to implement three functions for parse actions: `shift`, `left_arc` and `right_arc`.\n",
    "\n",
    "Each function should: \n",
    "\n",
    "1. Create a step for the action taken and append it to `steps`. Each step is a list consisting of three elements: `(b, s, op)`, where `b` should be a copy of the parameter `buffer`, `s` a copy the parameter `stack` and `op` a shift-reduce operation (`SHIFT`, `LEFTARC` or `RIGHTARC`, respectively).\n",
    "1. Make the appropriate changes to the `buffer` and/or the `stack` depending on the parse action. For example, in the `shift` function, the first word on `buffer` should be appended to the end of `stack`. \n",
    "1. Finally, if a word is removed from the stack (by `left_arc` or `right_arc`), it should be added to the set `done`. \n",
    "\n",
    "**Note 1**: There is no need to return anything, since you are changing the relevant data structures directly.\n",
    "\n",
    "**Note 2**: It's very important to copy `buffer` and `stack` before appending to `steps`. Otherwise the values keep changing in future calls to `shift`, `left_arc` and `right_arc`. A simple way to copy a list is to:\n",
    "```\n",
    "buffer_copy = buffer[:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(buffer, stack, steps, done):\n",
    "    # your code here  \n",
    "\n",
    "def left_arc(buffer, stack, steps, done):\n",
    "    # your code here\n",
    "    \n",
    "def right_arc(buffer, stack, steps, done):\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check that your code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "buffer = [\"John\", \"eats\", \"an\", \"apple\"]\n",
    "stack = []\n",
    "steps = []\n",
    "done = set()\n",
    "\n",
    "shift(buffer, stack, steps, done)\n",
    "shift(buffer, stack, steps, done)\n",
    "left_arc(buffer, stack, steps, done)\n",
    "shift(buffer, stack, steps, done)\n",
    "right_arc(buffer, stack, steps, done)\n",
    "shift(buffer, stack, steps, done)\n",
    "left_arc(buffer, stack, steps, done)\n",
    "\n",
    "\n",
    "assert(steps == [[['John', 'eats', 'an', 'apple'], [], 'SHIFT'],\n",
    "                 [['eats', 'an', 'apple'], ['John'], 'SHIFT'],\n",
    "                 [['an', 'apple'], ['John', 'eats'], 'LEFTARC'],\n",
    "                 [['an', 'apple'], ['eats'], 'SHIFT'],\n",
    "                 [['apple'], ['eats', 'an'], 'RIGHTARC'],\n",
    "                 [['apple'], ['eats'], 'SHIFT'],\n",
    "                 [[], ['eats', 'apple'], 'LEFTARC']])\n",
    "assert(not buffer)\n",
    "assert(len(stack) == 1)\n",
    "assert(len(done) == 3)\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "\n",
    "# correct order: \n",
    "# shift(buffer, stack, steps, done)\n",
    "# shift(buffer, stack, steps, done)\n",
    "# left_arc(buffer, stack, steps, done)\n",
    "# shift(buffer, stack, steps, done)\n",
    "# shift(buffer, stack, steps, done)\n",
    "# left_arc(buffer, stack, steps, done)\n",
    "# right_arc(buffer, stack, steps, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT\n",
      "STEPS :  [['John', 'eats', 'an', 'apple'], [], 'SHIFT']\n",
      "BUFFER:  ['eats', 'an', 'apple']\n",
      "STACK :  ['John']\n",
      "DONE. :  set()\n",
      "-----\n",
      "\n",
      "SHIFT\n",
      "STEPS :  [['eats', 'an', 'apple'], ['John'], 'SHIFT']\n",
      "BUFFER:  ['an', 'apple']\n",
      "STACK :  ['John', 'eats']\n",
      "DONE. :  set()\n",
      "-----\n",
      "\n",
      "LEFT-ARC\n",
      "STEPS :  [['an', 'apple'], ['John', 'eats'], 'LEFTARC']\n",
      "BUFFER:  ['an', 'apple']\n",
      "STACK :  ['eats']\n",
      "DONE. :  {'John'}\n",
      "-----\n",
      "\n",
      "SHIFT\n",
      "STEPS :  [['an', 'apple'], ['eats'], 'SHIFT']\n",
      "BUFFER:  ['apple']\n",
      "STACK :  ['eats', 'an']\n",
      "DONE. :  {'John'}\n",
      "-----\n",
      "\n",
      "SHIFT\n",
      "STEPS :  [['apple'], ['eats', 'an'], 'SHIFT']\n",
      "BUFFER:  []\n",
      "STACK :  ['eats', 'an', 'apple']\n",
      "DONE. :  {'John'}\n",
      "-----\n",
      "\n",
      "LEFT-ARC\n",
      "STEPS :  [[], ['eats', 'an', 'apple'], 'LEFTARC']\n",
      "BUFFER:  []\n",
      "STACK :  ['eats', 'apple']\n",
      "DONE. :  {'an', 'John'}\n",
      "-----\n",
      "\n",
      "RIGHT-ARC\n",
      "STEPS :  [[], ['eats', 'apple'], 'RIGHTARC']\n",
      "BUFFER:  []\n",
      "STACK :  ['eats']\n",
      "DONE. :  {'an', 'John', 'apple'}\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buffer = [\"John\", \"eats\", \"an\", \"apple\"]\n",
    "stack = []\n",
    "steps = []\n",
    "done = set()\n",
    "\n",
    "\n",
    "shift(buffer, stack, steps, done)\n",
    "print(\"SHIFT\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "shift(buffer, stack, steps, done)\n",
    "print(\"SHIFT\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "left_arc(buffer, stack, steps, done)\n",
    "print(\"LEFT-ARC\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "shift(buffer, stack, steps, done)\n",
    "print(\"SHIFT\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "shift(buffer, stack, steps, done)\n",
    "print(\"SHIFT\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "left_arc(buffer, stack, steps, done)\n",
    "print(\"LEFT-ARC\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n",
    "right_arc(buffer, stack, steps, done)\n",
    "print(\"RIGHT-ARC\")\n",
    "print(\"STEPS : \", steps[-1])\n",
    "print(\"BUFFER: \", buffer)\n",
    "print(\"STACK : \", stack)\n",
    "print(\"DONE. : \", done)\n",
    "print(\"-----\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4\n",
    "\n",
    "rubric={accuracy:3,quality:1}\n",
    "\n",
    "Implement the core of the algorithm in the form of a function `shift_reduce_steps`. The basic logic is the following (if you need more, check out *J&M 14.4.1*). To start with:\n",
    "\n",
    "1. Create your parser buffer by calling `get_buffer`.\n",
    "1. Then create a dependency lookup `dlookup` by calling `get_dependency_lookup`. This dictionary will allow you to find the dependents for each word in the sentence.\n",
    "\n",
    "You can then implement the main loop of the algoritm:\n",
    "\n",
    "1. While there are **still words in the buffer** and the stack `!= [(\"ROOT\", 0)]`, keep looping. \n",
    "1. In each iteration, if there are **less than three elements on the stack, you must shift*** (unless the buffer is empty).\n",
    "1. Otherwise, check to see **if the second to last element on the stack is a dependent of the last one** (according to your dependency lookup). If so, do a **left arc**.\n",
    "1. Otherwise, if **the top word on the stack is a dependent of the second** ***AND*** **all the dependents of the top word have already been processed** (this condition is why we keep a done list!), then do a **right arc**.\n",
    "1. If neither of these applies, do a shift. When you're done, **return the steps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_reduce_steps(dependency_graph):\n",
    "    steps = []\n",
    "    stack = [(\"ROOT\", 0)]\n",
    "    done = set()\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An assertion to check your code. If you can't pass the assertion, then try a smaller example to figure out the bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_parse = dependency_treebank.parsed_sents()[0]\n",
    "assert shift_reduce_steps(test_parse) == [[[('Pierre', 1), ('Vinken', 2), (',', 3), ('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0)], 'SHIFT'],\n",
    "                                          [[('Vinken', 2), (',', 3), ('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Pierre', 1)], 'SHIFT'],\n",
    "                                          [[(',', 3), ('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Pierre', 1), ('Vinken', 2)], 'LEFTARC'],\n",
    "                                          [[(',', 3), ('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2)], 'SHIFT'],\n",
    "                                          [[('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), (',', 3)], 'RIGHTARC'],\n",
    "                                          [[('61', 4), ('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2)], 'SHIFT'],\n",
    "                                          [[('years', 5), ('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('61', 4)], 'SHIFT'],\n",
    "                                          [[('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('61', 4), ('years', 5)], 'LEFTARC'],\n",
    "                                          [[('old', 6), (',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('years', 5)], 'SHIFT'],\n",
    "                                          [[(',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('years', 5), ('old', 6)], 'LEFTARC'],\n",
    "                                          [[(',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('old', 6)], 'RIGHTARC'],\n",
    "                                          [[(',', 7), ('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2)], 'SHIFT'],\n",
    "                                          [[('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), (',', 7)], 'RIGHTARC'],\n",
    "                                          [[('will', 8), ('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2)], 'SHIFT'],\n",
    "                                          [[('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('Vinken', 2), ('will', 8)], 'LEFTARC'],\n",
    "                                          [[('join', 9), ('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8)], 'SHIFT'],\n",
    "                                          [[('the', 10), ('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9)], 'SHIFT'],\n",
    "                                          [[('board', 11), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('the', 10)], 'SHIFT'],\n",
    "                                          [[('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('the', 10), ('board', 11)], 'LEFTARC'],\n",
    "                                          [[('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('board', 11)], 'RIGHTARC'],\n",
    "                                          [[('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9)], 'SHIFT'],\n",
    "                                          [[('a', 13), ('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12)], 'SHIFT'],\n",
    "                                          [[('nonexecutive', 14), ('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12), ('a', 13)], 'SHIFT'],\n",
    "                                          [[('director', 15), ('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12), ('a', 13), ('nonexecutive', 14)], 'SHIFT'],\n",
    "                                          [[('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12), ('a', 13), ('nonexecutive', 14), ('director', 15)], 'LEFTARC'],\n",
    "                                          [[('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12), ('a', 13), ('director', 15)], 'LEFTARC'],\n",
    "                                          [[('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12), ('director', 15)], 'RIGHTARC'],\n",
    "                                          [[('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('as', 12)], 'RIGHTARC'],\n",
    "                                          [[('Nov.', 16), ('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9)], 'SHIFT'],\n",
    "                                          [[('29', 17), ('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('Nov.', 16)], 'SHIFT'],\n",
    "                                          [[('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('Nov.', 16), ('29', 17)], 'RIGHTARC'],\n",
    "                                          [[('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9), ('Nov.', 16)], 'RIGHTARC'],\n",
    "                                          [[('.', 18)], [('ROOT', 0), ('will', 8), ('join', 9)], 'RIGHTARC'],\n",
    "                                          [[('.', 18)], [('ROOT', 0), ('will', 8)], 'SHIFT'],\n",
    "                                          [[], [('ROOT', 0), ('will', 8), ('.', 18)], 'RIGHTARC'],\n",
    "                                          [[], [('ROOT', 0), ('will', 8)], 'RIGHTARC']]\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Decision function\n",
    "\n",
    "The key to good shift-reduce parsing is making the correct decision at each step. To do that, some kind of decision function is necessary. When parsing a sentence, the decision function will score the possible parse actions `SHIFT`, `LEFTARC` and `RIGHTARC` based on the current buffer and stack.\n",
    "\n",
    "#### 3.1\n",
    "\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Here we will build a simple scoring function using statistics contained in the dependency version of the Penn Treebank. Our function will look at the POS tags of the topmost tokens on the stack in order to decide the action.\n",
    "\n",
    "We will start by forming a training set `train_set`, which contains 80% of sentences in the Penn Dependency Treebank. We'll then count how many times `SHIFT`, `LEFTARC` and `RIGHTARC` occur with different combinations of POS tags in the training data: \n",
    "\n",
    "1. Iterate over `train_set` and use `shift_reduce_steps` to convert each dependency graph into sequence of shift-reduce steps. \n",
    "1. You should also use the helper function `get_POS` to extract the POS tags from each dependency graph (note that the POS for the `ROOT` element is `TOP`).\n",
    "1. You should count triplets `(POS1, POS2, action)`, where `action` is a shift-reduce action, and `POS1` and `POS2` are the POS tags of the two topmost tokens on the stack. Store counts from each step, **where the stack has more than one item** in `stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(dependency_graph):\n",
    "    poses = []\n",
    "    for i in range(len(dependency_graph.nodes)):\n",
    "        poses.append(dependency_graph.nodes[i][\"tag\"])\n",
    "    return poses\n",
    "\n",
    "# Store counts of 3-tuples (pos1, pos2, parse-action) in this dictionary.\n",
    "stats = defaultdict(int)\n",
    "# {(pos1, pos2, parse-action): number, ...}\n",
    "\n",
    "# The first 80% of the Penn Dependency Treebank\n",
    "cutoff = int(len(dependency_treebank.parsed_sents()) * 4/5)\n",
    "train_set = dependency_treebank.parsed_sents()[:cutoff]\n",
    "\n",
    "# your code here\n",
    "for graph in train_set:\n",
    "    # get poses using `get_POS`\n",
    "    # iterate using `shift_reduce_steps(graph)` to get `buffer, stack, action`:\n",
    "    #        update your stats;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "[None, 'Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "\n",
      "POS:\n",
      "['TOP', 'NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n",
      "\n",
      "STATS:\n",
      "defaultdict(<class 'int'>, {('TOP', 'NNP', 'SHIFT'): 5, ('NNP', 'NNP', 'LEFTARC'): 1, ('NNP', ',', 'RIGHTARC'): 2, ('NNP', 'CD', 'SHIFT'): 1, ('CD', 'NNS', 'LEFTARC'): 1, ('NNP', 'NNS', 'SHIFT'): 1, ('NNS', 'JJ', 'LEFTARC'): 1, ('NNP', 'JJ', 'RIGHTARC'): 1, ('NNP', 'MD', 'LEFTARC'): 1, ('TOP', 'MD', 'SHIFT'): 2, ('MD', 'VB', 'SHIFT'): 3, ('VB', 'DT', 'SHIFT'): 1, ('DT', 'NN', 'LEFTARC'): 2, ('VB', 'NN', 'RIGHTARC'): 1, ('VB', 'IN', 'SHIFT'): 1, ('IN', 'DT', 'SHIFT'): 1, ('DT', 'JJ', 'SHIFT'): 1, ('JJ', 'NN', 'LEFTARC'): 1, ('IN', 'NN', 'RIGHTARC'): 1, ('VB', 'IN', 'RIGHTARC'): 1, ('VB', 'NNP', 'SHIFT'): 1, ('NNP', 'CD', 'RIGHTARC'): 1, ('VB', 'NNP', 'RIGHTARC'): 1, ('MD', 'VB', 'RIGHTARC'): 1, ('MD', '.', 'RIGHTARC'): 1, ('TOP', 'MD', 'RIGHTARC'): 1})\n"
     ]
    }
   ],
   "source": [
    "def get_words(dependency_graph):\n",
    "    words = []\n",
    "    for i in range(len(dependency_graph.nodes)):\n",
    "        words.append(dependency_graph.nodes[i][\"lemma\"])\n",
    "    return words\n",
    "\n",
    "graph = dependency_treebank.parsed_sents()[0]\n",
    "\n",
    "print(\"WORDS:\")\n",
    "print(get_words(graph))\n",
    "\n",
    "print(\"POS:\")\n",
    "print(get_POS(graph))\n",
    "\n",
    "print(\"STATS:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertions to check that your function works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert stats[(\"DT\", \"NN\", \"LEFTARC\")] == 4470\n",
    "assert stats[(\"VBD\", \"NN\", \"RIGHTARC\")] == 559\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "\n",
    "rubric={accuracy:3,quality:1}\n",
    "\n",
    "Then, iterate over the last 20% of the Penn Treebank (i.e. `test_set`), and for each step, see if each step taken to create the dependency parse corresponds to the highest probability action based on the statistics derived from your training data. Calculate and print out this accuracy (it should be about 72%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dependency_treebank.parsed_sents()[cutoff:]\n",
    "\n",
    "#your code here\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# your code here\n",
    "    # get poses using `get_POS`\n",
    "    # iterate using `shift_reduce_steps(graph)` to get `buffer, stack, action`:\n",
    "    #        get possible actions (there might be many, so sort them)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f\" % (correct * 100 / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Optional\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Discuss at least one major problem in evaluating the performance of a parser in this way. Also, discuss what you might do to get a more accurate measurement of the ultimate effect of your scoring function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Optional\n",
    "\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Instead of using simple statistics, use scikit learn to build a machine learning classifier which selects the best action using relevant features (including the POS and lexical features taken from both the buffer and the stack), and show it works better. You can use a [DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html). \n",
    "\n",
    "You can try different machine learning algorithms and see what works. Please tune hyperparameters using cross-validation.\n",
    "\n",
    "**Note** You should be able to get above of 90% accuracy with the right features and algorithm, though you don't have to do that well to get full points here, an improvement of 1% is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
