{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KKwgFHVvRMql"
   },
   "source": [
    "# COLX 561 Lab Assignment 3: Question-Answering with BERT\n",
    "## Assignment Objectives\n",
    "\n",
    "In this lab, you will implement and train a (distil)BERT model for Question and Answering on a subset of the [SQuAD v2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset. Lab objectives include:\n",
    "\n",
    "1. Convert the data to tensors using the BERT tokenizer\n",
    "2. Train a model for Question-Answering by tuning on top of a pre-trained BERT model \n",
    "3. Optimize the choice of start and end indicies\n",
    "\n",
    "We use distBERT in this lab because it is significantly smaller and faster than BERT, but with very similar performance. Even though we are using distBERT, we will call it BERT throughout this lab\n",
    "\n",
    "If you do not have access to a GPU locally, you'll likely want to run this on Google Colab with a GPU backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install pulp\n",
    "# !python3 -m pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVe9XpXPRMqm"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "Run the code below to access relevant modules (you can add to this as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXd_5YurRMqm"
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "import numpy as np\n",
    "import torch\n",
    "import pulp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmzOERS4RMqq"
   },
   "source": [
    "For this lab, you'll be working with the SQuAD database. Download the SQuAD data from [google drive](https://drive.google.com/file/d/1jAXaGLyCllMoa6suFiZro4cuWf0Mnx9G/view?usp=sharing), unzip it into a directory outside of your lab repo and change the path below. Later you will probably want to put the data on Google drive and change this path so it points to your mounted data.\n",
    "\n",
    "The question, context (also called the passage), and answer for a given set of QA training data are stored in separate files with corresponding line numbers. You should open up the data files to make sure you understand what they each represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ9_ABDURMqq"
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "squad_path = '../Data/Lab3/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "==> train.question <==\n",
    "what percentage of imperial 's staff was classified as world leading in 2008 ?\n",
    "what paradox did sheptycki point out ?\n",
    "\n",
    "==> train.answer <==\n",
    "26 %\n",
    "the harder policing agencies work to produce security , the greater are feelings of insecurity\n",
    "\n",
    "==> train.span <==\n",
    "6 7\n",
    "166 180\n",
    "\n",
    "==> train.context <==  **[[[ ANSWER ]]]** <--  I added\n",
    "the 2008 research assessment exercise returned **[[[26 %]]]** of the 1225 staff submitted as being world-leading ( 4* ) and a further 47 % as being internationally excellent ( 3* ) . the 2008 research assessment exercise also showed five subjects – pure mathematics , epidemiology and public health , chemical engineering , civil engineering , and mechanical , aeronautical and manufacturing engineering – were assessed to be the best [ clarification needed ] in terms of the proportion of internationally recognised research quality .\n",
    "studies of this kind outside of europe are even rarer , so it is difficult to make generalizations , but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in north america and europe confirmed that low visibility of police information and intelligence sharing was a common feature ( alain , 2001 ) . intelligence-led policing is now common practice in most advanced countries ( ratcliffe , 2007 ) and it is likely that police intelligence sharing and information exchange has a common morphology around the world ( ratcliffe , 2007 ) . james sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies ' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic . he argues that transnational police information circuits help to \" compose the panic scenes of the security-control society \" . the paradoxical effect is that , **[[[the harder policing agencies work to produce security , the greater are feelings of insecurity]]]** .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EhCTfduRMqt"
   },
   "source": [
    "## Tidy Submission\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this Jupyter notebook with your answers embedded\n",
    "- Be sure to follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7Ei4yBlRMqt"
   },
   "source": [
    "## Exercise 1: Initial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6ob53ujRMqu"
   },
   "source": [
    "### Exercise 1.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Your first task is to write a function, `convert_to_BERT_tensors`, which uses the build-in BERT tokenizer to create tensors for input to the BERT model. You should call the tokenizer directly, look at the tokenizer [docs](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__). This function should involve only two lines of code, but you need to get the arguments right. Your tokenization process must\n",
    "* return pytorch tensors corresponding to the input_ids and attention masks (which prevent BERT from attending to padding)\n",
    "* combine questions and contexts into a single input with a separator character\n",
    "* truncate when the question and context is too long to work with BERT (longer than 512) \n",
    "* add padding when the question and context is too short "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SP00GzbiRMqu"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def convert_to_BERT_tensors(questions, contexts):\n",
    "    '''takes a parallel list of question strings and answer strings'''\n",
    "    #your code here\n",
    "    # use tokenize with `return_tensors='pt', truncation=True, padding=True` for `question` and `contexts`\n",
    "    stuff = tokenizer(questions, contexts, return_tensors='pt', truncation=True, padding=True)\n",
    "    return stuff['input_ids'], stuff['attention_mask']\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_fq2PNTRMqx",
    "outputId": "128fcd04-ac96-43cc-9758-f875bdea3fb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\"Why?\", \"How?\"]\n",
    "test_contexts = [\"I think it is because we can bluminate\", \"It was done\"\" \".join([\"very\"]*1000) + \" well\"]\n",
    "\n",
    "ids, mask = convert_to_BERT_tensors(test_questions,test_contexts)\n",
    "assert ids.shape == (2,512) # 512 because that's the max allowed\n",
    "assert ids[0][3] == 102 # fourth token is separator\n",
    "assert list(ids[0][-100:]) == [0]*100 # first row is mostly padding\n",
    "assert list(ids[1][-100:]) != [0]*100 # second row is not\n",
    "assert list(mask[0][-100:]) == [0]*100 # first row padding is masked\n",
    "assert list(mask[1][-100:]) != [0]*100 # second row is not padding, no mask\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 2339 1029 102 1045 2228 ...\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab['[CLS]'], tokenizer.vocab['why'], \n",
    "      tokenizer.vocab['?'], tokenizer.vocab['[SEP]'], tokenizer.vocab['i'], tokenizer.vocab['think'], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2339,  1029,   102,  1045,  2228,  2009,  2003,  2138,  2057,\n",
       "         2064, 14154, 19269,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0][:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYW5MOfqRMq2"
   },
   "source": [
    "### Exercise 1.2\n",
    "rubric={accuracy:3, efficiency:1}\n",
    "\n",
    "As our target for training, we want tensors of indicies which correspond to the beginning and end of the answer span. For example, given the question `Who wrote Hamlet ?`, the answer `W . Shakespeare` is given by the span `[5, 7]` in the context `Between 1599 and 1601 , W . Shakespeare wrote Hamlet`.\n",
    "\n",
    "This gets a bit tricky because BERT will pack the question and context in the same vector and tokenize the input words:\n",
    "\n",
    "```[CLS] Who wrote Ham ##let ? [SEP] Between 1599 and 16 ##01 , W . Shakes ##peare wrote Ham ##let [SEP]```\n",
    "\n",
    "This means that the span will change from the original `[5,7]`, in the span file, to `[13,16]` in the BERT input.\n",
    "\n",
    "You should implement the function `get_answer_span_tensor()` which takes strings corresponding to a question, context, and answer as input. It then identifies the correct span in the BERT input. To find the correct answer span, you can use the [tokenize()](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.tokenize) method of BERT's tokenizer which splits the input into sub-word units.\n",
    "\n",
    "Start by forming the BERT input string (`[CLS] question [SEP] context [SEP]`). Remember to truncate the input to 512 tokens, which is the BERT maximum. We'll simply discard all tokens beyond that. Then, apply `tokenize()` both to the BERT input and the answer, and match the tokenized answer to the correct substring in the tokenized input. Return `torch.tensor([start,end])` for the identified span `[start,end]`.\n",
    "\n",
    "**Note** because we're truncating, it may happen that the answer does not appear in the input (because it would have a start index >= 512). In that case, return the range `torch.tensor([0,0])`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GwfsTGKRMq3"
   },
   "outputs": [],
   "source": [
    "def get_answer_span_tensor(question,context,answer):\n",
    "    # your code here\n",
    "    input_tokens = tokenizer.tokenize('[CLS] ' + question + ' [SEP] ' + context)\n",
    "    answer_tokens = tokenizer.tokenize(answer)\n",
    "    # print(\"input: \", input_tokens)\n",
    "    # print(\"answer: \", answer_tokens)\n",
    "    span_len = len(answer_tokens)\n",
    "    # print(\"span length: \", span_len)\n",
    "    for i in range(min(len(input_tokens) - span_len+1, 512 - span_len - 1)):\n",
    "        if input_tokens[i:i+span_len] == answer_tokens:\n",
    "            span = torch.tensor([i,i+span_len - 1])\n",
    "            break\n",
    "    else:\n",
    "        span = torch.tensor([0,0])\n",
    "        \n",
    "    return span\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgFinF9_RMq5",
    "outputId": "d8ec6ff3-6899-4b98-9459-8187cc2fbc6f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  ['[CLS]', 'why', '?', '[SEP]', 'i', 'think', 'it', 'is', 'because', 'we', 'can', 'blu', '##minate']\n",
      "answer:  ['because', 'we', 'can', 'blu', '##minate']\n",
      "span length:  5\n",
      "input:  ['[CLS]', 'why', '?', '[SEP]', 'i', 'think', 'it', 'is', 'because', 'we', 'can', 'blu', '##minate']\n",
      "answer:  ['because', 'we', 'can', 'fu', '##mia', '##ge']\n",
      "span length:  6\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Why?\"\n",
    "test_context = \"I think it is because we can bluminate\"\n",
    "test_answer = \"because we can bluminate\"\n",
    "bad_answer  = \"because we can fumiage\"\n",
    "span = get_answer_span_tensor(test_question,test_context,test_answer)\n",
    "assert span.shape == (2,)\n",
    "assert list(span) == [8,12]\n",
    "span = get_answer_span_tensor(test_question,test_context,bad_answer)\n",
    "assert list(span) == [0,0]\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Now write code that builds a `QAdataset` (defined below) and a corresponding dataloader for each of the train, dev, and test splits with the provided `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "batch_size = 16\n",
    "\n",
    "class QAdataset(Dataset):\n",
    "    '''A dataset for housing QA data, including input_data, output_data, and padding mask'''\n",
    "    def __init__(self, input_data, output_data,mask):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = self.output_data[index]\n",
    "        data_val = self.input_data[index]\n",
    "        mask = self.mask[index]\n",
    "        return data_val,target,mask "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialize data loaders for `QAdataset`. We start by writing a function `prepare_QA_dataset` which reads QA data from the directory `squad_path`.\n",
    "\n",
    "The function takes a string `prefix` (either `train`, `dev` or `test`) as input, and reads the SQuAD files `prefix.question` and `prefix.context` in the directory `squad_path` into two lists: `questions` and `contexts` (these should simply be lists of strings corresponding to the lines in the original data files). \n",
    "\n",
    "The function then passes the `questions` and `contexts` lists to `convert_to_BERT_tensors` which returns a list of BERT input tensors `QA_input` and a list of masks `masks`. \n",
    "\n",
    "Next we will generate a list `spans` of answers spans. There are two cases.\n",
    "\n",
    "1. If we're reading the test set (i.e. `prefix == \"test\"`), then the span for each example should be empty: `torch.tensor([0,0])`\n",
    "1. If we're reading the training or development set (i.e. `prefix in \"train dev\".split()`), then start by reading the answers in the file `prefix.answer` into a list `answers`. Then call `get_answer_span_tensor` on with the arguments `questions[i]`, `contexts[i]` and `answers[i]` to get the correct span.\n",
    "\n",
    "Finally, return `QAdataset(QA_input, spans, masks)`.\n",
    "\n",
    "You should then generate `train_dataset`, `dev_dataset` and `test_dataset` by calling `prepare_QA_dataset`. Use the datasets to initialize [data loaders](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) `train_dataloader`, `dev_dataloader` and `test_dataloader`. When initializating the data loaders, set `batch_size=batch_size` and `shuffle=False`. \n",
    "\n",
    "**Note** It can take several minutes to read the datasets because they are large. It can be a good idea to first complete the entire lab with small subsets of these datasets, e.g. the first 5000 examples from each dataset. When everything is working, you can then switch to the full datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_QA_dataset(split):\n",
    "    \n",
    "    # '''for split in \"train\", \"dev\", \"test\", perpares Pytorch dataset by reading the files and \n",
    "    # converting the data to tensors. For test, provides dummy answers'''    \n",
    "    with open(squad_path + split + \".question\", encoding=\"utf-8\") as f:\n",
    "        questions = f.readlines()\n",
    "    with open(squad_path + split + \".context\", encoding=\"utf-8\") as f:\n",
    "        contexts = f.readlines()    \n",
    "    QA_input, masks = convert_to_BERT_tensors(questions, contexts)\n",
    "\n",
    "    # only for train and dev; \n",
    "    if \"train\" == split or \"dev\" == split: \n",
    "        with open(squad_path + split + \".answer\", encoding=\"utf-8\") as f:\n",
    "            answers = f.readlines()\n",
    "            spans = []\n",
    "            # based on the lenght of questions, `get_answer_span_tensor`\n",
    "            for i in range(len(questions)):\n",
    "                spans.append(get_answer_span_tensor(questions[i], contexts[i], answers[i]))\n",
    "    else:\n",
    "        spans = [torch.tensor([0,0])]*len(questions)\n",
    "    return QAdataset(QA_input, spans, masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT LOAD the train dataset before finishing Ex 3. \n",
    "\n",
    "# train_dataset = prepare_QA_dataset(\"train\")\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "dev_dataset = prepare_QA_dataset('dev')\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = prepare_QA_dataset('test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDrQP9NyRMrS"
   },
   "source": [
    "## Exercise 2: BERT Training \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPaXZFwfRMrS"
   },
   "source": [
    "### Exercise 2.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "We will now train a BERT model for QA. The Huggingface library has a [BERT QA model](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforquestionanswering) that includes the main pre-trained BERT model as well as the QA heads. We will use this model (the preinitialized `model` object below). \n",
    "\n",
    "Given an input `batch` of dimension `batch_size x 512` (where 512 is the BERT maximum input size) and input `masks`, the model forward function will return an object `output` with two members:\n",
    "\n",
    "1. `output.start_logits`, a log-distributions of shape `batch_size x 512` over the start position for the QA span. \n",
    "1. `output.end_logits`, a log-distribution of shape `batch_size x 512` over the end position of the QA span.\n",
    "\n",
    "----------\n",
    "\n",
    "Use should start by defining a loss function. Please use [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). You should then define an [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer for your model parameters using learning rate 0.00003.\n",
    "\n",
    "Then, iterate over the data using your `train_dataloader`:\n",
    "```\n",
    "for train_text_batch, train_span_batch, masks in train_dataloader:\n",
    "   ... \n",
    "```\n",
    "\n",
    "You should pass the inputs and masks to `model.forward`. This returns an object `output` which was explained above. Calculate the loss as a sum of the losses for `output.start_logits` and `output.end_logits` using `train_span_batch`. It is a `batch_size x 2` tensor giving the gold standard spans: `[start, end]` for each example in the training batch. \n",
    "\n",
    "Remember that the objective is to raise the probabilities `output.start_logits[i,start]` and `output.end_logits[i,end]` as high as possible for each example `i`.\n",
    "\n",
    "Print out the loss regularly, if everything is correct you should see it drop rapidly. \n",
    "\n",
    "You only need to train your model for a single epoch here (you may want to increase this number later for the Kaggle competition).\n",
    "\n",
    "**Note** If you're running on a GPU on Google Colab, you will need to copy your tensors and model over to the GPU (check practical work 4 for COLX 581 to see how this is done).\n",
    "\n",
    "**Note** If this is too slow, run training only on the 10,000 first (or even 2000 first) batches. This will reduce your accuracy but it's more important to be able to run the training in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDzJrSVwRMrT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  160 QA pairs of  77558\n",
      "Last loss: 11.232280731201172\n",
      "Processed  320 QA pairs of  77558\n",
      "Last loss: 9.259109497070312\n",
      "Processed  480 QA pairs of  77558\n",
      "Last loss: 9.044548034667969\n",
      "Processed  640 QA pairs of  77558\n",
      "Last loss: 7.710631370544434\n",
      "Processed  800 QA pairs of  77558\n",
      "Last loss: 8.131200790405273\n",
      "Processed  960 QA pairs of  77558\n",
      "Last loss: 6.788171768188477\n",
      "Processed  1120 QA pairs of  77558\n",
      "Last loss: 7.019942283630371\n",
      "Processed  1280 QA pairs of  77558\n",
      "Last loss: 6.349470138549805\n",
      "Processed  1440 QA pairs of  77558\n",
      "Last loss: 6.955586910247803\n",
      "Processed  1600 QA pairs of  77558\n",
      "Last loss: 7.553208351135254\n",
      "Processed  1760 QA pairs of  77558\n",
      "Last loss: 6.197084426879883\n",
      "Processed  1920 QA pairs of  77558\n",
      "Last loss: 4.91841459274292\n",
      "Processed  2080 QA pairs of  77558\n",
      "Last loss: 4.939706802368164\n",
      "Processed  2240 QA pairs of  77558\n",
      "Last loss: 5.47674560546875\n",
      "Processed  2400 QA pairs of  77558\n",
      "Last loss: 5.493062973022461\n",
      "Processed  2560 QA pairs of  77558\n",
      "Last loss: 4.428867340087891\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# #your code here\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# epochs = 1\n",
    "\n",
    "# model.to(device)\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "#     batch_counter = 0\n",
    "#     for train_text_batch, train_span_batch, masks in train_dataloader:       \n",
    "#         model.zero_grad()\n",
    "#         train_text_batch, train_span_batch, masks = train_text_batch.to(device), train_span_batch.to(device), masks.to(device)\n",
    "#         output = model(train_text_batch,attention_mask=masks)\n",
    "#         loss = loss_function(output.start_logits, train_span_batch[:,0])\n",
    "#         loss += loss_function(output.end_logits, train_span_batch[:,1])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         batch_counter += 1\n",
    "#         if batch_counter % 10 == 0:\n",
    "#             print(\"Processed \", batch_counter*batch_size, \"QA pairs of \", len(train_dataset))\n",
    "#             print(\"Last loss:\", loss.item())\n",
    "#         epoch_loss += loss.item()\n",
    "#     print('After epoch:', epoch, 'Loss is:', epoch_loss)\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# After epoch: 0 Loss is: 44305.126106739044\n",
    "# After epoch: 1 Loss is: 40180.46493291855\n",
    "# After epoch: 2 Loss is: 43801.96783733368\n",
    "# After epoch: 3 Loss is: 56590.43050909042\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here | Commencer ici:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"COLX_563_adv-semantics_lab3.bin\" # download from googledrife (see the github/jungyeul/labs site);\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu'))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vbbr2MWdRMrW"
   },
   "source": [
    "### Exercise 2.2\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Now run the trained classifier over the dev set and calculate the accuracy for each of the start and end predictions (independently). You can iterate over your `dev_dataloader`, pass `batch` and `masks` to the QA model and use argmax on `output.start_logits` and `output.end_logits`.\n",
    "\n",
    "You should get close to 50% performance for both (if you trained on at least 10,000 examples). \n",
    "\n",
    "**Note** Don't forget to put the model in `eval` mode, with no gradients in order to disable dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFleiJx7RMrW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts accuracy\n",
      "0.6228220020498805\n",
      "Ends accuracy\n",
      "0.6592073795695251\n"
     ]
    }
   ],
   "source": [
    "# takes 18mins... \n",
    "\n",
    "predicted_starts = []\n",
    "gold_starts = []\n",
    "predicted_ends = []\n",
    "gold_ends = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for dev_text_batch, dev_span_batch, masks in dev_dataloader:\n",
    "        dev_text_batch, masks = dev_text_batch.to(device), masks.to(device)\n",
    "        output = model(dev_text_batch,attention_mask=masks)\n",
    "\n",
    "        # `start_scores` and `end_scores` from `start_logitic` and `end_logtis` of `output`\n",
    "        start_scores = \n",
    "        end_scores = \n",
    "\n",
    "        # `target``from `dev_span_batch`\n",
    "        targets = \n",
    "\n",
    "        # `extend` for \n",
    "        # `predicted_starts` and `predicted_ends`; \n",
    "        # argmax of start_scores and end_scores, and list\n",
    "        predicted_starts.extend(...)\n",
    "        predicted_ends.extend(...)\n",
    "\n",
    "        # and `gold_starts` and `gold_ends`\n",
    "        # list of targets; \n",
    "        gold_starts.extend(...)\n",
    "        gold_ends.extend(...)\n",
    "\n",
    "print(\"Starts accuracy\")\n",
    "print(accuracy_score(gold_starts,predicted_starts))\n",
    "print(\"Ends accuracy\")\n",
    "print(accuracy_score(gold_ends,predicted_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaT0PM3pRMro"
   },
   "source": [
    "## Exercise 3: Discrete optimization of answer spans\n",
    "rubric={accuracy:3, efficiency:2, quality:1}\n",
    "\n",
    "The model from exercise 2 independently predicts both start and end indicies for the answer span. However, this is a case where there is a dependency between predictions that needs to be considered. In particular, it doesn't make sense to have the end index appear before the start index, or too long after it. You want to pick the highest probability pair that satisfies those basic constraints.\n",
    "\n",
    "In this exercise, we will enforce these constraints using discrete optimization. You will write a function `select_best_answer_span` which takes a two numpy matrices of equal size; each row of the first matrix corresponds to the log probabilities for start prediction, and each row of the second matrix is the corresponding log probabilties for the end prediction (i.e. these are the `start_logits` and `end_logits` from above, appropriately logsoftmaxed). The third argument to the function is an integer `distance` which indicates how soon after the start token the end token must appear (i.e. distance = 0 indicates that start and end must be the same token).\n",
    "\n",
    "An example is provided in the form of `select_best_answer_span_slow`. It solves this problem but it does so in a slow, fairly brute force way. You need to implement a better version. You have two choices: \n",
    "\n",
    "1. For each row of start/end probabilities provided, set up a PuLP problem (see Lecture 7 of DSCI 512). Your variable dict for each problem should consist of only two rows, with as many columns as you have tokens in your input. The first row corresponds to the choice of the start index, the second corresponds to the choice of end index. You'll need to constrain there to be only one 1 per row (i.e. only one index can be assigned for each start/end). The other set of constraints are trickier: one way to formulate things is that, for each possible index, the (single) corresponding element of the start row minus all the elements of the end row which are ends for that start should be less than zero; this means you can't have an index at a particular location the start row without having an index at an allowed location in the end row (this is very similar to the \"connectivity\" constraint from the ILP problem in Lab 4 of DSCI 512, look back at that and adapt it if you're confused!). Make sure you do NOT setup constraints for every possible pair of start/end indicies, since that would be no better than the provided solution. Note that in this case this solution won't *actually* be faster than `select_best_answer_span_slow` because of the overhead in setting up each PuLP problem (PuLP doesn't support solving the same problem with different inputs, so you need to recreate the problem from scratch each time), but it is still more elegant, and good linear programming practice!\n",
    "\n",
    "2. Solve it in regular Python, but using a smarter approach. Instead of searching all possible pairings in a brute force fashion, you should try searching in such a way that you're always checking spans (pairs of start/end indicies) which have high probability. There are number of ways to solve this, one recommended way would be to first use argsort (to get starts/ends which are individually high probability), and then you can apply an algorithm similar to Dijkstra's (DSCI 512 Lecture 8), i.e. keep a list of spans sorted by the sum of the probabilities of their end points, checking them to see if they satisfy your requirements. The tricky part is knowing when and how to add new spans to your sorted list: you should more add spans not only when your list is empty, but also when you can no longer be sure the highest probability span in your list is the highest probability span overall (excluding those you've already checked). But when is that? You obviously can't just add all spans, that would be an $O(n^2)$ solution no better than the provided. If you do this one correctly, it should be quite a bit faster than the provided slow solution. \n",
    "\n",
    "You can get one bonus point here if you code both kinds of solutions.\n",
    "\n",
    "After you're done, use predictions from your model on the dev set to show that the `select_best_answer_span` you wrote always gives the same result as `select_best_answer_span_slow` (iterate over your batches and use an assert). You can use a max distance of 20 here, and in Exercise 4 (if you are participating in the kaggle competition, you may want to consider this as a hyperparameter to be optimized)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "\n",
    "```\n",
    "def select_best_answer_span(start_probs, end_probs, distance):\n",
    "    ...\n",
    "    if j <= k <= j + distance:\n",
    "        ...\n",
    "    return output_spans\n",
    "```\n",
    "\n",
    "```\n",
    "start_probs = np.array([0.1, 0.5, 0.2, 0.1, 0.1]) \n",
    "end_probs   = np.array([0.4, 0.1, 0.3, 0.1, 0.1]) \n",
    "distance    = 2\n",
    "```\n",
    "\n",
    "```\n",
    "j     k\n",
    "---------\n",
    "0     0     0.5     <- update best \n",
    "0     1     0.2\n",
    "0     2     0.4\n",
    "1     1     0.6     <- update best\n",
    "1     2     0.8     <- update best\n",
    "1     3     0.6\n",
    "2     2     0.5\n",
    "2     3     0.3\n",
    "2     4     0.3\n",
    "3     3     0.2\n",
    "3     4     0.2\n",
    "4     4     0.2\n",
    "```\n",
    "where *j* is `starts` and *k* is `ends`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. you can apply an algorithm similar to Dijkstra's \n",
    "\n",
    "```\n",
    "start_probs = np.array([0.1, 0.5, 0.2, 0.1, 0.1]) \n",
    "end_probs   = np.array([0.4, 0.1, 0.3, 0.1, 0.1]) \n",
    "\n",
    "best_starts = np.argsort(start_probs*-1) =>  [1 2 0 3 4]\n",
    "best_ends  = np.argsort(end_probs*-1)    =>  [0 2 1 3 4]\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "----------\n",
    "iter 0:\n",
    "[(best_starts, best_ends)] = (start_probs, end_probs)\n",
    "[(0, 0)] = (1,0)                  \n",
    "starts = 1 & ends 0             => (1,0) 0.9  <-- NOT BEST because of starts > ends\n",
    "----------\n",
    "iter 1:\n",
    "[(0, 1)] = (1,2)\n",
    "[(1, 0)] = (2,0)                 \n",
    "[(1, 1)] = (2,2)                 \n",
    "starts = 1 & ends = 2           => (1,2) 0.8  <-- BEST  (we will stop HERE) \n",
    "starts = 2 & ends = 0           => (2,0) 0.6  <-- NOT BEST because of starts > ends\n",
    "starts = 2 & ends = 2           => (2,2) 0.5  <-- not BEST\n",
    "----------\n",
    "iter 2:                         => not necessary because we \"already\" found BEST\n",
    "[(0, 2), (1, 2)] = (1,1), (2,1)\n",
    "[(2, 0), (2, 1)] = (0,0), (0,2)\n",
    "[(2, 2)] = (0,1)\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0\n",
      "0 0\n",
      "0 0\n",
      "curr (0.9, 1, 0)\n",
      "curr (0.9, 1, 0)\n",
      "iter  1\n",
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "curr (0.8, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "start_probs = test_start = np.array([0.1, 0.5, 0.2, 0.1, 0.1]) \n",
    "end_probs = test_ends  = np.array([0.4, 0.1, 0.3, 0.1, 0.1]) \n",
    "\n",
    "best_starts = np.argsort(start_probs*-1)            # axis = 1 if you have [[] [] ... []]  where len is # of batch;\n",
    "best_ends = np.argsort(end_probs*-1)\n",
    "output_spans = []\n",
    "distance = 10\n",
    "\n",
    "step = 0\n",
    "found = False\n",
    "sorted_spans = []\n",
    "# bound = 0\n",
    "while not found:\n",
    "    print(\"iter \", step)\n",
    "    for j in range(step + 1):\n",
    "        print(j, step)\n",
    "        print(step, j)\n",
    "    sorted_spans.extend([(start_probs[best_starts[j]] + end_probs[best_ends[step]],         # start_probs[i, best_starts[i,j]] + end_probs[i,best_ends[i,step]]\n",
    "                                                                                            # where i in `range(len(start_probs)` (iterate # of batch)\n",
    "                                best_starts[j], best_ends[step]) for j in range(step + 1)])\n",
    "    sorted_spans.extend([(start_probs[best_starts[step]] + end_probs[best_ends[j]], \n",
    "                                best_starts[step], best_ends[j]) for j in range(step + 1)])\n",
    "    sorted_spans.sort()\n",
    "\n",
    "    while not found:\n",
    "        if len(sorted_spans) > 0:\n",
    "            curr = sorted_spans.pop()\n",
    "            print(\"curr\", curr)\n",
    "            if curr[1] <= curr[2] <= curr[1] + distance:\n",
    "                found = (curr[1], curr[2])\n",
    "        else:\n",
    "            break\n",
    "    step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. set up a PuLP problem (see Lecture 7 of DSCI 512)\n",
    "\n",
    "For each row of start/end probabilities provided, set up a PuLP problem (See a `pulp` example at https://coin-or.github.io/pulp/CaseStudies/a_transportation_problem.html)\n",
    "\n",
    "- Creates the `problem` variable to contain the problem data\n",
    "```\n",
    "problem = pulp.LpProblem(\"Beer Distribution Problem\", pulp.LpMinimize)     \n",
    "```\n",
    "**[Note]** that we are maximizing `pulp.LpMaximize`;\n",
    "```\n",
    "problem = pulp.LpProblem(\"QASpanSelection\", pulp.LpMaximize)\n",
    "```\n",
    "\n",
    "--\n",
    "\n",
    "```\n",
    "Warehouses = [\"A\", \"B\"]                         # warehouse has their beers\n",
    "Bars = [\"1\", \"2\", \"3\", \"4\", \"5\"]                # bar requires beers\n",
    "                                                # there is a cost w -> b \n",
    "\n",
    "vars = pulp.LpVariable.dicts(\"Route\", (Warehouses, Bars), 0, None, pulp.LpInteger)\n",
    "\n",
    "```\n",
    "**[Note]** that we have `(locs, indicies)` where locs = warehouse & indicies = bar, and `0, 1` (instead of `0, None`)\n",
    "\n",
    "- The objective function is added to 'problem' first\n",
    "\n",
    "```\n",
    "problem += (\n",
    "    pulp.lpSum([vars[w][b] * costs[w][b] for (w, b) in Routes]),\n",
    "    \"Sum_of_Transporting_Costs\",\n",
    ")\n",
    "```\n",
    "\n",
    "**[Note]** that we have `[ start_probs[i], end_probs[i] ]` as costs, where $i$ is the current index;\n",
    "\n",
    "- **constraint 1** each of start/end row has only one 1 (i.e. only one index can be assigned for each start/end): \n",
    "    - `problem += pulp.lpSum(x[loc][index] ... ) == 1` for `loc` (start/end)\n",
    "    - \n",
    "\n",
    "- **constraint 2**  index of start row has 1, only span of index to `index + distance` can have 1 in end row:\n",
    "    - `problem += pulp.lpSum(starts - ends...) <= 0` for`index1` in `indices`\n",
    "    - `starts = x[0][index1]` and `x[1][index2]` where `index2` is `range(index1, min(index1 + distance+1, len(indicies)))`\n",
    " \n",
    "\n",
    "Then, \n",
    "```\n",
    "problem.solve()\n",
    "for index in range(len(Bars)):      # indices\n",
    "    if vars[0][index].value() == 1:                            \n",
    "        start = index\n",
    "    if vars[1][index].value() == 1:\n",
    "        end = index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "def select_best_answer_span_slow(start_probs, end_probs, distance):\n",
    "    ''' returns a list of spans corresponding to the highest probability QA solution which satisfy the restriction that the end index must\n",
    "    be within distance after the start index'''\n",
    "    output_spans = []\n",
    "    for i in range(start_probs.shape[0]):\n",
    "        best_indicies = None\n",
    "        best_prob = -9999 # essentially zero probability in log space, could also use -np.inf\n",
    "        for j in range(start_probs.shape[1]):\n",
    "            for k in range(end_probs.shape[1]):\n",
    "                if j <= k <= j + distance:\n",
    "                    prob = start_probs[i,j] + end_probs[i,k]\n",
    "                    if prob > best_prob:\n",
    "                        best_prob = prob\n",
    "                        best_indicies = (j,k)\n",
    "        output_spans.append(best_indicies)\n",
    "    return output_spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts accuracy\n",
      "0.6253843525794328\n",
      "Ends accuracy\n",
      "0.6545951486163307\n"
     ]
    }
   ],
   "source": [
    "predicted_starts = []\n",
    "gold_starts = []\n",
    "predicted_ends = []\n",
    "gold_ends = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dev_text_batch, dev_span_batch, masks in dev_dataloader:\n",
    "        dev_text_batch, masks = dev_text_batch.to(device), masks.to(device)\n",
    "        output = model(dev_text_batch,attention_mask=masks)\n",
    "\n",
    "        # you can copy from Ex2.2; \n",
    "        # \n",
    "\n",
    "print(\"Starts accuracy\")\n",
    "print(accuracy_score(gold_starts,predicted_starts))\n",
    "print(\"Ends accuracy\")\n",
    "print(accuracy_score(gold_ends,predicted_ends))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### your solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OrHpAxrRMro"
   },
   "outputs": [],
   "source": [
    "def select_best_answer_span_v1(start_probs, end_probs, distance):\n",
    "    '''given 2 matrices of probabilities associated with \n",
    "    indicies of a text being the start or end of an answer spans, respectively,\n",
    "    solves the ILP with the objective function being the max probability, \n",
    "    under the restriction that the end index must be no more \n",
    "    than distance after the start. Returns a tuple (start index, end index)\n",
    "    corresponding to the best solution'''\n",
    "    output_spans = []\n",
    "    locs = [0,1]                                    # warehous; [0,1] (start/end)\n",
    "    indicies = list(range(start_probs.shape[1]))    # bars;     [len(start_probs[0])] (results of log_softmax)\n",
    "    \n",
    "    for i in range(start_probs.shape[0]):\n",
    "        probs =                                     # list of [[start_probs] and [end_probs]]\n",
    "\n",
    "        problem = pulp.LpProblem(\"QASpanSelection\", pulp.LpMaximize)\n",
    "        x = pulp.LpVariable.dicts(...)\n",
    "        \n",
    "        #objective function\n",
    "        problem += pulp.lpSum(...)\n",
    "\n",
    "        # constraint #1 each of start/end row has only one 1\n",
    "        for loc in locs:\n",
    "            problem += pulp.lpSum(...) == 1\n",
    "\n",
    "        # constraint #2, if index of start row has 1, only span of index to index + \n",
    "        # distance can have 1 in end row\n",
    "        for index1 in indicies:\n",
    "            problem += pulp.lpSum(...) <= 0\n",
    "\n",
    "        problem.solve()\n",
    "\n",
    "        for index in indicies:\n",
    "            if x[0][index].value() == 1:                            \n",
    "                start = index\n",
    "            if x[1][index].value() == 1:\n",
    "                end = index\n",
    "        output_spans.append((start,end))\n",
    "    return output_spans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_answer_span_v2(start_probs, end_probs, distance):\n",
    "    '''given 2 matrices of probabilities associated with \n",
    "    indicies of a text being the start or end of an answer spans, respectively,\n",
    "    finds the highest probability spans under the restriction that the end index must be no more \n",
    "    than distance after the start. Returns a list (start index, end index) 2-plues\n",
    "    corresponding to the best solution for each row of start/end_probs'''\n",
    "    best_starts = np.argsort(start_probs*-1, axis=1)\n",
    "    best_ends = np.argsort(end_probs*-1, axis=1)\n",
    "    output_spans = []\n",
    "    for i in range(len(start_probs)):\n",
    "        step = 0\n",
    "        found = False\n",
    "        sorted_spans = []\n",
    "        bound = 0\n",
    "        while not found:\n",
    "            #  ...\n",
    "\n",
    "        output_spans.append(found)\n",
    "\n",
    "    return output_spans\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogXXQd6rRMrq"
   },
   "outputs": [],
   "source": [
    "test_starts = np.array([[0.1,0.5,0.2,0.1,0.1], [0.3,0.2,0.2,0.1,0.1]])\n",
    "test_ends = np.array([[0.4,0.1,0.3,0.1,0.1], [0.1,0.1,0.1,0.1,0.6]])\n",
    "# assert select_best_answer_span_v1(test_starts,test_ends,2) == [(1,2),(2,4)]\n",
    "# select_best_answer_span_v1(test_starts,test_ends,2)\n",
    "# print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 20\n",
    "predicted_starts = []\n",
    "gold_starts = []\n",
    "predicted_ends = []\n",
    "gold_ends = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for dev_text_batch, dev_span_batch, masks in dev_dataloader:\n",
    "        dev_text_batch, masks = dev_text_batch.to(device), masks.to(device)\n",
    "        output = model(dev_text_batch,attention_mask=masks)\n",
    "        start_scores = output.start_logits.to('cpu').detach()\n",
    "        end_scores = output.end_logits.to('cpu').detach()\n",
    "        start_probs = F.log_softmax(start_scores,dim=1).numpy()\n",
    "        end_probs = F.log_softmax(end_scores,dim=1).numpy()  \n",
    "        spans = select_best_answer_span_slow(start_probs, end_probs, distance)\n",
    "        # spans = select_best_answer_span_v1(start_probs, end_probs, distance)\n",
    "        # spans = select_best_answer_span_v2(start_probs, end_probs, distance)\n",
    "        assert spans == select_best_answer_span_slow(start_probs, end_probs, distance)\n",
    "print(\"Success!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
