{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29c908e",
   "metadata": {},
   "source": [
    "# DSCI 563 Lab 2: SGD (and optionally Newton's method)\n",
    "\n",
    "### Assignment Topics\n",
    "* Training a logistic regression model using numpy and pytorch\n",
    "* Regularization\n",
    "* Hyperparameter tuning\n",
    "* Visualization of model weights\n",
    "* Optionally: Newton's method for logistic regression\n",
    "\n",
    "### Software Requirements\n",
    "* Python (>=3.6)\n",
    "* PyTorch (>=1.2.0)\n",
    "* Jupyter (latest)\n",
    "* Scikit Learn (>=0.23.2)\n",
    "* Skorch (>=0.9)\n",
    "\n",
    "### Submission Info\n",
    "* TBD\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe1588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Run this to import the required libraries \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits  \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# We'll use double values in our tensors\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Checks if GPU is available, otherwise use CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910efa93",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "\n",
    "* Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "* Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)\n",
    "\n",
    "## Before you get started...\n",
    "\n",
    "In this assignment, you will be use numpy and vectorize many of the operations which we handled using for-loops in the lecture. For full score, many of the assignments require that you don't use loops. However, you will still get partial credit for solutions which use for-loops. \n",
    "\n",
    "If you can't come up with a vectorized solution after several attempts, it can be a good idea to implement a loop-based solution first and continue with the rest of the homework. If you have time, you can then come back to the tricky assignment later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a8155",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "In this final obligatory assignment, we'll build a logistic regression model for digit classification and implement training using stochastic gradient descent. As before, the digits we'll classify are 8-by-8 pixel grayscale images like the one below (representing the digit 0):\n",
    "\n",
    "<img src=\"zero.png\" alt=\"Zero\" style=\"width: 100px;\"/>\n",
    "\n",
    "This time, we'll encode our examples into 65-dimensional feature vectors. The 64 first dimensions correspond to pixels in the input image and the last dimension corresponds to a bias feature which always has the activation 1. \n",
    "\n",
    "Run the following code to create the training, development and test set. We'll use 1000 examples for training, 400 for development and 397 examples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e10959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 65-dim train examples\n",
      "400 65-dim dev examples\n",
      "397 65-dim test examples\n"
     ]
    }
   ],
   "source": [
    "# Load 1789 digit images (X) with gold standard labels (y)\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# Append a bias feature with activation 1 at then end of each example. \n",
    "# This gives us 65-dimensional feature vectors. \n",
    "X = np.append(X,np.ones((X.shape[0], 1)),axis=1)\n",
    "\n",
    "# Form train, dev and test set\n",
    "train_X = X[:1000]\n",
    "dev_X = X[1000:1400]\n",
    "test_X = X[1400:]\n",
    "\n",
    "train_y  = y[:1000]\n",
    "dev_y = y[1000:1400]\n",
    "test_y = y[1400:]\n",
    "\n",
    "for split, name in zip([train_X, dev_X, test_X], \"train dev test\".split()):\n",
    "    print(f\"{split.shape[0]} {split.shape[1]}-dim {name} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e8ad3",
   "metadata": {},
   "source": [
    "### Assignment 1.1\n",
    "rubric={accuracy:1, efficiency:1}\n",
    "\n",
    "We'll start by defining a the softmax operation which converts a vector of activations into a probability distribution. \n",
    "\n",
    "The function `softmax(Y)` takes one argument: `Y` a numpy array of shape $m \\times n$, representing $n$ activations for $m$ examples. It applies the softmax operation to each vector `Y[i]`:\n",
    "\n",
    "$$ (r_1, ..., r_n) \\mapsto (\\frac{\\exp(r_1)}{S}, ..., \\frac{\\exp(r_n)}{S}){\\rm,\\ where\\ } S = \\exp(r_1) + ... + \\exp(r_n)$$\n",
    "\n",
    "Note, to get full score for this assignment, you should present a fully vectorized solution (i.e. **no loops**).\n",
    "\n",
    "Hint: [`numpy.repeat`](https://numpy.org/devdocs/reference/generated/numpy.repeat.html) and [`numpy.reshape`](https://numpy.org/devdocs/reference/generated/numpy.reshape.html) can be useful in the normalization step (i.e. when dividing by $S$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16644967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[0 0 1]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "x.sum(axis=1) [1 1 3]\n",
      "x.sum(axis=1).reshape(3,1) [[1]\n",
      " [1]\n",
      " [3]]\n",
      "x.sum(axis=1).reshape(-1,1) [[1]\n",
      " [1]\n",
      " [3]]\n",
      "3\n",
      "3\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0,0,1],[1,0,0],[1,1,1]])\n",
    "print('x', x)\n",
    "\n",
    "print('x.sum(axis=1)', x.sum(axis=1))\n",
    "print('x.sum(axis=1).reshape(3,1)', x.sum(axis=1).reshape(3,1))\n",
    "print('x.sum(axis=1).reshape(-1,1)', x.sum(axis=1).reshape(-1,1))\n",
    "S = x.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "print(x.shape[0]) # # of lines\n",
    "print(x.shape[1]) # # of columns \n",
    "print(np.repeat(S, repeats=x.shape[1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "170ef60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def softmax(Y):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d243a",
   "metadata": {},
   "source": [
    "A small test for your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92ae940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM of each row:\n",
      " [4.71828183 4.71828183 8.15484549] (3,)\n",
      "SUM reshape:\n",
      " [[4.71828183]\n",
      " [4.71828183]\n",
      " [8.15484549]] (3, 1)\n",
      "SUM reshape and repeat:\n",
      " [[4.71828183 4.71828183 4.71828183]\n",
      " [4.71828183 4.71828183 4.71828183]\n",
      " [8.15484549 8.15484549 8.15484549]] (3, 3)\n",
      "Then, you can exp(vec) / SUM\n"
     ]
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "x = np.array([[0,0,1],[1,0,0],[1,1,1]])\n",
    "sm_x = softmax(np.array([[0,0,1],[1,0,0],[1,1,1]]))\n",
    "assert abs(sm_x[0].sum()) - 1 < 0.001\n",
    "assert abs(sm_x[1].sum()) - 1 < 0.001\n",
    "assert abs(sm_x[2].sum()) - 1 < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb866d1",
   "metadata": {},
   "source": [
    "### Assignment 1.2\n",
    "rubric={accuracy:2, efficiency:1}\n",
    "\n",
    "Next, we'll implement the negative log-likelihood loss function `nllloss`. The function takes two arguments:\n",
    "\n",
    "* `y_hat`, an array of classifier output probabilities having dimension $m \\times n$. There are $m$ examples and $n$ possible classes.\n",
    "* `y` an array of $n$ gold standard labels, one for each example in `y_hat`:\n",
    "\n",
    "You should return the sum of the negative log-likelihood of the gold standard labels:\n",
    "\n",
    "$$ \\mathcal{L} = -(\\log(\\hat{y}_{1}[y_1]) + ... + \\log(\\hat{y}_{m}[y_m])), $$\n",
    "\n",
    "where $x[i]$ means the $i$-th element of vector $x$.\n",
    "\n",
    "Note, to get full score for this assignment, you should present a fully vectorized solution (i.e. **no loops**).\n",
    "\n",
    "Hint: [`numpy.arange`](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) can be useful for picking the values `y_hat[0,y[0]]`, `y_hat[1,y[1]]`, ... `y_hat[m, y[m]]` using a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ab3c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1. 1. 1.]\n",
      "[-0. -0. -0.]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.zeros((3,3))\n",
    "y_hat[0,1] = 1\n",
    "y_hat[1,1] = 1\n",
    "y_hat[2,2] = 1\n",
    "\n",
    "print(y_hat)\n",
    "print(y_hat[[0,1,2], np.array([1,1,2])]) # [0,1,2] -> line number which can be obtained by np.arange(# of lines); \n",
    "print(-np.log(y_hat[[0,1,2], np.array([1,1,2])]))\n",
    "print(-np.log(y_hat[[0,1,2], np.array([1,1,2])]).sum() == 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5a876ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def nllloss(y_hat, y):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0649d7",
   "metadata": {},
   "source": [
    "A small test for your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0f79048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_HAT:\n",
      " [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Gold position:\n",
      " [1 1 2]\n",
      "np.arange(3):\n",
      " [0 1 2]\n",
      "Gold position's value in Y_HAT:\n",
      " [1. 1. 1.]\n",
      "VALUE of the GOLD position: [1. 1. 1.]\n",
      "Then, you can sum of all -np.log(values)\n"
     ]
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "y_hat = np.zeros((3,3))\n",
    "y_hat[0,1] = 1\n",
    "y_hat[1,1] = 1\n",
    "y_hat[2,2] = 1\n",
    "# print(\"Y_HAT:\\n\", y_hat)\n",
    "# print(\"Gold position:\\n\", (np.array([1,1,2])))\n",
    "\n",
    "\n",
    "# ar = np.arange(3)\n",
    "# print(\"np.arange(3):\\n\", ar)\n",
    "\n",
    "# Y_HAT[NP.ARANGE, GOLD_POSITION] = GOLD_VALUE\n",
    "\n",
    "# print(\"Gold position's value in Y_HAT:\\n\", y_hat[ar, np.array([1,1,2])])\n",
    "assert nllloss(y_hat,np.array([1,1,2])) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbada5e",
   "metadata": {},
   "source": [
    "### Assignment 1.3\n",
    "rubric={accuracy:1}\n",
    "\n",
    "We'll now start constructing out logistic regression model. You'll first define an `init` function which initializes and returns an array of model weights. The function should take two arguments:\n",
    "\n",
    "* `features`, the number of features (in our case, this will always be 65, that is, 64 pixels + the bias feature)\n",
    "* `classes`, the number of output classes (in our case, this will always be 10 because there are 10 digits)\n",
    "\n",
    "You should initialize all weights to 0 and return an array of shape `features x classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7993b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "features = 65\n",
    "classes = 10\n",
    "print(np.zeros((features, classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a611162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def init(features, classes):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04541606",
   "metadata": {},
   "source": [
    "A small test for your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27430930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "assert init(65,10).shape == (65, 10)\n",
    "assert (init(65,10)**2).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83523b",
   "metadata": {},
   "source": [
    "We'll then initialize our model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "746657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = init(65,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c06723",
   "metadata": {},
   "source": [
    "### Assignment 1.4\n",
    "rubric={accuracy:1}\n",
    "\n",
    "We'll then define a `forward` function for our logistic regression model. As arguments, the function will take:\n",
    "\n",
    "* An array of $m$ examples `X` having shape $m \\times n$, where $n$ is the number of features for an individual examples (in our case, 65).\n",
    "* A weight matrix $W$ of shape $n \\times c$, where $c$ is the number of output classes (10 in our case).\n",
    "\n",
    "The function should use your `softmax` implementation and return an array of class probabilities having dimension `m x c`:\n",
    "\n",
    "$$softmax(XW)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18ca0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "def forward(X,W):\n",
    "    # use `softmax()` with X and W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922942f",
   "metadata": {},
   "source": [
    "A small test of your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cdfad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "W = init(65,10)\n",
    "X = np.zeros((20,65))\n",
    "assert forward(X,W).shape == (20, 10)\n",
    "assert abs(forward(X,W)[0].sum() - 1) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "849a3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
      "-\n",
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "-\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(forward(X,W))\n",
    "print('-')\n",
    "print(forward(X,W)[0])\n",
    "print('-')\n",
    "print(forward(X,W)[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60492f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(X,W)[0].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f32b7",
   "metadata": {},
   "source": [
    "### Assignment 1.5\n",
    "rubric={acuracy:3,efficiency:1}\n",
    "\n",
    "Next, we'll implement the function `gradient` which returns the gradient of the negative log-likelihood loss with regard to the model parameters. The function takes four arguments:\n",
    "\n",
    "* A single input example `x` having shape `1 x number_of_features` (1 x 65 in our case)\n",
    "* A single label `y` (a regular integer like `0`).\n",
    "* A weight matrix `W` of shape `number_of_features x number_of_classes` (65 x 10 in our case)\n",
    "* The regularization strength `beta` for L2 regularization\n",
    "\n",
    "You function should compute the gradient for each class and return all of them as an array having the same shape as `W`, that is, `number_of_features x number_of_classes`.\n",
    "\n",
    "Recall that the gradient of the regularized loss for class $c$ and a single example $(x, y)$ is given by the following formula (for some reason, github can't render the matrices correctly but they should look okay in jupyter):\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_c} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}(x)}{\\partial w_{c,1}} \\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}(x)}{\\partial w_{c,n}}\\end{bmatrix} = \\begin{bmatrix} {(\\hat{p}(c|x) - 1)\\cdot x_i + 2\\beta w_1} \\\\ \\vdots \\\\ {(\\hat{p}(c|x) - 1) \\cdot x_{n} + 2 \\beta w_n} \\end{bmatrix}$$\n",
    "\n",
    "when $c = y$, and\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_c} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}(x)}{\\partial w_{c,1}}\\\\ \\vdots \\\\ \\frac{\\partial \\mathcal{L}(x)}{\\partial w_{c,n}}\\end{bmatrix} = \\begin{bmatrix} {\\hat{p}(c|x)\\cdot x_i + 2\\beta w_1} \\\\ \\vdots \\\\ {\\hat{p}(c|x)\\cdot x_{n} + 2\\beta w_n} \\end{bmatrix}$$\n",
    "\n",
    "when $c \\ne y$. \n",
    "\n",
    "You should use your `forward` function to compute the probabilities $\\hat{p}(c|x)$ for each class $c$. \n",
    "\n",
    "Note, to get full score for this assignment, you should present a fully vectorized solution (i.e. **no loops**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d40bd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward [[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
      "ys [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((1, 65))\n",
    "y = 0\n",
    "W = np.zeros((65,10))\n",
    "beta = 0.001\n",
    "\n",
    "print('forward', forward(x,W))\n",
    "# print('shape of forward', forward(x,W).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4abeb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def gradient(x, y, W, beta):\n",
    "    # (foward x @ W ) X + 2 beta W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1f768",
   "metadata": {},
   "source": [
    "A small test of your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4e5c8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 1) (1, 10)\n",
      "(65, 10)\n"
     ]
    }
   ],
   "source": [
    "# A test which your function should pass. Note, that simply passing the test does not \n",
    "# guarantee that your function is working fully correctly.\n",
    "x = np.ones((1, 65))\n",
    "y = 0\n",
    "W = np.zeros((65,10))\n",
    "assert gradient(x,y,W,0.001).shape == W.shape\n",
    "# gradient1(x,y,W,0.001) == gradient1(x,y,W,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05d708",
   "metadata": {},
   "source": [
    "### Assignment 1.6\n",
    "rubric={accuracy:3,quality:1}\n",
    "\n",
    "You'll now implement a training function `train_sgd`. It takes three arguments:\n",
    "\n",
    "* `alpha`, the learning rate\n",
    "* `beta`, the regularization strength\n",
    "* `epochs`, the number of training epochs\n",
    "* `quiet`, a boolean which indicates if the function should print information during training\n",
    "\n",
    "The function starts by initializing model parameters $W$ using your `init` function (we need a 65 x 10 dimensional weight matrix). In every epoch, the function will iterate through the training data (`train_X` and `train_y`). It will compute the gradient of the negative log-loikelihood loss using your `gradient` function. It will then update the model parameters $W$ according to the formula:\n",
    "\n",
    "$$W := W - \\alpha \\cdot \\nabla \\mathcal{L}$$\n",
    "\n",
    "After every epoch, you should use your `nllloss` function to compute the loss over the training set. Unless `quiet=True`, print the average loss for a training example after each epoch (simply divide the loss by the number of training examples). The average loss for a training example should decrease through training (although it can oscillate a bit) and you should definitely attain an average loss << 1 at the end of training.  \n",
    "\n",
    "After every epoch, you should also evaluate your model on the development set. Tag the development set using your `forward` function and compute the accuracy using [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "Keep track of the model parameters which produced the best development accuracy. Return these parameters along with the best development accuracy after training. This is called early stopping.\n",
    "\n",
    "To test your function, train for 10 epochs using `alpha=0.01` and `beta=0.001`. Your dev accuracy should be better than 90%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1f4e2cd",
   "metadata": {},
   "source": [
    "```\n",
    "def train_sgd():\n",
    "    W = init\n",
    "    best_W = np.array(W)\n",
    "    best_acc = 0\n",
    "    for x, y from train  (through epochs...)\n",
    "        gradient:       Note that your x is (65,) --> (1,65)\n",
    "        W = W - alpha *grad\n",
    "    nllloss\n",
    "    prediction \n",
    "    get accuracy \n",
    "    update accuracy \n",
    "    return W, best_accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6cbd6168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.19405969359582229, dev accuracy: 93.75\n",
      "Epoch 2 loss: 0.13026479364421456, dev accuracy: 95.0\n",
      "Epoch 3 loss: 0.10401230648225789, dev accuracy: 95.75\n",
      "Epoch 4 loss: 0.08932121773098119, dev accuracy: 95.5\n",
      "Epoch 5 loss: 0.08071591244627968, dev accuracy: 95.5\n",
      "Epoch 6 loss: 0.0748273174424501, dev accuracy: 95.25\n",
      "Epoch 7 loss: 0.07012783117859879, dev accuracy: 94.75\n",
      "Epoch 8 loss: 0.06605684780132011, dev accuracy: 94.25\n",
      "Epoch 9 loss: 0.06229368675692227, dev accuracy: 94.25\n",
      "Epoch 10 loss: 0.05860935614082263, dev accuracy: 94.0\n",
      "Best development accuracy: 95.75%\n"
     ]
    }
   ],
   "source": [
    "# Please use these constants instead of the numbers 10 and 65 in your code\n",
    "# because magic numbers make your code less readable and less maintainable \n",
    "# (https://stackoverflow.com/questions/47882/what-is-a-magic-number-and-why-is-it-bad)\n",
    "CLASSES = 10\n",
    "FEATURES = 65\n",
    "\n",
    "# your code here\n",
    "\n",
    "def train_sgd(alpha, beta, epochs, quiet):\n",
    "\n",
    "    best_acc = 0\n",
    "    ...\n",
    "\n",
    "print(f\"Best development accuracy: {best_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b530000",
   "metadata": {},
   "source": [
    "### Assignment 1.7\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Perform full grid search for hyperparameter optimization of the learning rate `alpha` and regularization strength `beta`. Save the best parameters (i.e. the ones delivering optimal development accuracy) that you find as `best_W_sgd` and print the best `alpha`, `beta` configuration and best development accuracy. Test reasonable values for `alpha` and `beta` like `[1.0, 0.1, 0.01, 0.001, 0.0001]`. Use maximum epoch count `50` when calling `train_sgd`.\n",
    "\n",
    "The grid search can take a few minutes. You should be able to get accuracy > 94%.\n",
    "\n",
    "Finally, check performance on the test set (`test_X` and `test_y`) using `best_W_sgd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d97c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "# your code here\n",
    "best_acc = 0\n",
    "best_alpha = 0\n",
    "best_beta = 0\n",
    "best_W_sgd = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "997dc3cd",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "We will then visualize the weights which our logistic regression model learns for each digit class (0, 1, 2, 3, 4, 5, 6, 7, 8, 9). \n",
    "\n",
    "### Assignment 2.1\n",
    "rubric={accuracy:2}\n",
    "\n",
    "We will visualize weights in `best_W_sgd` using the function [`plt.imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html). Our model has 65 features and each class has, therefore, 65 weights. The first 64 of these correspond to pixels in the digit images, whereas the last feature is a specialized bias feature. We will visualize only the first 64 weights. \n",
    "\n",
    "In order to visualize each weight vector `best_W_sgd[i]`, extract the 64 first elements of the weight vector. Then rearrange these into an 8x8 pixel image using `reshape` and plot using `imshow`. Please also print the digit label.  \n",
    "\n",
    "It is recommended to use a diverging colormap like `seismic` (i.e. call `imshow` using the argument `cmap=\"seismic\"`), because this will very clearly illustrate high and low weights. Also, 8x8 images are extremely grainy which can make them hard to interpret. You should use the argument `interpolation=\"bicubic\"` when calling `imshow` to produce a smoother [interpolated](https://matplotlib.org/stable/gallery/images_contours_and_fields/interpolation_methods.html) visualization of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f9c5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_W_sgd[i]\n",
    "\n",
    "# your code here\n",
    "\n",
    "# for i in range(10):\n",
    "#     ...\n",
    "#     wi =  .. \n",
    "#     plt.imshow(wi,cmap=\"seismic\",interpolation=\"bicubic\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7871c2",
   "metadata": {},
   "source": [
    "### Assignment 2.2\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Do you think the weights learned by our model are interpretable? What do you think the high and low weights correspond to? Do you think some of the integers have more easily interpretable weight vectors than others? Why do you think that might be? \n",
    "\n",
    "Note, we're looking for your thoughts here and we want you to examine the weights and think about them. There is no \"correct\" answer but a perfect score requires that you explain your thought process (please keep your answer to two paragraphs, at most)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2abe544",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad5bc3",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "\n",
    "You should now compare our custom made logistic regression implementation to pytorch logistic regression. In pytorch terms, a logistic regression model is a neural network which has one linear layer followed by a softmax layer. You can use `nn.Sequential` to implement your pytorch model or build it from scratch (check last week's assignment for pointers). \n",
    "\n",
    "Once again, you are given a training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ec1f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(model, train_X, train_y,epochs,quiet=False):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    sgd = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        tot_loss = 0\n",
    "        for x, y in zip(train_X, train_y):\n",
    "            model.zero_grad()\n",
    "            sys_y = model(x).log()\n",
    "            loss = loss_function(sys_y, torch.tensor([y]))\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "            tot_loss += loss\n",
    "        if not quiet:\n",
    "            print(f\"Epoch {epoch+1} loss: {tot_loss/len(train_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cccfb",
   "metadata": {},
   "source": [
    "### Assignment 3.1\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Implement your pytorch logistic regression model below. This will be very similar to the models in laste week's assignment but simpler because there is only one linear layer and no non-linearity (apart from the final softmax layer). \n",
    "\n",
    "Note that your model should take vectors of dimensionality 65 (or `FEATURES`) as input and generate output vectors of dimension 10 (or `CLASSES`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1dc4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "torch_lr = nn.Sequential( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b136ee6",
   "metadata": {},
   "source": [
    "### Assignment 3.2\n",
    "rubric={accuracy:1}\n",
    "\n",
    "You should then convert the training, development and test sets (`train_X`, `dev_X` and `test_X`) into lists of pytorch tensors. Check last week's assignment for an example. \n",
    "\n",
    "Save your torch data as `torch_train_X`, `torch_dev_X` and `torch_test_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc12269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/3r6hrp015t9d1n6m8l36t0yh0000gn/T/ipykernel_12195/1700092446.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  torch_train_X = [torch.tensor([x]) for x in train_X]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "torch_train_X = \n",
    "torch_dev_X = \n",
    "torch_test_X = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2659c0b",
   "metadata": {},
   "source": [
    "You should then train your model for 10 epochs using the `train_torch` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a22f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.7421488991586361\n",
      "Epoch 2 loss: 0.16148844391350953\n",
      "Epoch 3 loss: 0.1114043386890857\n",
      "Epoch 4 loss: 0.08933341683984906\n",
      "Epoch 5 loss: 0.07417186683326941\n",
      "Epoch 6 loss: 0.0626775238868657\n",
      "Epoch 7 loss: 0.05370857570053636\n",
      "Epoch 8 loss: 0.04668567787101115\n",
      "Epoch 9 loss: 0.04120821706374175\n",
      "Epoch 10 loss: 0.03690530687982026\n",
      "Epoch 11 loss: 0.0335146796052019\n",
      "Epoch 12 loss: 0.030802930396568357\n",
      "Epoch 13 loss: 0.028576748157307954\n",
      "Epoch 14 loss: 0.026705113748319746\n",
      "Epoch 15 loss: 0.025100195347737646\n",
      "Epoch 16 loss: 0.023701169735824167\n",
      "Epoch 17 loss: 0.022464839096111223\n",
      "Epoch 18 loss: 0.021359626192797278\n",
      "Epoch 19 loss: 0.020361826749215155\n",
      "Epoch 20 loss: 0.01945333247030215\n",
      "Epoch 21 loss: 0.01862017499746036\n",
      "Epoch 22 loss: 0.01785149960740934\n",
      "Epoch 23 loss: 0.017138791180271944\n",
      "Epoch 24 loss: 0.016475283987456537\n",
      "Epoch 25 loss: 0.015855521359912646\n",
      "Epoch 26 loss: 0.015275035136248553\n",
      "Epoch 27 loss: 0.014730114313940801\n",
      "Epoch 28 loss: 0.014217636043497057\n",
      "Epoch 29 loss: 0.01373493911833133\n",
      "Epoch 30 loss: 0.013279727187734564\n",
      "Epoch 31 loss: 0.012849993964478897\n",
      "Epoch 32 loss: 0.01244396542354352\n",
      "Epoch 33 loss: 0.012060055184677874\n",
      "Epoch 34 loss: 0.011696829883084653\n",
      "Epoch 35 loss: 0.011352981927870013\n",
      "Epoch 36 loss: 0.011027307753945491\n",
      "Epoch 37 loss: 0.010718690365786689\n",
      "Epoch 38 loss: 0.010426085493508772\n",
      "Epoch 39 loss: 0.0101485109627866\n",
      "Epoch 40 loss: 0.00988503895023367\n",
      "Epoch 41 loss: 0.009634790740710471\n",
      "Epoch 42 loss: 0.009396933512986403\n",
      "Epoch 43 loss: 0.009170678618957113\n",
      "Epoch 44 loss: 0.00895528081811653\n",
      "Epoch 45 loss: 0.008750037983542371\n",
      "Epoch 46 loss: 0.008554290891748851\n",
      "Epoch 47 loss: 0.008367422823940344\n",
      "Epoch 48 loss: 0.00818885881987452\n",
      "Epoch 49 loss: 0.008018064523009067\n",
      "Epoch 50 loss: 0.007854544629153313\n",
      "Epoch 51 loss: 0.007697840998844399\n",
      "Epoch 52 loss: 0.007547530518757571\n",
      "Epoch 53 loss: 0.007403222804577501\n",
      "Epoch 54 loss: 0.00726455783248184\n",
      "Epoch 55 loss: 0.007131203573849608\n",
      "Epoch 56 loss: 0.007002853692088056\n",
      "Epoch 57 loss: 0.006879225344450749\n",
      "Epoch 58 loss: 0.006760057117165728\n",
      "Epoch 59 loss: 0.006645107109992978\n",
      "Epoch 60 loss: 0.0065341511767496715\n",
      "Epoch 61 loss: 0.006426981321256969\n",
      "Epoch 62 loss: 0.006323404243247108\n",
      "Epoch 63 loss: 0.006223240025616275\n",
      "Epoch 64 loss: 0.006126320952615312\n",
      "Epoch 65 loss: 0.006032490447771467\n",
      "Epoch 66 loss: 0.005941602120235273\n",
      "Epoch 67 loss: 0.005853518908607123\n",
      "Epoch 68 loss: 0.005768112311943351\n",
      "Epoch 69 loss: 0.0056852616984414345\n",
      "Epoch 70 loss: 0.005604853683169239\n",
      "Epoch 71 loss: 0.005526781567075345\n",
      "Epoch 72 loss: 0.0054509448303548015\n",
      "Epoch 73 loss: 0.005377248674030589\n",
      "Epoch 74 loss: 0.005305603604328278\n",
      "Epoch 75 loss: 0.005235925055070496\n",
      "Epoch 76 loss: 0.005168133043895947\n",
      "Epoch 77 loss: 0.005102151858620426\n",
      "Epoch 78 loss: 0.005037909770507988\n",
      "Epoch 79 loss: 0.004975338771615775\n",
      "Epoch 80 loss: 0.004914374333721401\n",
      "Epoch 81 loss: 0.004854955186641913\n",
      "Epoch 82 loss: 0.004797023114016265\n",
      "Epoch 83 loss: 0.00474052276485054\n",
      "Epoch 84 loss: 0.004685401479322952\n",
      "Epoch 85 loss: 0.004631609127518888\n",
      "Epoch 86 loss: 0.004579097959915475\n",
      "Epoch 87 loss: 0.0045278224685663905\n",
      "Epoch 88 loss: 0.004477739258051383\n",
      "Epoch 89 loss: 0.0044288069253547785\n",
      "Epoch 90 loss: 0.004380985947924675\n",
      "Epoch 91 loss: 0.004334238579240359\n",
      "Epoch 92 loss: 0.004288528751283687\n",
      "Epoch 93 loss: 0.0042438219833685195\n",
      "Epoch 94 loss: 0.004200085296835103\n",
      "Epoch 95 loss: 0.004157287135162167\n",
      "Epoch 96 loss: 0.004115397289091112\n",
      "Epoch 97 loss: 0.004074386826392311\n",
      "Epoch 98 loss: 0.004034228025937164\n",
      "Epoch 99 loss: 0.003994894315767486\n",
      "Epoch 100 loss: 0.003956360214880708\n"
     ]
    }
   ],
   "source": [
    "train_torch(torch_lr, torch_train_X, train_y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e69b4",
   "metadata": {},
   "source": [
    "### Assignment 3.3\n",
    "rubric={accuracy:1}\n",
    "\n",
    "You should now classify the development examples `torch_dev_X` using your torch model. You can copy the `classify` function from last week's assignment and apply it to the development and test set. Use [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to figure out the accuracy. Make sure that performance is similar to our custom made logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3487dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev acc: 0.9525\n",
      "Test acc: 0.9017632241813602\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def classify(data, model):\n",
    "    res = []\n",
    "    # ...\n",
    "    return res\n",
    "\n",
    "sys_dev_y = classify(torch_dev_X, torch_lr)\n",
    "print(f\"Dev acc: {sklearn.metrics.accuracy_score(dev_y,sys_dev_y)}\")\n",
    "\n",
    "sys_test_y = classify(torch_test_X, torch_lr)\n",
    "print(f\"Test acc: {sklearn.metrics.accuracy_score(test_y,sys_test_y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26697b9",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "In this optional assignment, we'll implement Newton's method for model training. This is a bit more challenging than the assignments above.\n",
    "\n",
    "### Assignment 4.1 Optional\n",
    "rubric={accuracy:2,efficiency:1}\n",
    "\n",
    "We'll start by computing the 2nd order derivative of the loss function, that is, the Hessian. Your `hessian` function takes two arguments:\n",
    "\n",
    "* An array of input examples `data_X` which has shape `number_of_examples x number_of_features`. In our case, when calling the function with train_X, the array will have dimension 1000 x 65.\n",
    "* The current model weights `W` having shape `number_of_features x number_of_classes` (65 x 10 in our case).\n",
    "\n",
    "Recall that the hessian of the negative log-likelihood loss for the logistic regression model given a single training example $x$ and the class label $y$ is:\n",
    "\n",
    "$$H(x,y) = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial w_1^2} & \\frac{\\partial \\mathcal{L}}{\\partial w_1 \\partial w_2} & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_1 \\partial w_n} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2 \\partial w_1} & \\frac{\\partial \\mathcal{L}}{\\partial w_2^2} & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_2 \\partial w_n} \\\\\n",
    " \\vdots & \\vdots & \\ddots  & \\vdots\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_n \\partial w_1} & \\frac{\\partial \\mathcal{L}}{\\partial w_n \\partial w_2} & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_n^2} \\end{bmatrix} = \n",
    "\\begin{bmatrix} p(y|x)(1 - p(y|x))x_1x_1 & p(y|x)(1 - p(y|x))x_1x_2 & \\ldots & p(y|x)(1 - p(y|x))x_1x_n\\\\\n",
    "p(y|x)(1 - p(y|x))x_2x_1 & p(y|x)(1 - p(y|x))x_2x_2 & \\ldots & p(y|x)(1 - p(y|x))x_2x_n \\\\\n",
    " \\vdots & \\vdots & \\ddots  & \\vdots\\\\\n",
    "p(y|x)(1 - p(y|x))x_nx_1 & p(y|x)(1 - p(y|x))x_nx_2 & \\ldots & p(y|x)(1 - p(y|x))x_nx_n \\\\ \\end{bmatrix}$$\n",
    "\n",
    "For each class $y$, you should compute the Hessian over the entire training set as an average of the individual $H(x,y)$:\n",
    "\n",
    "$$H(y) = \\frac{\\Sigma_{k=1}^m H(x_m, y)}{n}$$\n",
    "\n",
    "You should then return an array having shape `number_of_classes x number_of_features x number_of_features` which encodes the Hessian for each class $y$.\n",
    "\n",
    "Note, to get full score for this assignment, you should use as few loops as possible. It is very tricky to avoid looping over the the classes `y` so you are allowed to do this, but try to avoid looping over the data in `data_X` because this is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed6b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def hessian(data_X, W):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bed0dd",
   "metadata": {},
   "source": [
    "A small test for your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccadcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = init(FEATURES, CLASSES)\n",
    "assert hessian(train_X, W).shape == (CLASSES, FEATURES, FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22e8ba",
   "metadata": {},
   "source": [
    "### Assignment 4.2 Optional\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "We'll now implement Newton's method. You'll write a function `train_newton` which takes three arguments:\n",
    "\n",
    "* `beta`, the regularization strength\n",
    "* `epochs`, the number of training epochs\n",
    "* `quiet`, a boolean which indicates if the function should print information during training\n",
    "\n",
    "Note that there is no learning rate because Newton's method does not require one.\n",
    "\n",
    "The function starts by initializing model parameters $W$ using your `init` function (we need a 65 x 10 dimensional weight matrix). \n",
    "\n",
    "In every epoch, you will need to first compute the average gradient $\\nabla \\mathcal{L}$ over all the examples in `data_X`. You can use your existing `gradient` function to compute a gradient for each example and average them. This should give you an array of shape `number_of_features x number_of_classes` (65 x 10 in our case).\n",
    "\n",
    "You will then compute the Hessian $H$ using your function `hessian`. \n",
    "\n",
    "The function will then update the model parameters $W_c$ for class $c$ according to the formula:\n",
    "\n",
    "$$W_c := W_c - \\alpha \\cdot  (H_c + \\beta I)^{-1} \\nabla \\mathcal{L}_c$$\n",
    "\n",
    "Here, $\\beta$ is regularization. You can invert the matrix $H + \\beta$ using [`numpy.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html). To update all parameters, you can loop over the digit classes (0, 1, ..., 9) and update the parameters separately for each class.\n",
    "\n",
    "After every epoch, you should use your `nllloss` function to compute the loss over the training set. Unless `quiet=True`, print the average loss for a training example after each epoch (simply divide the loss by the number of training examples). The average loss for a training example should decrease through training and you should definitely attain an average loss << 1 at the end of training.  \n",
    "\n",
    "After every epoch, you should also evaluate your model on the development set. Tag the development set using your `forward` function and compute the accuracy using [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "Keep track of the model parameters which produced the best development accuracy. Return these parameters along with the best development accuracy after training. This is called early stopping.\n",
    "\n",
    "To test your function, train for 10 epochs using `beta=0.01`. Your dev accuracy should be better than 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c43ae2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.20608650471513504, dev accuracy: 93.5\n",
      "Epoch 2 loss: 0.11563878315200629, dev accuracy: 95.5\n",
      "Epoch 3 loss: 0.08571011664244932, dev accuracy: 95.5\n",
      "Epoch 4 loss: 0.0694566082751175, dev accuracy: 96.25\n",
      "Epoch 5 loss: 0.058914394941683684, dev accuracy: 95.75\n",
      "Epoch 6 loss: 0.05140121811770905, dev accuracy: 95.75\n",
      "Epoch 7 loss: 0.04571505565927129, dev accuracy: 95.75\n",
      "Epoch 8 loss: 0.04123333179885361, dev accuracy: 95.75\n",
      "Epoch 9 loss: 0.03759499028576157, dev accuracy: 95.75\n",
      "Epoch 10 loss: 0.034574595465139, dev accuracy: 95.75\n",
      "Epoch 11 loss: 0.032022537299291234, dev accuracy: 95.75\n",
      "Epoch 12 loss: 0.02983505537829985, dev accuracy: 95.75\n",
      "Epoch 13 loss: 0.027937530229234956, dev accuracy: 95.75\n",
      "Epoch 14 loss: 0.026274761441801863, dev accuracy: 95.75\n",
      "Epoch 15 loss: 0.02480493257833273, dev accuracy: 95.75\n",
      "Epoch 16 loss: 0.023495736428455875, dev accuracy: 95.75\n",
      "Epoch 17 loss: 0.022321785116260463, dev accuracy: 95.75\n",
      "Epoch 18 loss: 0.021262833655848034, dev accuracy: 95.75\n",
      "Epoch 19 loss: 0.02030252835669195, dev accuracy: 95.75\n",
      "Epoch 20 loss: 0.01942750706045276, dev accuracy: 95.75\n",
      "Best development accuracy: 96.25%\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def train_newton(beta, epochs, quiet):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return W, best_acc\n",
    "\n",
    "W, best_acc = train_newton(beta=1, epochs=20, quiet=False)\n",
    "print(f\"Best development accuracy: {best_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dba1f",
   "metadata": {},
   "source": [
    "### Assignment 4.3 Optional\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Optimization of the regularization strength `beta`. Save the best parameters (i.e. the ones delivering optimal development accuracy) that you find as `best_W_newton` and print the best `beta` and best development accuracy. Test reasonable values for `beta` like `[1.0, 0.1, 0.01, 0.001, 0.0001]`. Use maximum epoch count `50` when calling `train_newton`.\n",
    "\n",
    "The grid search can take a few minutes. You should be able to get accuracy > 94%.\n",
    "\n",
    "Finally, check performance on the test set (`test_X` and `test_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "929950b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta=1.0\n",
      "Found best dev acc so far: 96.25\n",
      "beta=0.1\n",
      "beta=0.01\n",
      "beta=0.001\n",
      "beta=0.0001\n",
      "\n",
      "Best alpha=0.001, beta=1.0, acc=96.25\n",
      "\n",
      "Test acc: 90.6801007556675\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 50\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103fdcd",
   "metadata": {},
   "source": [
    "### Assignment 4.5 Optional\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Do you see a difference in accuracy between SGD and Newton training? Do you think this is surprising?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab76c1c0",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
